{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Este *notebook* incluye:\n",
    "- Pequeños ejemplos de uso de *pandas*.\n",
    "- Un *DataSet* (de *PyTorch*) que almacena información de los archivos de audio con los cantos de las aves. Este *DataSet*, al solicitársele el i-ésimo *item*, devuelve un cacho del i-ésimo audio, un cacho de un j-ésimo audio, y un 0 o 1 si `i != j` o `i == j` respectivamente.\n",
    "- Un *DataLoader* (de *PyTorch*) que envuelve al *DataSet* previamente descrito.\n",
    "- Una Red Neuronal (de *PyTorch*) que toma los espectrogramas de dos audios de longitud 1s cada uno, y devuelve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones\n",
    "\n",
    "Importación de las bibliotecas a utilizar, y una pequeña descripción de cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas is an open source data analysis and manipulation tool.\n",
    "import pandas as pd\n",
    "\n",
    "# NumPy is for scientific computing with Python\n",
    "import numpy as np\n",
    "\n",
    "# \n",
    "import tensorflow as tf\n",
    "\n",
    "# PyTorch is an open source machine learning framework.\n",
    "import torch\n",
    "\n",
    "# PyTorch provides the torch.nn module to help us\n",
    "# in creating and training of the neural network.\n",
    "import torch.nn as nn\n",
    "\n",
    "# PyTorch has two primitives to work with data:\n",
    "# torch.utils.data.Dataset stores the samples and their corresponding labels.\n",
    "# torch.utils.data.DataLoader wraps an iterable around the Dataset.\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# \"The easiest way to use deep metric learning in your application\".\n",
    "# Written in PyTorch.\n",
    "# https://github.com/KevinMusgrave/pytorch-metric-learning\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "# librosa is for music and audio analysis; it provides\n",
    "# the building blocks necessary to create music\n",
    "# information retrieval systems.\n",
    "import librosa\n",
    "\n",
    "# Displays a spectrogram/chromagram/cqt/etc.\n",
    "from librosa.display import specshow\n",
    "\n",
    "# matplotlib.pyplot is a collection of functions that make\n",
    "# matplotlib work like MATLAB. Each pyplot function makes\n",
    "# some change to a figure: e.g., creates a figure, creates\n",
    "# a plotting area in a figure, plots some lines in a plotting\n",
    "# area, decorates the plot with labels, etc.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorBoard is a visualization toolkit for machine learning\n",
    "# experimentation. TensorBoard allows tracking and visualizing\n",
    "# metrics such as loss and accuracy, visualizing the model graph,\n",
    "# viewing histograms, displaying images and much more.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Para tomar el tiempo que toman ciertos procesos de la siguiente manera:\n",
    "# start = timer()\n",
    "# (algún proceso)\n",
    "# end = timer()\n",
    "# El tiempo en segundos es end-start.\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# CUDA-accelerated PyTorch implementation of the\n",
    "# T-Stochastic Neighbor Embedding algorithm.\n",
    "#from tsne_torch import TorchTSNE as TSNE\n",
    "\n",
    "# Manejo de guardado y cargado de objetos mediante archivos.\n",
    "import pickle\n",
    "\n",
    "# Manejo de pseudo-aleatoriedad.\n",
    "import random\n",
    "\n",
    "# Manejo de funciones matemáticas.\n",
    "import math\n",
    "\n",
    "# Manejo de fecha y tiempo.\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables generales\n",
    "\n",
    "Variables generales/globales que se utilizarán a lo largo del *notebook*. Conviene tener este apartado para consultarlas y modificarlas fácilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizando cuda:0 para el procesamiento de datos.\n"
     ]
    }
   ],
   "source": [
    "# Uso del GPU, si está disponible.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilizando {device} para el procesamiento de datos.\")\n",
    "\n",
    "# Cadena con la ubicación del archivo CSV que contiene\n",
    "# el DataFrame con datos de los audios de aves.\n",
    "birds_csv = \"/media/birds/BirdsDataFrame.csv\"\n",
    "\n",
    "# Cadena con la ubicación de los archivos WAV y XML\n",
    "# correspondientes a los audios de aves a procesar.\n",
    "birds_path = \"/media/birds/data/\"\n",
    "\n",
    "# Otras ubicaciones útiles.\n",
    "DIR_runs = \"./runs/\"\n",
    "DIR_objects = DIR_runs+\"python_objects/\"\n",
    "DIR_tensorboard = DIR_runs+\"tensorboard/\"\n",
    "DIR_notebooks = DIR_runs+\"notebooks/\"\n",
    "DIR_models = DIR_runs+\"models/\"\n",
    "\n",
    "# Nombre de la columna, dentro del DataFrame,\n",
    "# que contiene el nombre de los archivos de audio.\n",
    "file_col_name = \"FileName\"\n",
    "\n",
    "# DataFrame (de 'pandas') del archivo CSV dado.\n",
    "birds_df = pd.read_csv(birds_csv)\n",
    "\n",
    "# Los audios de aves se cortarán en cachos cuya longitud\n",
    "# varíe entre len_min segundos y len_max segundos.\n",
    "len_min = 1\n",
    "len_max = 1\n",
    "\n",
    "# Ancho y alto de cada espectrograma.\n",
    "# TO-DO: ¿Es posible calcular esto mediante una fórmula? Resulta del size()/shape de aplicar \"stft\" al audio \"y\".\n",
    "ancho,alto = 1025,87\n",
    "\n",
    "# Número de canales que tendrá cada audio.\n",
    "# Hasta ahora, si un audio tiene 1 canal, y aquí se\n",
    "# especifican 2, se copia el primer canal en un\n",
    "# segundo canal. Si un audio tiene más de 2 canales,\n",
    "# la operación no está definida.\n",
    "audio_channels = 2\n",
    "\n",
    "# Frecuencia de muestreo a la cual TODOS los audios se\n",
    "# muestrearán. Esto es necesario para que los vectores\n",
    "# que representan a los audios tengan los mismos tamaños.\n",
    "sr = 44100\n",
    "\n",
    "# Probabilidad de que dos audios de aves (o\n",
    "# cachos de audios) compartan cierta propiedad.\n",
    "p_prop = 0.5\n",
    "\n",
    "# Variables asociadas a la Red Neuronal.\n",
    "batch_size = 32 # Número de muestras que se tomarán por lote/epoch.\n",
    "epochs = 120 # Veces que se recorrerá un DataSet entero.\n",
    "lr = 6e-05 # Learning Rate.\n",
    "#momentum = 0.5 # The SGD momentum (default: 0.5) is the moving average of our gradients (helps to keep direction).\n",
    "\n",
    "# Para TensorBoard, creamos un SummaryWriter.\n",
    "# Éste escribiría al directorio ./runs/ por defecto.\n",
    "dt_string = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "writer = SummaryWriter(log_dir=DIR_tensorboard+dt_string+\"_adbekunkus\")\n",
    "\n",
    "# Función a utilizar para procesar los audios de aves.\n",
    "def librosa_process(path, cut, cut_len=None):\n",
    "    \"\"\"\n",
    "    Función que carga un audio con Librosa y devuelve el vector\n",
    "    unidimensional que representa al audio, y su frecuencia de muestreo.\n",
    "    :param str path: Ruta donde se ubica el audio.\n",
    "    :param bool cut: ¿Se cortará (y devolverá) sólo un cacho aleatorio del audio?\n",
    "    :param float cut_len: Longitud del cacho de audio (si cut==True).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Longitud del audio completo en segundos.\n",
    "    audio_len = librosa.get_duration(filename=path)\n",
    "    \n",
    "    # Si se desea el audio completo, 'librosa' lo\n",
    "    # cargará desde el inicio hasta el final.\n",
    "    start = 0\n",
    "    duracion = audio_len\n",
    "        \n",
    "    # Si se desea sólo un cacho del audio...\n",
    "    if cut:\n",
    "        \n",
    "        # Determinamos la longitud del cacho\n",
    "        # aleatorio de audio en segundos.\n",
    "        duracion = cut_len if cut_len != None else random.uniform(len_min, len_max) # Rango [a,b].\n",
    "        \n",
    "        # Aseguramos que el audio completo es más\n",
    "        # grande que el tamaño del cacho que queremos.\n",
    "        assert audio_len > duracion\n",
    "        \n",
    "        # Definimos en dónde empezará\n",
    "        # (aleatoriamente) el cacho de audio.\n",
    "        start = random.uniform(0, audio_len-duracion) # Rango [a,b].\n",
    "    \n",
    "    # Obtenemos el audio-vector y su (nueva) frecuencia de muestreo.\n",
    "    y, sampling_rate = librosa.load(path, sr=sr, offset=start, duration=duracion, mono=False)\n",
    "    \n",
    "    # Algunos audios fueron grabados en dos canales (stereo), y otros en\n",
    "    # uno (mono). Convertimos los que fueron grabados en un canal en\n",
    "    # audios de dos canales (al duplicar el único canal que tienen).\n",
    "    if y.ndim == 1:\n",
    "        y = np.repeat(y[np.newaxis, :], 2, axis=0)\n",
    "    \n",
    "    # Función no definida para audios que tienen más de dos canales.\n",
    "    # Igual se lanza un error si los vectores no tienen la longitud adecuada (sr).\n",
    "    assert(y.shape == (2, sr))\n",
    "    \n",
    "    # Short-time Fourier transform (STFT).\n",
    "    # The STFT represents a signal in the time-frequency domain by computing\n",
    "    # discrete Fourier transforms (DFT) over short overlapping windows.\n",
    "    stft = librosa.stft(y)\n",
    "    \n",
    "    # This function (stft) returns a complex-valued matrix D such that\n",
    "    # np.abs(D[..., f, t]) is the magnitude of frequency bin f at frame t.\n",
    "    magnitude = np.abs(stft)\n",
    "    \n",
    "    # Converts an amplitude spectrogram to dB-scaled spectrogram.\n",
    "    spectogram = librosa.amplitude_to_db(magnitude)\n",
    "    \n",
    "    # Devolvemos el espectrograma y su frecuencia de muestreo.\n",
    "    return spectogram, sampling_rate\n",
    "\n",
    "# Comprobaciones sobre las variables aquí definidas.\n",
    "assert len_min <= len_max # Lógicamente, min<=max.\n",
    "assert p_prop >= 0 and p_prop <= 1 # Las probabilidades se encuentran en este rango."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _pandas_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el archivo `birds_csv` cuenta con *N* columnas `columna0,columna1,...,columnaN-1`, imprimimos a continuación el nombre de cada columna, enumerándolas desde cero.\n",
    "\n",
    "**NOTA**: La primera columna no tiene nombre, por lo que *pandas*, al convertir el archivo CSV en un *DataFrame* mediante la función `read_csv()`, le asigna el nombre `Unnamed: 0`. Esta columna sirve para indexar a las entradas dentro del archivo CSV (no confundir con la columna 'index' cuyo propósito es indexar a los archivos de audio de otra manera)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Por cada columna del DataFrame, imprimimos dicha columna.\n",
    "#for i,col in enumerate(birds_df.columns):\n",
    "#    print(f\"{i}:{col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplificamos con la primera entrada del archivo al imprimir qué valor tiene asociado a cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"iloc\" permite indexar por posiciones mediante el uso de enteros.\n",
    "# Por cada columna y valor en la primera línea, imprimimos ambos.\n",
    "#for col, val in birds_df.iloc[0].iteritems():\n",
    "#    print(f\"{col}:{val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algunas columnas tal que todas las entradas del archivo comparten un mismo valor dentro de esa columna. A continuación imprimimos los nombres de las columnas que cumplen ésto, así como el valor que todas las entradas comparten en dicha columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Por cada columna del DataFrame...\n",
    "#for col in birds_df.columns:\n",
    "    \n",
    "    # Si todas las entradas tienen el mismo valor en dicha\n",
    "    # columna, imprimimos la columna y el valor correspondiente.\n",
    "    #if (birds_df[col] == birds_df[col][0]).all():\n",
    "        #print(f\"{col}:{birds_df[col][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del *DataSet*\n",
    "\n",
    "Creamos el *DataSet* de *PyTorch* que guarda y maneja los datos de los archivos de audio (que contienen los cantos de las aves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBirdDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset de audios de aves.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, process_func, audio_path, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        The __init__ function is run once when instantiating the Dataset object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 'df' es el DataFrame a almacenar.\n",
    "        self.df = df\n",
    "        \n",
    "        # 'process_func' toma la ruta de un audio a procesar, y lo procesa.\n",
    "        self.process_func = process_func\n",
    "        \n",
    "        # 'audio_path' es la ruta donde se ubican los archivos de audio.\n",
    "        self.audio_path = audio_path\n",
    "        \n",
    "        # 'transform' and 'target_transform' modify the samples and labels respectively.\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The __len__ function returns the number of samples in our dataset.\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx=None):\n",
    "        \"\"\"\n",
    "        The __getitem__ function loads and returns a sample from the dataset at the given index 'idx'.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Si no se especifica un índice, se toma una muestra aleatoria.\n",
    "        if idx == None:\n",
    "            idx = random.randrange(0, birds_ds.__len__()) # Rango [a,b).\n",
    "        \n",
    "        # Obtenemos la idx-ésima línea del DataFrame almacenado.\n",
    "        # Y el nombre del archivo de audio a procesar.\n",
    "        item = self.df.iloc[idx]\n",
    "        filename = item[file_col_name]\n",
    "        \n",
    "        # Procesamos el primer cacho de audio.\n",
    "        x,_ = self.process_func(self.audio_path+filename, True)\n",
    "        \n",
    "        # Si se desea que ambos cachos de audio compartan la propiedad,\n",
    "        # sólo dejamos la etiqueta como \"1\", y volvemos a procesar\n",
    "        # el mismo archivo de audio de manera aleatoria (más adelante).\n",
    "        if (random.random() < p_prop):\n",
    "            target = 1\n",
    "            \n",
    "        # Si, por otro lado, se desea que los cachos no compartan la\n",
    "        # propiedad, dejamos la etiqueta como \"-1\", y buscamos otro\n",
    "        # archivo de audio para procesar.\n",
    "        else:\n",
    "            target = -1\n",
    "            \n",
    "            # Guardamos la especie del ave del primer cacho de audio.\n",
    "            primera_especie = item[\"Species\"]\n",
    "            \n",
    "            # Quitamos el primer archivo de audio (que ya fue procesado) del\n",
    "            # DataFrame (temporalmente), obtenemos algún renglón aleatorio de\n",
    "            # este nuevo DataFrame (sample() devuelve un DataFrame, por lo que\n",
    "            # es necesario tomar el primer renglón con iloc[0]), y obtenemos\n",
    "            # el nombre del nuevo archivo de audio a procesar.\n",
    "            item = self.df.drop(idx).sample().iloc[0]\n",
    "            filename = item[file_col_name]\n",
    "                  \n",
    "            # Guardamos la especie del ave del segundo cacho de audio.\n",
    "            segunda_especie = item[\"Species\"]\n",
    "            \n",
    "            # TO-DO: Si son la misma especie, ¿sigo buscando otro segundo cacho\n",
    "            # de audio, o cambio el \"target\" a 1? Por ahora sólo lo cambio a 1.\n",
    "            #print(f\"El primer cacho de audio pertenece a un ave {primera_especie}, y el segundo pertenece a un ave {segunda_especie}.\")\n",
    "            if primera_especie == segunda_especie:\n",
    "                target = 1\n",
    "\n",
    "        # Procesamos el segundo cacho de audio.\n",
    "        y,_ = self.process_func(self.audio_path+filename, True)\n",
    "            \n",
    "        # NOTA:\n",
    "        # Aún no se define el uso para 'transform' y 'target_transform'.\n",
    "        # Una propuesta es que 'transform' sustituya a 'process_func'.\n",
    "        \n",
    "        # Devolvemos el primer cacho de audio, el segundo cacho de audio,\n",
    "        # y la etiqueta que indica si ambos comparten (1) o no (0) la propiedad.\n",
    "        return x, y, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el *DataSet* al pasarle:\n",
    "- El *DataFrame* creado previamente con *pandas*.\n",
    "- La función a utilizar para procesar los audios.\n",
    "- La ruta del directorio en el cual se encuentran los archivos de audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_ds = CustomBirdDataset(birds_df, librosa_process, birds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo del *DataSet*\n",
    "\n",
    "Y obtenemos una muestra aleatoria del *DataSet* mediante su función `__getitem__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#birds_ds.__getitem__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del *DataLoader*\n",
    "\n",
    "Creamos dos *DataLoader* de *PyTorch* que envuelven el *DataSet* previamente definido. Uno está definido para el entrenamiento, mientras que otro está definido para el testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_dl_train = [\n",
    "    DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    DataLoader(birds_ds, batch_size=batch_size, shuffle=False, drop_last=True),\n",
    "    #DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    #DataLoader(birds_ds, batch_size=batch_size, shuffle=False, drop_last=True),\n",
    "    #DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "]\n",
    "\n",
    "birds_dl_test = DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo del **DataLoader**\n",
    "\n",
    "El *DataLoader* contiene listas (que regresa la función `__getitem__()` correspondiente al *DataSet*). Estas listas contienen los lotes de tamaño `batch_size` y, para abarcar todos los datos, contiene aproximadamente `tamaño_de_todos_los_datos/batch_size` listas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tamaño del DataSet (de PyTorch) = 3277 = 3277 = Tamaño del DataFrame (de pandas)\n"
     ]
    }
   ],
   "source": [
    "print(f\"- Tamaño del DataSet (de PyTorch) = {len(birds_ds)} = {len(birds_df)} = Tamaño del DataFrame (de pandas)\")\n",
    "#print(f\"- Tamaño del DataLoader (de PyTorch): {len(birds_dl)}\")\n",
    "#iterador = iter(birds_dl)\n",
    "#primer_lote = next(iterador)\n",
    "#print(f\"- Tamaño de la primera lista del DataLoader: {len(primer_lote)}\")\n",
    "#print(f\"- Tamaño de los elementos de la primera lista: {len(primer_lote[0])} {len(primer_lote[1])} {len(primer_lote[2])}\")\n",
    "#print(f\"- Tamaño del DataLoader por el tamaño de cada lote: {len(birds_dl)*batch_size} ≈ {len(birds_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos datos sobre el primer lote para ejemplificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#primeros_cachos, segundos_cachos, labels = primer_lote\n",
    "#print(f\"- Los primeros cachos de audio del primer lote tienen tamaño: {primeros_cachos.size()}\")\n",
    "#print(f\"- Los segundos cachos de audio del primer lote tienen tamaño: {segundos_cachos.size()}\")\n",
    "#print(f\"- Las etiquetas del primer lote tienen tamaño: {labels.size()}\")\n",
    "#print(f\"- Etiquetas del primer lote: {labels}\")\n",
    "#print(f\"- Primeros cachos del primer lote: {primeros_cachos}\")\n",
    "#print(f\"- Segundos cachos del primer lote: {segundos_cachos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module is the base class for all neural network modules.\n",
    "# Our models should also subclass this class.\n",
    "# Modules can also contain other Modules, allowing to nest them in a tree structure.\n",
    "class RN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red Neuronal.\n",
    "    \"\"\"\n",
    "    \n",
    "    #This defines the structure of the NN.\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicialización de la Red Neuronal.\n",
    "        Aquí se define su estructura.\n",
    "        \"\"\"\n",
    "        \n",
    "        #\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inicio de las capas convolucionales.\n",
    "        conv_layers = []\n",
    "        \n",
    "        # Primera capa convolucional.\n",
    "        self.conv1 = nn.Conv2d(in_channels=audio_channels, out_channels=batch_size, kernel_size=(10,10), stride=(2,1))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        conv_layers += [self.conv1, self.relu1, self.mp1]\n",
    "        \n",
    "        # Segunda capa convolucional.\n",
    "        self.conv2 = nn.Conv2d(in_channels=batch_size, out_channels=(batch_size//2), kernel_size=(7,7), stride=(2,1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        conv_layers += [self.conv2, self.relu2, self.mp2]\n",
    "        \n",
    "        # Tercera capa convolucional.\n",
    "        self.conv3 = nn.Conv2d(in_channels=(batch_size//2), out_channels=(batch_size//4), kernel_size=(4,4), stride=(2,1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.mp3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        conv_layers += [self.conv3, self.relu3, self.mp3]\n",
    "        \n",
    "        # Fin de las capas convoluciones.\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        # Inicio de las capas lineales (fully-connected).\n",
    "        fc_layers = []\n",
    "        \n",
    "        # Primera capa lineal.\n",
    "        # TO-DO: Determinar entrada.\n",
    "        # TO-DO: Determinar salida.\n",
    "        self.fc1 = nn.Linear(int(22.5*batch_size),512)\n",
    "        fc_layers += [self.fc1]\n",
    "        \n",
    "        # Fin de las capas lineales.\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "        \n",
    "        # Segunda capa lineal.\n",
    "        # TO-DO: Determinar entrada.\n",
    "        # TO-DO: Determinar salida. ¿Es 1 valor para cada entrada?\n",
    "        self.fc2 = nn.Linear(4063232, 1)\n",
    "        # Ésta no se agrega a las demás,\n",
    "        # pues no se aplica individualmente a\n",
    "        # cada entrada; primero es necesario\n",
    "        # realizar la operación de distancia\n",
    "        # sobre éstas para después aplicar\n",
    "        # esta capa lineal.\n",
    "    \n",
    "    def invididual_process(self, z):\n",
    "        \n",
    "        start = timer()\n",
    "        z = self.conv(z)\n",
    "        end = timer()\n",
    "        #print(f\"\\t\\t[time] Capas convolucionales: time={end-start}s out_size={z.size()}\") # DEBUG\n",
    "        \n",
    "        # Para que 'z' tenga sólo una dimensión...\n",
    "        #print(f\"\\t\\tz antes de z.view: {z.size()}\") # DEBUG\n",
    "        z = z.view(batch_size, -1)\n",
    "        #print(f\"\\t\\tz después de z.view: {z.size()}\") # DEBUG\n",
    "        \n",
    "        start = timer()\n",
    "        z = self.fc(z)\n",
    "        end = timer()\n",
    "        #print(f\"\\t\\t[time] Capas lineales: time={end-start}s out_size={z.size()}\") # DEBUG\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        # TO-DO: Analizar las múltiples dimensiones en 2 o 3 dimensiones.\n",
    "        #TSNE(n_components=2, verbose=True).fit_transform(x)\n",
    "        \n",
    "        #print(f\"\\tProcesando x: {x.size()}\") # DEBUG\n",
    "        x = self.invididual_process(x)\n",
    "        #print(f\"\\tProcesando y: {y.size()}\") # DEBUG\n",
    "        y = self.invididual_process(y)\n",
    "        \n",
    "        # Para que 'x' y 'y' tengan sólo una dimensión...\n",
    "        x = x.view(batch_size, -1)\n",
    "        #print(f\"\\t\\tDespués de aplanar x: {x.size()}\") # DEBUG\n",
    "        y = y.view(batch_size, -1)\n",
    "        #print(f\"\\t\\tDespués de aplanar y: {y.size()}\") # DEBUG\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Definición del modelo.\n",
    "red = RN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.5104278326034546\n",
      "\tLoss: 0.37603986263275146\n",
      "\tLoss: 0.42956897616386414\n",
      "\tLoss: 0.4163520634174347\n",
      "\tLoss: 0.3564877510070801\n",
      "\tLoss: 0.42469972372055054\n",
      "\tLoss: 0.5005171895027161\n",
      "\tLoss: 0.387552946805954\n",
      "\tLoss: 0.383977472782135\n",
      "\tLoss: 0.42136895656585693\n",
      "\tLoss: 0.3719683885574341\n",
      "\tLoss: 0.38388457894325256\n",
      "\tLoss: 0.3699222803115845\n",
      "\tLoss: 0.35700392723083496\n",
      "\tLoss: 0.3615623116493225\n",
      "\tLoss: 0.28692057728767395\n",
      "\tLoss: 0.43666791915893555\n",
      "\tLoss: 0.2738640308380127\n",
      "\tLoss: 0.3372272849082947\n",
      "\tLoss: 0.36542150378227234\n",
      "\tLoss: 0.3172438442707062\n",
      "\tLoss: 0.36962610483169556\n",
      "\tLoss: 0.317376971244812\n",
      "\tLoss: 0.35005828738212585\n",
      "\tLoss: 0.19330787658691406\n",
      "\tLoss: 0.30530792474746704\n",
      "\tLoss: 0.30429935455322266\n",
      "\tLoss: 0.3655340075492859\n",
      "\tLoss: 0.28312361240386963\n",
      "\tLoss: 0.29692691564559937\n",
      "\tLoss: 0.45289289951324463\n",
      "\tLoss: 0.30020517110824585\n",
      "\tLoss: 0.2759109139442444\n",
      "\tLoss: 0.30584216117858887\n",
      "\tLoss: 0.36821916699409485\n",
      "\tLoss: 0.29941096901893616\n",
      "\tLoss: 0.3831954300403595\n",
      "\tLoss: 0.2959301471710205\n",
      "\tLoss: 0.4049711227416992\n",
      "\tLoss: 0.41304853558540344\n",
      "\tLoss: 0.25969111919403076\n",
      "\tLoss: 0.28901684284210205\n",
      "\tLoss: 0.2670559287071228\n",
      "\tLoss: 0.28451597690582275\n",
      "\tLoss: 0.28649473190307617\n",
      "\tLoss: 0.26525020599365234\n",
      "\tLoss: 0.2788652777671814\n",
      "\tLoss: 0.24554049968719482\n",
      "\tLoss: 0.26319482922554016\n",
      "\tLoss: 0.2237197607755661\n",
      "\tLoss: 0.28437381982803345\n",
      "\tLoss: 0.2616663873195648\n",
      "\tLoss: 0.36810293793678284\n",
      "\tLoss: 0.26006844639778137\n",
      "\tLoss: 0.3150126338005066\n",
      "\tLoss: 0.31707990169525146\n",
      "\tLoss: 0.2758638858795166\n",
      "\tLoss: 0.1959148496389389\n",
      "\tLoss: 0.23912625014781952\n",
      "\tLoss: 0.19190236926078796\n",
      "\tLoss: 0.33203235268592834\n",
      "\tLoss: 0.28443917632102966\n",
      "\tLoss: 0.3279406428337097\n",
      "\tLoss: 0.21226045489311218\n",
      "\tLoss: 0.2741357684135437\n",
      "\tLoss: 0.25556811690330505\n",
      "\tLoss: 0.3254799544811249\n",
      "\tLoss: 0.3809938132762909\n",
      "\tLoss: 0.27132391929626465\n",
      "\tLoss: 0.24568913877010345\n",
      "\tLoss: 0.2975980043411255\n",
      "\tLoss: 0.28365200757980347\n",
      "\tLoss: 0.30366820096969604\n",
      "\tLoss: 0.3253553509712219\n",
      "\tLoss: 0.2552676498889923\n",
      "\tLoss: 0.2939337491989136\n",
      "\tLoss: 0.2371186912059784\n",
      "\tLoss: 0.19828052818775177\n",
      "\tLoss: 0.19211265444755554\n",
      "\tLoss: 0.2544000744819641\n",
      "\tLoss: 0.2581191658973694\n",
      "\tLoss: 0.36142832040786743\n",
      "\tLoss: 0.25083208084106445\n",
      "\tLoss: 0.36486339569091797\n",
      "\tLoss: 0.31458592414855957\n",
      "\tLoss: 0.31027349829673767\n",
      "\tLoss: 0.34221845865249634\n",
      "\tLoss: 0.2395920306444168\n",
      "\tLoss: 0.29003024101257324\n",
      "\tLoss: 0.2637227475643158\n",
      "\tLoss: 0.2597472667694092\n",
      "\tLoss: 0.20532916486263275\n",
      "\tLoss: 0.2541639804840088\n",
      "\tLoss: 0.25114941596984863\n",
      "\tLoss: 0.2626561224460602\n",
      "\tLoss: 0.24305947124958038\n",
      "\tLoss: 0.3482048213481903\n",
      "\tLoss: 0.25132691860198975\n",
      "\tLoss: 0.1862674206495285\n",
      "\tLoss: 0.24759545922279358\n",
      "\tLoss: 0.27355560660362244\n",
      "\tLoss: 0.29538241028785706\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.2744734287261963\n",
      "\tLoss: 0.27007293701171875\n",
      "\tLoss: 0.22759713232517242\n",
      "\tLoss: 0.3397834897041321\n",
      "\tLoss: 0.21564579010009766\n",
      "\tLoss: 0.2496868073940277\n",
      "\tLoss: 0.2361958920955658\n",
      "\tLoss: 0.2778509259223938\n",
      "\tLoss: 0.2123326063156128\n",
      "\tLoss: 0.28032463788986206\n",
      "\tLoss: 0.2868802845478058\n",
      "\tLoss: 0.27910444140434265\n",
      "\tLoss: 0.2325691133737564\n",
      "\tLoss: 0.2114652693271637\n",
      "\tLoss: 0.23782578110694885\n",
      "\tLoss: 0.24131688475608826\n",
      "\tLoss: 0.18484093248844147\n",
      "\tLoss: 0.25490182638168335\n",
      "\tLoss: 0.21393243968486786\n",
      "\tLoss: 0.19862133264541626\n",
      "\tLoss: 0.23752664029598236\n",
      "\tLoss: 0.25775808095932007\n",
      "\tLoss: 0.2544354200363159\n",
      "\tLoss: 0.19039857387542725\n",
      "\tLoss: 0.20122423768043518\n",
      "\tLoss: 0.1995386928319931\n",
      "\tLoss: 0.21772438287734985\n",
      "\tLoss: 0.2830442786216736\n",
      "\tLoss: 0.21267852187156677\n",
      "\tLoss: 0.20203739404678345\n",
      "\tLoss: 0.24112150073051453\n",
      "\tLoss: 0.259147971868515\n",
      "\tLoss: 0.17156563699245453\n",
      "\tLoss: 0.20878157019615173\n",
      "\tLoss: 0.18905484676361084\n",
      "\tLoss: 0.2457064986228943\n",
      "\tLoss: 0.29535433650016785\n",
      "\tLoss: 0.1615038365125656\n",
      "\tLoss: 0.25208553671836853\n",
      "\tLoss: 0.2735328674316406\n",
      "\tLoss: 0.16559317708015442\n",
      "\tLoss: 0.191222682595253\n",
      "\tLoss: 0.22848933935165405\n",
      "\tLoss: 0.2632576823234558\n",
      "\tLoss: 0.19509872794151306\n",
      "\tLoss: 0.22873076796531677\n",
      "\tLoss: 0.14237400889396667\n",
      "\tLoss: 0.23159240186214447\n",
      "\tLoss: 0.23225082457065582\n",
      "\tLoss: 0.2009035348892212\n",
      "\tLoss: 0.2270718812942505\n",
      "\tLoss: 0.23345306515693665\n",
      "\tLoss: 0.2517915368080139\n",
      "\tLoss: 0.22839215397834778\n",
      "\tLoss: 0.25114190578460693\n",
      "\tLoss: 0.2709345519542694\n",
      "\tLoss: 0.3461000919342041\n",
      "\tLoss: 0.2539353370666504\n",
      "\tLoss: 0.20459219813346863\n",
      "\tLoss: 0.2416737675666809\n",
      "\tLoss: 0.2627718448638916\n",
      "\tLoss: 0.20676791667938232\n",
      "\tLoss: 0.13358339667320251\n",
      "\tLoss: 0.2756270170211792\n",
      "\tLoss: 0.16303297877311707\n",
      "\tLoss: 0.20140023529529572\n",
      "\tLoss: 0.2123776525259018\n",
      "\tLoss: 0.1940305233001709\n",
      "\tLoss: 0.18834860622882843\n",
      "\tLoss: 0.2205130159854889\n",
      "\tLoss: 0.26410508155822754\n",
      "\tLoss: 0.2534494400024414\n",
      "\tLoss: 0.19033905863761902\n",
      "\tLoss: 0.257810115814209\n",
      "\tLoss: 0.17592881619930267\n",
      "\tLoss: 0.16572579741477966\n",
      "\tLoss: 0.1658470630645752\n",
      "\tLoss: 0.23071083426475525\n",
      "\tLoss: 0.1755474954843521\n",
      "\tLoss: 0.20917299389839172\n",
      "\tLoss: 0.24017122387886047\n",
      "\tLoss: 0.2182033210992813\n",
      "\tLoss: 0.20738369226455688\n",
      "\tLoss: 0.21443971991539001\n",
      "\tLoss: 0.2302597463130951\n",
      "\tLoss: 0.21485555171966553\n",
      "\tLoss: 0.1808224469423294\n",
      "\tLoss: 0.17233610153198242\n",
      "\tLoss: 0.2168767750263214\n",
      "\tLoss: 0.15205058455467224\n",
      "\tLoss: 0.20373083651065826\n",
      "\tLoss: 0.18234017491340637\n",
      "\tLoss: 0.23827362060546875\n",
      "\tLoss: 0.1870644986629486\n",
      "\tLoss: 0.18811267614364624\n",
      "\tLoss: 0.15622524917125702\n",
      "\tLoss: 0.24954089522361755\n",
      "\tLoss: 0.19230881333351135\n",
      "\tLoss: 0.24978691339492798\n",
      "\tLoss: 0.1757296323776245\n",
      "\tLoss: 0.1319480836391449\n",
      "\tLoss: 0.1954021453857422\n",
      "[time] Epoch 1: 432.4365316387266s = 7.20727552731211m\n",
      "\n",
      "Epoch 2...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.1339845508337021\n",
      "\tLoss: 0.2005344033241272\n",
      "\tLoss: 0.218328058719635\n",
      "\tLoss: 0.24712921679019928\n",
      "\tLoss: 0.23605838418006897\n",
      "\tLoss: 0.20391061902046204\n",
      "\tLoss: 0.10474082082509995\n",
      "\tLoss: 0.22495131194591522\n",
      "\tLoss: 0.1634630262851715\n",
      "\tLoss: 0.1531442105770111\n",
      "\tLoss: 0.21185627579689026\n",
      "\tLoss: 0.20277541875839233\n",
      "\tLoss: 0.16453124582767487\n",
      "\tLoss: 0.1678408980369568\n",
      "\tLoss: 0.12619748711585999\n",
      "\tLoss: 0.25844115018844604\n",
      "\tLoss: 0.21737553179264069\n",
      "\tLoss: 0.2231782078742981\n",
      "\tLoss: 0.27652856707572937\n",
      "\tLoss: 0.3102332353591919\n",
      "\tLoss: 0.2359914779663086\n",
      "\tLoss: 0.19814547896385193\n",
      "\tLoss: 0.2028997838497162\n",
      "\tLoss: 0.17158186435699463\n",
      "\tLoss: 0.219415545463562\n",
      "\tLoss: 0.2553335726261139\n",
      "\tLoss: 0.17963959276676178\n",
      "\tLoss: 0.26711246371269226\n",
      "\tLoss: 0.18854407966136932\n",
      "\tLoss: 0.16853824257850647\n",
      "\tLoss: 0.13564307987689972\n",
      "\tLoss: 0.2399279773235321\n",
      "\tLoss: 0.18374648690223694\n",
      "\tLoss: 0.1677059382200241\n",
      "\tLoss: 0.15478897094726562\n",
      "\tLoss: 0.22260162234306335\n",
      "\tLoss: 0.18797868490219116\n",
      "\tLoss: 0.18678046762943268\n",
      "\tLoss: 0.1776726394891739\n",
      "\tLoss: 0.19352810084819794\n",
      "\tLoss: 0.16085976362228394\n",
      "\tLoss: 0.14177241921424866\n",
      "\tLoss: 0.22304433584213257\n",
      "\tLoss: 0.20380885899066925\n",
      "\tLoss: 0.19183313846588135\n",
      "\tLoss: 0.19743049144744873\n",
      "\tLoss: 0.11546841263771057\n",
      "\tLoss: 0.23285982012748718\n",
      "\tLoss: 0.14718061685562134\n",
      "\tLoss: 0.18613485991954803\n",
      "\tLoss: 0.1685582995414734\n",
      "\tLoss: 0.23150335252285004\n",
      "\tLoss: 0.1841297149658203\n",
      "\tLoss: 0.10277395695447922\n",
      "\tLoss: 0.2570212483406067\n",
      "\tLoss: 0.2107706218957901\n",
      "\tLoss: 0.2362832874059677\n",
      "\tLoss: 0.19061504304409027\n",
      "\tLoss: 0.13461235165596008\n",
      "\tLoss: 0.2086314857006073\n",
      "\tLoss: 0.18718752264976501\n",
      "\tLoss: 0.21291346848011017\n",
      "\tLoss: 0.19944193959236145\n",
      "\tLoss: 0.1912175416946411\n",
      "\tLoss: 0.13546736538410187\n",
      "\tLoss: 0.16535384953022003\n",
      "\tLoss: 0.2386539876461029\n",
      "\tLoss: 0.20724470913410187\n",
      "\tLoss: 0.156194806098938\n",
      "\tLoss: 0.12861593067646027\n",
      "\tLoss: 0.2382591962814331\n",
      "\tLoss: 0.12971842288970947\n",
      "\tLoss: 0.21671631932258606\n",
      "\tLoss: 0.12021344155073166\n",
      "\tLoss: 0.24770605564117432\n",
      "\tLoss: 0.2041703313589096\n",
      "\tLoss: 0.17608284950256348\n",
      "\tLoss: 0.19874262809753418\n",
      "\tLoss: 0.1894957423210144\n",
      "\tLoss: 0.1391526311635971\n",
      "\tLoss: 0.17306555807590485\n",
      "\tLoss: 0.19670632481575012\n",
      "\tLoss: 0.17913255095481873\n",
      "\tLoss: 0.20635464787483215\n",
      "\tLoss: 0.17773549258708954\n",
      "\tLoss: 0.15665209293365479\n",
      "\tLoss: 0.18860366940498352\n",
      "\tLoss: 0.18393480777740479\n",
      "\tLoss: 0.15547707676887512\n",
      "\tLoss: 0.17916688323020935\n",
      "\tLoss: 0.11379724740982056\n",
      "\tLoss: 0.1709665060043335\n",
      "\tLoss: 0.1183914989233017\n",
      "\tLoss: 0.15416038036346436\n",
      "\tLoss: 0.25522157549858093\n",
      "\tLoss: 0.13254982233047485\n",
      "\tLoss: 0.2238127589225769\n",
      "\tLoss: 0.1415780633687973\n",
      "\tLoss: 0.24563543498516083\n",
      "\tLoss: 0.19721175730228424\n",
      "\tLoss: 0.18368926644325256\n",
      "\tLoss: 0.14555029571056366\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.14610043168067932\n",
      "\tLoss: 0.22065433859825134\n",
      "\tLoss: 0.21372219920158386\n",
      "\tLoss: 0.1490364968776703\n",
      "\tLoss: 0.20360735058784485\n",
      "\tLoss: 0.16330455243587494\n",
      "\tLoss: 0.2327820509672165\n",
      "\tLoss: 0.16979211568832397\n",
      "\tLoss: 0.17908990383148193\n",
      "\tLoss: 0.1797792911529541\n",
      "\tLoss: 0.24702854454517365\n",
      "\tLoss: 0.20876936614513397\n",
      "\tLoss: 0.22088408470153809\n",
      "\tLoss: 0.12849605083465576\n",
      "\tLoss: 0.21511048078536987\n",
      "\tLoss: 0.2161790430545807\n",
      "\tLoss: 0.18417233228683472\n",
      "\tLoss: 0.16828025877475739\n",
      "\tLoss: 0.17101126909255981\n",
      "\tLoss: 0.19778208434581757\n",
      "\tLoss: 0.19163116812705994\n",
      "\tLoss: 0.21735531091690063\n",
      "\tLoss: 0.19949404895305634\n",
      "\tLoss: 0.19325700402259827\n",
      "\tLoss: 0.21713346242904663\n",
      "\tLoss: 0.21399512887001038\n",
      "\tLoss: 0.1148914024233818\n",
      "\tLoss: 0.18051716685295105\n",
      "\tLoss: 0.28509920835494995\n",
      "\tLoss: 0.2549557387828827\n",
      "\tLoss: 0.1902901977300644\n",
      "\tLoss: 0.10752987116575241\n",
      "\tLoss: 0.20398736000061035\n",
      "\tLoss: 0.1889992654323578\n",
      "\tLoss: 0.18673689663410187\n",
      "\tLoss: 0.2145305871963501\n",
      "\tLoss: 0.2218153476715088\n",
      "\tLoss: 0.1718045324087143\n",
      "\tLoss: 0.13828614354133606\n",
      "\tLoss: 0.1688733696937561\n",
      "\tLoss: 0.20337751507759094\n",
      "\tLoss: 0.09109322726726532\n",
      "\tLoss: 0.21835210919380188\n",
      "\tLoss: 0.12297820299863815\n",
      "\tLoss: 0.17903774976730347\n",
      "\tLoss: 0.16149196028709412\n",
      "\tLoss: 0.20993448793888092\n",
      "\tLoss: 0.15363763272762299\n",
      "\tLoss: 0.11791524291038513\n",
      "\tLoss: 0.1356034129858017\n",
      "\tLoss: 0.20657579600811005\n",
      "\tLoss: 0.25534510612487793\n",
      "\tLoss: 0.20658496022224426\n",
      "\tLoss: 0.1782473623752594\n",
      "\tLoss: 0.2225172370672226\n",
      "\tLoss: 0.10789000242948532\n",
      "\tLoss: 0.22794827818870544\n",
      "\tLoss: 0.170141339302063\n",
      "\tLoss: 0.11610426008701324\n",
      "\tLoss: 0.10897265374660492\n",
      "\tLoss: 0.1728009432554245\n",
      "\tLoss: 0.26712650060653687\n",
      "\tLoss: 0.1207737848162651\n",
      "\tLoss: 0.19169095158576965\n",
      "\tLoss: 0.20783263444900513\n",
      "\tLoss: 0.14763280749320984\n",
      "\tLoss: 0.13028781116008759\n",
      "\tLoss: 0.2261253148317337\n",
      "\tLoss: 0.14982879161834717\n",
      "\tLoss: 0.15863296389579773\n",
      "\tLoss: 0.17468619346618652\n",
      "\tLoss: 0.23526223003864288\n",
      "\tLoss: 0.14396950602531433\n",
      "\tLoss: 0.0984143316745758\n",
      "\tLoss: 0.21957720816135406\n",
      "\tLoss: 0.1391175389289856\n",
      "\tLoss: 0.20647333562374115\n",
      "\tLoss: 0.18720075488090515\n",
      "\tLoss: 0.20901690423488617\n",
      "\tLoss: 0.17708364129066467\n",
      "\tLoss: 0.20168036222457886\n",
      "\tLoss: 0.175385981798172\n",
      "\tLoss: 0.16605113446712494\n",
      "\tLoss: 0.12287022173404694\n",
      "\tLoss: 0.1456771194934845\n",
      "\tLoss: 0.16754406690597534\n",
      "\tLoss: 0.16289621591567993\n",
      "\tLoss: 0.15810449421405792\n",
      "\tLoss: 0.1549324095249176\n",
      "\tLoss: 0.16073937714099884\n",
      "\tLoss: 0.1861255168914795\n",
      "\tLoss: 0.1358172595500946\n",
      "\tLoss: 0.20692595839500427\n",
      "\tLoss: 0.16610294580459595\n",
      "\tLoss: 0.14444050192832947\n",
      "\tLoss: 0.1517457515001297\n",
      "\tLoss: 0.10128212720155716\n",
      "\tLoss: 0.13410629332065582\n",
      "\tLoss: 0.12903359532356262\n",
      "\tLoss: 0.14765200018882751\n",
      "\tLoss: 0.1570936143398285\n",
      "\tLoss: 0.14278683066368103\n",
      "[time] Epoch 2: 426.4481661170721s = 7.107469435284535m\n",
      "\n",
      "Epoch 3...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.21715842187404633\n",
      "\tLoss: 0.1924518197774887\n",
      "\tLoss: 0.17640838027000427\n",
      "\tLoss: 0.11434197425842285\n",
      "\tLoss: 0.09558890759944916\n",
      "\tLoss: 0.23245948553085327\n",
      "\tLoss: 0.16299939155578613\n",
      "\tLoss: 0.17975234985351562\n",
      "\tLoss: 0.15888053178787231\n",
      "\tLoss: 0.16313536465168\n",
      "\tLoss: 0.07915252447128296\n",
      "\tLoss: 0.17049719393253326\n",
      "\tLoss: 0.2076438069343567\n",
      "\tLoss: 0.1911071538925171\n",
      "\tLoss: 0.15405693650245667\n",
      "\tLoss: 0.14054745435714722\n",
      "\tLoss: 0.18604609370231628\n",
      "\tLoss: 0.19553525745868683\n",
      "\tLoss: 0.1279643476009369\n",
      "\tLoss: 0.2180577516555786\n",
      "\tLoss: 0.13456308841705322\n",
      "\tLoss: 0.17096351087093353\n",
      "\tLoss: 0.26445847749710083\n",
      "\tLoss: 0.22152477502822876\n",
      "\tLoss: 0.231374591588974\n",
      "\tLoss: 0.17192089557647705\n",
      "\tLoss: 0.21381741762161255\n",
      "\tLoss: 0.12902691960334778\n",
      "\tLoss: 0.1327914446592331\n",
      "\tLoss: 0.14266613125801086\n",
      "\tLoss: 0.10445564985275269\n",
      "\tLoss: 0.1776098608970642\n",
      "\tLoss: 0.20146220922470093\n",
      "\tLoss: 0.17149782180786133\n",
      "\tLoss: 0.18120397627353668\n",
      "\tLoss: 0.13426023721694946\n",
      "\tLoss: 0.15799781680107117\n",
      "\tLoss: 0.10203607380390167\n",
      "\tLoss: 0.19492751359939575\n",
      "\tLoss: 0.1797010898590088\n",
      "\tLoss: 0.17959347367286682\n",
      "\tLoss: 0.23901692032814026\n",
      "\tLoss: 0.18588802218437195\n",
      "\tLoss: 0.19070321321487427\n",
      "\tLoss: 0.1684366762638092\n",
      "\tLoss: 0.18548785150051117\n",
      "\tLoss: 0.17866924405097961\n",
      "\tLoss: 0.17696410417556763\n",
      "\tLoss: 0.18558242917060852\n",
      "\tLoss: 0.21036255359649658\n",
      "\tLoss: 0.17333228886127472\n",
      "\tLoss: 0.20122617483139038\n",
      "\tLoss: 0.17841795086860657\n",
      "\tLoss: 0.18035221099853516\n",
      "\tLoss: 0.13682669401168823\n",
      "\tLoss: 0.1923515647649765\n",
      "\tLoss: 0.09701041877269745\n",
      "\tLoss: 0.13395583629608154\n",
      "\tLoss: 0.1282716989517212\n",
      "\tLoss: 0.15241125226020813\n",
      "\tLoss: 0.16178159415721893\n",
      "\tLoss: 0.12806978821754456\n",
      "\tLoss: 0.1176544576883316\n",
      "\tLoss: 0.17138199508190155\n",
      "\tLoss: 0.20315077900886536\n",
      "\tLoss: 0.14729991555213928\n",
      "\tLoss: 0.1727910339832306\n",
      "\tLoss: 0.17744526267051697\n",
      "\tLoss: 0.2481178194284439\n",
      "\tLoss: 0.1562838852405548\n",
      "\tLoss: 0.2142695188522339\n",
      "\tLoss: 0.14156165719032288\n",
      "\tLoss: 0.13233081996440887\n",
      "\tLoss: 0.2236168533563614\n",
      "\tLoss: 0.256904274225235\n",
      "\tLoss: 0.15620943903923035\n",
      "\tLoss: 0.1394025683403015\n",
      "\tLoss: 0.10365764051675797\n",
      "\tLoss: 0.16247069835662842\n",
      "\tLoss: 0.1720988005399704\n",
      "\tLoss: 0.16567324101924896\n",
      "\tLoss: 0.16295313835144043\n",
      "\tLoss: 0.14452260732650757\n",
      "\tLoss: 0.15010222792625427\n",
      "\tLoss: 0.26934945583343506\n",
      "\tLoss: 0.13162444531917572\n",
      "\tLoss: 0.17128099501132965\n",
      "\tLoss: 0.17525920271873474\n",
      "\tLoss: 0.14624840021133423\n",
      "\tLoss: 0.13325640559196472\n",
      "\tLoss: 0.14161276817321777\n",
      "\tLoss: 0.16772842407226562\n",
      "\tLoss: 0.19896483421325684\n",
      "\tLoss: 0.11823007464408875\n",
      "\tLoss: 0.20449912548065186\n",
      "\tLoss: 0.1881536841392517\n",
      "\tLoss: 0.1215614378452301\n",
      "\tLoss: 0.18157660961151123\n",
      "\tLoss: 0.23948988318443298\n",
      "\tLoss: 0.17920586466789246\n",
      "\tLoss: 0.18022991716861725\n",
      "\tLoss: 0.14257486164569855\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.2142259031534195\n",
      "\tLoss: 0.2004266083240509\n",
      "\tLoss: 0.1475200355052948\n",
      "\tLoss: 0.19650524854660034\n",
      "\tLoss: 0.21801164746284485\n",
      "\tLoss: 0.1634635329246521\n",
      "\tLoss: 0.21115326881408691\n",
      "\tLoss: 0.1281438022851944\n",
      "\tLoss: 0.15080656111240387\n",
      "\tLoss: 0.16813167929649353\n",
      "\tLoss: 0.2149088829755783\n",
      "\tLoss: 0.14235275983810425\n",
      "\tLoss: 0.20478308200836182\n",
      "\tLoss: 0.132532998919487\n",
      "\tLoss: 0.18318554759025574\n",
      "\tLoss: 0.1576920598745346\n",
      "\tLoss: 0.19206738471984863\n",
      "\tLoss: 0.20414519309997559\n",
      "\tLoss: 0.2016289234161377\n",
      "\tLoss: 0.2068730741739273\n",
      "\tLoss: 0.12333281338214874\n",
      "\tLoss: 0.2079211324453354\n",
      "\tLoss: 0.1810009777545929\n",
      "\tLoss: 0.1337319016456604\n",
      "\tLoss: 0.11464463919401169\n",
      "\tLoss: 0.13277693092823029\n",
      "\tLoss: 0.1304633617401123\n",
      "\tLoss: 0.14280639588832855\n",
      "\tLoss: 0.19180023670196533\n",
      "\tLoss: 0.10826611518859863\n",
      "\tLoss: 0.09156248718500137\n",
      "\tLoss: 0.18184103071689606\n",
      "\tLoss: 0.1618923842906952\n",
      "\tLoss: 0.1797884851694107\n",
      "\tLoss: 0.14939194917678833\n",
      "\tLoss: 0.15139168500900269\n",
      "\tLoss: 0.1837708055973053\n",
      "\tLoss: 0.1762680560350418\n",
      "\tLoss: 0.14290538430213928\n",
      "\tLoss: 0.22711293399333954\n",
      "\tLoss: 0.1471593827009201\n",
      "\tLoss: 0.1339351087808609\n",
      "\tLoss: 0.21171557903289795\n",
      "\tLoss: 0.10360302776098251\n",
      "\tLoss: 0.15199187397956848\n",
      "\tLoss: 0.14247402548789978\n",
      "\tLoss: 0.21536317467689514\n",
      "\tLoss: 0.14269334077835083\n",
      "\tLoss: 0.19277486205101013\n",
      "\tLoss: 0.115144282579422\n",
      "\tLoss: 0.18248528242111206\n",
      "\tLoss: 0.14760246872901917\n",
      "\tLoss: 0.19068630039691925\n",
      "\tLoss: 0.165851891040802\n",
      "\tLoss: 0.12217673659324646\n",
      "\tLoss: 0.2036592662334442\n",
      "\tLoss: 0.19765527546405792\n",
      "\tLoss: 0.1369979828596115\n",
      "\tLoss: 0.12929323315620422\n",
      "\tLoss: 0.15344524383544922\n",
      "\tLoss: 0.1721040904521942\n",
      "\tLoss: 0.2262885719537735\n",
      "\tLoss: 0.14488697052001953\n",
      "\tLoss: 0.19390593469142914\n",
      "\tLoss: 0.14015085995197296\n",
      "\tLoss: 0.10839186608791351\n",
      "\tLoss: 0.13916265964508057\n",
      "\tLoss: 0.17243430018424988\n",
      "\tLoss: 0.08593831211328506\n",
      "\tLoss: 0.13195425271987915\n",
      "\tLoss: 0.19081002473831177\n",
      "\tLoss: 0.13262765109539032\n",
      "\tLoss: 0.16260680556297302\n",
      "\tLoss: 0.13977709412574768\n",
      "\tLoss: 0.20670495927333832\n",
      "\tLoss: 0.15409965813159943\n",
      "\tLoss: 0.14725804328918457\n",
      "\tLoss: 0.14847131073474884\n",
      "\tLoss: 0.1344202756881714\n",
      "\tLoss: 0.18530896306037903\n",
      "\tLoss: 0.17473867535591125\n",
      "\tLoss: 0.16681289672851562\n",
      "\tLoss: 0.14783789217472076\n",
      "\tLoss: 0.16107603907585144\n",
      "\tLoss: 0.12720277905464172\n",
      "\tLoss: 0.16817983984947205\n",
      "\tLoss: 0.17724478244781494\n",
      "\tLoss: 0.17290687561035156\n",
      "\tLoss: 0.169771209359169\n",
      "\tLoss: 0.13951246440410614\n",
      "\tLoss: 0.194851815700531\n",
      "\tLoss: 0.20059198141098022\n",
      "\tLoss: 0.19260820746421814\n",
      "\tLoss: 0.1642811894416809\n",
      "\tLoss: 0.244235098361969\n",
      "\tLoss: 0.12070110440254211\n",
      "\tLoss: 0.1755794882774353\n",
      "\tLoss: 0.09698379039764404\n",
      "\tLoss: 0.154926136136055\n",
      "\tLoss: 0.15262603759765625\n",
      "\tLoss: 0.17342609167099\n",
      "\tLoss: 0.17138980329036713\n",
      "[time] Epoch 3: 427.4018548191525s = 7.123364246985875m\n",
      "\n",
      "Epoch 4...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.09007994830608368\n",
      "\tLoss: 0.17352570593357086\n",
      "\tLoss: 0.15575312077999115\n",
      "\tLoss: 0.19310179352760315\n",
      "\tLoss: 0.1196603924036026\n",
      "\tLoss: 0.15784993767738342\n",
      "\tLoss: 0.17925621569156647\n",
      "\tLoss: 0.12131921201944351\n",
      "\tLoss: 0.06490922719240189\n",
      "\tLoss: 0.1686943918466568\n",
      "\tLoss: 0.2437310814857483\n",
      "\tLoss: 0.2996256947517395\n",
      "\tLoss: 0.13108213245868683\n",
      "\tLoss: 0.10872049629688263\n",
      "\tLoss: 0.1627185046672821\n",
      "\tLoss: 0.15554627776145935\n",
      "\tLoss: 0.14155232906341553\n",
      "\tLoss: 0.1303141564130783\n",
      "\tLoss: 0.20038671791553497\n",
      "\tLoss: 0.18324516713619232\n",
      "\tLoss: 0.15415096282958984\n",
      "\tLoss: 0.176215261220932\n",
      "\tLoss: 0.20340216159820557\n",
      "\tLoss: 0.10795769840478897\n",
      "\tLoss: 0.1686609536409378\n",
      "\tLoss: 0.12937624752521515\n",
      "\tLoss: 0.1493777185678482\n",
      "\tLoss: 0.14810195565223694\n",
      "\tLoss: 0.1749592274427414\n",
      "\tLoss: 0.1259680837392807\n",
      "\tLoss: 0.16830089688301086\n",
      "\tLoss: 0.1129615381360054\n",
      "\tLoss: 0.157741978764534\n",
      "\tLoss: 0.1102171540260315\n",
      "\tLoss: 0.23288822174072266\n",
      "\tLoss: 0.16891667246818542\n",
      "\tLoss: 0.16034537553787231\n",
      "\tLoss: 0.10450509190559387\n",
      "\tLoss: 0.13997861742973328\n",
      "\tLoss: 0.20526719093322754\n",
      "\tLoss: 0.10988323390483856\n",
      "\tLoss: 0.10136197507381439\n",
      "\tLoss: 0.10632410645484924\n",
      "\tLoss: 0.12470510601997375\n",
      "\tLoss: 0.1845894455909729\n",
      "\tLoss: 0.12107493728399277\n",
      "\tLoss: 0.16935622692108154\n",
      "\tLoss: 0.1745012253522873\n",
      "\tLoss: 0.15721775591373444\n",
      "\tLoss: 0.17458677291870117\n",
      "\tLoss: 0.19244560599327087\n",
      "\tLoss: 0.2234300673007965\n",
      "\tLoss: 0.08274857699871063\n",
      "\tLoss: 0.1912027895450592\n",
      "\tLoss: 0.14892810583114624\n",
      "\tLoss: 0.15429142117500305\n",
      "\tLoss: 0.16547544300556183\n",
      "\tLoss: 0.15453214943408966\n",
      "\tLoss: 0.16258075833320618\n",
      "\tLoss: 0.1607854813337326\n",
      "\tLoss: 0.11380721628665924\n",
      "\tLoss: 0.17510229349136353\n",
      "\tLoss: 0.2074597030878067\n",
      "\tLoss: 0.1656729131937027\n",
      "\tLoss: 0.11275853216648102\n",
      "\tLoss: 0.1331275999546051\n",
      "\tLoss: 0.12453398108482361\n",
      "\tLoss: 0.2073802649974823\n",
      "\tLoss: 0.1118941605091095\n",
      "\tLoss: 0.13807079195976257\n",
      "\tLoss: 0.12591803073883057\n",
      "\tLoss: 0.1769566833972931\n",
      "\tLoss: 0.13398073613643646\n",
      "\tLoss: 0.15439318120479584\n",
      "\tLoss: 0.15883156657218933\n",
      "\tLoss: 0.1458919644355774\n",
      "\tLoss: 0.20368295907974243\n",
      "\tLoss: 0.12792310118675232\n",
      "\tLoss: 0.12881088256835938\n",
      "\tLoss: 0.1910722851753235\n",
      "\tLoss: 0.20971247553825378\n",
      "\tLoss: 0.13818088173866272\n",
      "\tLoss: 0.12465502321720123\n",
      "\tLoss: 0.15341588854789734\n",
      "\tLoss: 0.18108776211738586\n",
      "\tLoss: 0.17072920501232147\n",
      "\tLoss: 0.16633206605911255\n",
      "\tLoss: 0.132280170917511\n",
      "\tLoss: 0.19237355887889862\n",
      "\tLoss: 0.1456376314163208\n",
      "\tLoss: 0.14940083026885986\n",
      "\tLoss: 0.1801082044839859\n",
      "\tLoss: 0.22715315222740173\n",
      "\tLoss: 0.15204980969429016\n",
      "\tLoss: 0.13941092789173126\n",
      "\tLoss: 0.15321703255176544\n",
      "\tLoss: 0.10389740765094757\n",
      "\tLoss: 0.18516051769256592\n",
      "\tLoss: 0.14266742765903473\n",
      "\tLoss: 0.16420282423496246\n",
      "\tLoss: 0.17551645636558533\n",
      "\tLoss: 0.214847594499588\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.10864636301994324\n",
      "\tLoss: 0.13132037222385406\n",
      "\tLoss: 0.08404260873794556\n",
      "\tLoss: 0.13432246446609497\n",
      "\tLoss: 0.16893061995506287\n",
      "\tLoss: 0.13725607097148895\n",
      "\tLoss: 0.16598919034004211\n",
      "\tLoss: 0.10225218534469604\n",
      "\tLoss: 0.1664409637451172\n",
      "\tLoss: 0.11023257672786713\n",
      "\tLoss: 0.17028844356536865\n",
      "\tLoss: 0.14415909349918365\n",
      "\tLoss: 0.17243587970733643\n",
      "\tLoss: 0.17495004832744598\n",
      "\tLoss: 0.19027310609817505\n",
      "\tLoss: 0.11704578995704651\n",
      "\tLoss: 0.1695711314678192\n",
      "\tLoss: 0.10673867911100388\n",
      "\tLoss: 0.13261251151561737\n",
      "\tLoss: 0.12230236828327179\n",
      "\tLoss: 0.1272163689136505\n",
      "\tLoss: 0.20533163845539093\n",
      "\tLoss: 0.18060144782066345\n",
      "\tLoss: 0.1222153902053833\n",
      "\tLoss: 0.164764866232872\n",
      "\tLoss: 0.15907548367977142\n",
      "\tLoss: 0.15956062078475952\n",
      "\tLoss: 0.12678450345993042\n",
      "\tLoss: 0.10622252523899078\n",
      "\tLoss: 0.15775960683822632\n",
      "\tLoss: 0.12276807427406311\n",
      "\tLoss: 0.11816726624965668\n",
      "\tLoss: 0.20251989364624023\n",
      "\tLoss: 0.13879552483558655\n",
      "\tLoss: 0.15789707005023956\n",
      "\tLoss: 0.1700291633605957\n",
      "\tLoss: 0.19660219550132751\n",
      "\tLoss: 0.1432333141565323\n",
      "\tLoss: 0.15356343984603882\n",
      "\tLoss: 0.18943926692008972\n",
      "\tLoss: 0.11392383277416229\n",
      "\tLoss: 0.14689767360687256\n",
      "\tLoss: 0.11794161796569824\n",
      "\tLoss: 0.1959722340106964\n",
      "\tLoss: 0.17078471183776855\n",
      "\tLoss: 0.16158509254455566\n",
      "\tLoss: 0.24227024614810944\n",
      "\tLoss: 0.17671245336532593\n",
      "\tLoss: 0.21438992023468018\n",
      "\tLoss: 0.09120497107505798\n",
      "\tLoss: 0.1667606234550476\n",
      "\tLoss: 0.12353783845901489\n",
      "\tLoss: 0.12694743275642395\n",
      "\tLoss: 0.21675527095794678\n",
      "\tLoss: 0.2218780219554901\n",
      "\tLoss: 0.17274218797683716\n",
      "\tLoss: 0.22618445754051208\n",
      "\tLoss: 0.12849608063697815\n",
      "\tLoss: 0.14102518558502197\n",
      "\tLoss: 0.18307280540466309\n",
      "\tLoss: 0.15201574563980103\n",
      "\tLoss: 0.22832942008972168\n",
      "\tLoss: 0.16123028099536896\n",
      "\tLoss: 0.10871472209692001\n",
      "\tLoss: 0.11494961380958557\n",
      "\tLoss: 0.1160157322883606\n",
      "\tLoss: 0.1892085075378418\n",
      "\tLoss: 0.2177131623029709\n",
      "\tLoss: 0.12195751070976257\n",
      "\tLoss: 0.1485040932893753\n",
      "\tLoss: 0.14058417081832886\n",
      "\tLoss: 0.15582343935966492\n",
      "\tLoss: 0.18091490864753723\n",
      "\tLoss: 0.12239310890436172\n",
      "\tLoss: 0.15036191046237946\n",
      "\tLoss: 0.13563159108161926\n",
      "\tLoss: 0.2417057752609253\n",
      "\tLoss: 0.1824670433998108\n",
      "\tLoss: 0.12490174919366837\n",
      "\tLoss: 0.15195578336715698\n",
      "\tLoss: 0.1419631987810135\n",
      "\tLoss: 0.17798960208892822\n",
      "\tLoss: 0.16767792403697968\n",
      "\tLoss: 0.1169237494468689\n",
      "\tLoss: 0.15396395325660706\n",
      "\tLoss: 0.1478072702884674\n",
      "\tLoss: 0.13989564776420593\n",
      "\tLoss: 0.17765149474143982\n",
      "\tLoss: 0.09563928842544556\n",
      "\tLoss: 0.19336149096488953\n",
      "\tLoss: 0.13229215145111084\n",
      "\tLoss: 0.1370716691017151\n",
      "\tLoss: 0.1357690691947937\n",
      "\tLoss: 0.15461474657058716\n",
      "\tLoss: 0.18921953439712524\n",
      "\tLoss: 0.13309428095817566\n",
      "\tLoss: 0.141532763838768\n",
      "\tLoss: 0.15037164092063904\n",
      "\tLoss: 0.1919238269329071\n",
      "\tLoss: 0.12624868750572205\n",
      "\tLoss: 0.16350454092025757\n",
      "\tLoss: 0.1456805020570755\n",
      "[time] Epoch 4: 433.97246719384566s = 7.232874453230761m\n",
      "\n",
      "Epoch 5...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.1443188488483429\n",
      "\tLoss: 0.1586354374885559\n",
      "\tLoss: 0.07936662435531616\n",
      "\tLoss: 0.1712663769721985\n",
      "\tLoss: 0.1816684901714325\n",
      "\tLoss: 0.11612817645072937\n",
      "\tLoss: 0.19676503539085388\n",
      "\tLoss: 0.08653964847326279\n",
      "\tLoss: 0.1434224545955658\n",
      "\tLoss: 0.1525893211364746\n",
      "\tLoss: 0.11774744093418121\n",
      "\tLoss: 0.15566273033618927\n",
      "\tLoss: 0.16713488101959229\n",
      "\tLoss: 0.15306103229522705\n",
      "\tLoss: 0.14679384231567383\n",
      "\tLoss: 0.17212310433387756\n",
      "\tLoss: 0.19137540459632874\n",
      "\tLoss: 0.10935425758361816\n",
      "\tLoss: 0.14185097813606262\n",
      "\tLoss: 0.17468339204788208\n",
      "\tLoss: 0.14400549232959747\n",
      "\tLoss: 0.14219897985458374\n",
      "\tLoss: 0.1831451803445816\n",
      "\tLoss: 0.22138817608356476\n",
      "\tLoss: 0.10737356543540955\n",
      "\tLoss: 0.13889171183109283\n",
      "\tLoss: 0.12823621928691864\n",
      "\tLoss: 0.15672804415225983\n",
      "\tLoss: 0.18103519082069397\n",
      "\tLoss: 0.13408289849758148\n",
      "\tLoss: 0.2530405819416046\n",
      "\tLoss: 0.19474461674690247\n",
      "\tLoss: 0.22094720602035522\n",
      "\tLoss: 0.1265953779220581\n",
      "\tLoss: 0.18261918425559998\n",
      "\tLoss: 0.18352477252483368\n",
      "\tLoss: 0.1411367654800415\n",
      "\tLoss: 0.15187270939350128\n",
      "\tLoss: 0.1347312331199646\n",
      "\tLoss: 0.1461823433637619\n",
      "\tLoss: 0.1707027554512024\n",
      "\tLoss: 0.1761161983013153\n",
      "\tLoss: 0.15775804221630096\n",
      "\tLoss: 0.11851179599761963\n",
      "\tLoss: 0.09796777367591858\n",
      "\tLoss: 0.15811514854431152\n",
      "\tLoss: 0.20500126481056213\n",
      "\tLoss: 0.22388100624084473\n",
      "\tLoss: 0.2280978560447693\n",
      "\tLoss: 0.17388096451759338\n",
      "\tLoss: 0.1416192650794983\n",
      "\tLoss: 0.12247282266616821\n",
      "\tLoss: 0.1555902659893036\n",
      "\tLoss: 0.17855378985404968\n",
      "\tLoss: 0.15710842609405518\n",
      "\tLoss: 0.15863043069839478\n",
      "\tLoss: 0.18654075264930725\n",
      "\tLoss: 0.22654469311237335\n",
      "\tLoss: 0.16931480169296265\n",
      "\tLoss: 0.15947853028774261\n",
      "\tLoss: 0.1062856912612915\n",
      "\tLoss: 0.18067845702171326\n",
      "\tLoss: 0.15647611021995544\n",
      "\tLoss: 0.15056347846984863\n",
      "\tLoss: 0.17353256046772003\n",
      "\tLoss: 0.140580952167511\n",
      "\tLoss: 0.15513595938682556\n",
      "\tLoss: 0.13468092679977417\n",
      "\tLoss: 0.11853674054145813\n",
      "\tLoss: 0.18759173154830933\n",
      "\tLoss: 0.13070429861545563\n",
      "\tLoss: 0.1911296248435974\n",
      "\tLoss: 0.16007301211357117\n",
      "\tLoss: 0.1219918355345726\n",
      "\tLoss: 0.1850922852754593\n",
      "\tLoss: 0.1245592013001442\n",
      "\tLoss: 0.148832306265831\n",
      "\tLoss: 0.22776919603347778\n",
      "\tLoss: 0.13589154183864594\n",
      "\tLoss: 0.14853766560554504\n",
      "\tLoss: 0.15756027400493622\n",
      "\tLoss: 0.17542438209056854\n",
      "\tLoss: 0.12313259392976761\n",
      "\tLoss: 0.0941566601395607\n",
      "\tLoss: 0.1518779844045639\n",
      "\tLoss: 0.21202358603477478\n",
      "\tLoss: 0.1788036823272705\n",
      "\tLoss: 0.14155006408691406\n",
      "\tLoss: 0.07198859006166458\n",
      "\tLoss: 0.1178433895111084\n",
      "\tLoss: 0.1845768690109253\n",
      "\tLoss: 0.11605184525251389\n",
      "\tLoss: 0.1744140386581421\n",
      "\tLoss: 0.10220664739608765\n",
      "\tLoss: 0.15100456774234772\n",
      "\tLoss: 0.12619441747665405\n",
      "\tLoss: 0.13080313801765442\n",
      "\tLoss: 0.12215040624141693\n",
      "\tLoss: 0.16033312678337097\n",
      "\tLoss: 0.1442963182926178\n",
      "\tLoss: 0.12999437749385834\n",
      "\tLoss: 0.1243901401758194\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.19228297472000122\n",
      "\tLoss: 0.14543233811855316\n",
      "\tLoss: 0.05669108033180237\n",
      "\tLoss: 0.20225588977336884\n",
      "\tLoss: 0.19155409932136536\n",
      "\tLoss: 0.11274689435958862\n",
      "\tLoss: 0.2235143929719925\n",
      "\tLoss: 0.15888404846191406\n",
      "\tLoss: 0.08585710823535919\n",
      "\tLoss: 0.14307913184165955\n",
      "\tLoss: 0.12785036861896515\n",
      "\tLoss: 0.15695035457611084\n",
      "\tLoss: 0.15168577432632446\n",
      "\tLoss: 0.1196148470044136\n",
      "\tLoss: 0.13859990239143372\n",
      "\tLoss: 0.19962549209594727\n",
      "\tLoss: 0.11581593751907349\n",
      "\tLoss: 0.11368615180253983\n",
      "\tLoss: 0.13054731488227844\n",
      "\tLoss: 0.11024294048547745\n",
      "\tLoss: 0.16184085607528687\n",
      "\tLoss: 0.17146676778793335\n",
      "\tLoss: 0.15498261153697968\n",
      "\tLoss: 0.2215801179409027\n",
      "\tLoss: 0.1339411735534668\n",
      "\tLoss: 0.1640414595603943\n",
      "\tLoss: 0.13104869425296783\n",
      "\tLoss: 0.22555586695671082\n",
      "\tLoss: 0.12739449739456177\n",
      "\tLoss: 0.13596346974372864\n",
      "\tLoss: 0.0965707004070282\n",
      "\tLoss: 0.14619281888008118\n",
      "\tLoss: 0.20483183860778809\n",
      "\tLoss: 0.10107304155826569\n",
      "\tLoss: 0.1374208778142929\n",
      "\tLoss: 0.15215978026390076\n",
      "\tLoss: 0.10830777138471603\n",
      "\tLoss: 0.19501811265945435\n",
      "\tLoss: 0.21172408759593964\n",
      "\tLoss: 0.16014805436134338\n",
      "\tLoss: 0.09946863353252411\n",
      "\tLoss: 0.13321201503276825\n",
      "\tLoss: 0.17036904394626617\n",
      "\tLoss: 0.12476738542318344\n",
      "\tLoss: 0.1664806306362152\n",
      "\tLoss: 0.09926512837409973\n",
      "\tLoss: 0.10122969001531601\n",
      "\tLoss: 0.19877725839614868\n",
      "\tLoss: 0.08565215766429901\n",
      "\tLoss: 0.15452389419078827\n",
      "\tLoss: 0.10644544661045074\n",
      "\tLoss: 0.20023216307163239\n",
      "\tLoss: 0.11522388458251953\n",
      "\tLoss: 0.1532100886106491\n",
      "\tLoss: 0.1592763066291809\n",
      "\tLoss: 0.11275427043437958\n",
      "\tLoss: 0.10745657235383987\n",
      "\tLoss: 0.10099788010120392\n",
      "\tLoss: 0.13941633701324463\n",
      "\tLoss: 0.12555494904518127\n",
      "\tLoss: 0.14305365085601807\n",
      "\tLoss: 0.22950875759124756\n",
      "\tLoss: 0.11406326293945312\n",
      "\tLoss: 0.10360821336507797\n",
      "\tLoss: 0.1105356365442276\n",
      "\tLoss: 0.1490870714187622\n",
      "\tLoss: 0.11769977957010269\n",
      "\tLoss: 0.18739411234855652\n",
      "\tLoss: 0.09648875892162323\n",
      "\tLoss: 0.0873054713010788\n",
      "\tLoss: 0.21041393280029297\n",
      "\tLoss: 0.15092822909355164\n",
      "\tLoss: 0.17343038320541382\n",
      "\tLoss: 0.19131943583488464\n",
      "\tLoss: 0.19193392992019653\n",
      "\tLoss: 0.16224852204322815\n",
      "\tLoss: 0.1567903608083725\n",
      "\tLoss: 0.13444311916828156\n",
      "\tLoss: 0.16352105140686035\n",
      "\tLoss: 0.1364491581916809\n",
      "\tLoss: 0.1311245858669281\n",
      "\tLoss: 0.1354045271873474\n",
      "\tLoss: 0.14560732245445251\n",
      "\tLoss: 0.17215478420257568\n",
      "\tLoss: 0.15325625240802765\n",
      "\tLoss: 0.12823908030986786\n",
      "\tLoss: 0.13551390171051025\n",
      "\tLoss: 0.2810572683811188\n",
      "\tLoss: 0.1838548481464386\n",
      "\tLoss: 0.1916946917772293\n",
      "\tLoss: 0.10460223257541656\n",
      "\tLoss: 0.1393667757511139\n",
      "\tLoss: 0.17904561758041382\n",
      "\tLoss: 0.18008384108543396\n",
      "\tLoss: 0.11879721283912659\n",
      "\tLoss: 0.18600109219551086\n",
      "\tLoss: 0.10906653106212616\n",
      "\tLoss: 0.16897165775299072\n",
      "\tLoss: 0.19968578219413757\n",
      "\tLoss: 0.16250070929527283\n",
      "\tLoss: 0.16225579380989075\n",
      "\tLoss: 0.1461535096168518\n",
      "[time] Epoch 5: 439.6794742741622s = 7.327991237902704m\n",
      "\n",
      "Epoch 6...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.148191899061203\n",
      "\tLoss: 0.1413985788822174\n",
      "\tLoss: 0.1144353449344635\n",
      "\tLoss: 0.13476070761680603\n",
      "\tLoss: 0.12159522622823715\n",
      "\tLoss: 0.1021634191274643\n",
      "\tLoss: 0.14853280782699585\n",
      "\tLoss: 0.11234292387962341\n",
      "\tLoss: 0.14443260431289673\n",
      "\tLoss: 0.13138563930988312\n",
      "\tLoss: 0.11179527640342712\n",
      "\tLoss: 0.1497105062007904\n",
      "\tLoss: 0.15551236271858215\n",
      "\tLoss: 0.18606936931610107\n",
      "\tLoss: 0.07801011949777603\n",
      "\tLoss: 0.13739997148513794\n",
      "\tLoss: 0.12881305813789368\n",
      "\tLoss: 0.09500491619110107\n",
      "\tLoss: 0.12375327944755554\n",
      "\tLoss: 0.09901732206344604\n",
      "\tLoss: 0.13368645310401917\n",
      "\tLoss: 0.15543581545352936\n",
      "\tLoss: 0.16448746621608734\n",
      "\tLoss: 0.12370051443576813\n",
      "\tLoss: 0.16010528802871704\n",
      "\tLoss: 0.10546872019767761\n",
      "\tLoss: 0.15763112902641296\n",
      "\tLoss: 0.11593979597091675\n",
      "\tLoss: 0.17526227235794067\n",
      "\tLoss: 0.12086556851863861\n",
      "\tLoss: 0.18101182579994202\n",
      "\tLoss: 0.10465766489505768\n",
      "\tLoss: 0.1703508496284485\n",
      "\tLoss: 0.15960168838500977\n",
      "\tLoss: 0.14598751068115234\n",
      "\tLoss: 0.12728382647037506\n",
      "\tLoss: 0.21324238181114197\n",
      "\tLoss: 0.15730316936969757\n",
      "\tLoss: 0.1906260848045349\n",
      "\tLoss: 0.18524521589279175\n",
      "\tLoss: 0.18380405008792877\n",
      "\tLoss: 0.12375183403491974\n",
      "\tLoss: 0.1408892273902893\n",
      "\tLoss: 0.19775402545928955\n",
      "\tLoss: 0.17640900611877441\n",
      "\tLoss: 0.1419794112443924\n",
      "\tLoss: 0.17694184184074402\n",
      "\tLoss: 0.1684192717075348\n",
      "\tLoss: 0.1338612586259842\n",
      "\tLoss: 0.07471813261508942\n",
      "\tLoss: 0.12886211276054382\n",
      "\tLoss: 0.19811156392097473\n",
      "\tLoss: 0.09307921677827835\n",
      "\tLoss: 0.1602603942155838\n",
      "\tLoss: 0.11850956827402115\n",
      "\tLoss: 0.16876913607120514\n",
      "\tLoss: 0.18739622831344604\n",
      "\tLoss: 0.09264203906059265\n",
      "\tLoss: 0.1100345253944397\n",
      "\tLoss: 0.16100454330444336\n",
      "\tLoss: 0.2334870994091034\n",
      "\tLoss: 0.2038027048110962\n",
      "\tLoss: 0.1822332739830017\n",
      "\tLoss: 0.15038901567459106\n",
      "\tLoss: 0.17450197041034698\n",
      "\tLoss: 0.15668359398841858\n",
      "\tLoss: 0.18314304947853088\n",
      "\tLoss: 0.13746562600135803\n",
      "\tLoss: 0.10383749008178711\n",
      "\tLoss: 0.06876257061958313\n",
      "\tLoss: 0.13810467720031738\n",
      "\tLoss: 0.18218567967414856\n",
      "\tLoss: 0.09824849665164948\n",
      "\tLoss: 0.10458129644393921\n",
      "\tLoss: 0.12281359732151031\n",
      "\tLoss: 0.16948679089546204\n",
      "\tLoss: 0.12719887495040894\n",
      "\tLoss: 0.13087207078933716\n",
      "\tLoss: 0.13394491374492645\n",
      "\tLoss: 0.12856245040893555\n",
      "\tLoss: 0.12707893550395966\n",
      "\tLoss: 0.1307590752840042\n",
      "\tLoss: 0.15681031346321106\n",
      "\tLoss: 0.07138211280107498\n",
      "\tLoss: 0.11884668469429016\n",
      "\tLoss: 0.13938988745212555\n",
      "\tLoss: 0.12051267176866531\n",
      "\tLoss: 0.17941036820411682\n",
      "\tLoss: 0.04572794586420059\n",
      "\tLoss: 0.10927009582519531\n",
      "\tLoss: 0.11791203171014786\n",
      "\tLoss: 0.13868524134159088\n",
      "\tLoss: 0.12482477724552155\n",
      "\tLoss: 0.09885993599891663\n",
      "\tLoss: 0.14620152115821838\n",
      "\tLoss: 0.15166501700878143\n",
      "\tLoss: 0.048452120274305344\n",
      "\tLoss: 0.14391019940376282\n",
      "\tLoss: 0.1629488617181778\n",
      "\tLoss: 0.14623934030532837\n",
      "\tLoss: 0.2564696967601776\n",
      "\tLoss: 0.11464535444974899\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.1339598298072815\n",
      "\tLoss: 0.10226687788963318\n",
      "\tLoss: 0.2153032124042511\n",
      "\tLoss: 0.20338863134384155\n",
      "\tLoss: 0.19371679425239563\n",
      "\tLoss: 0.13621854782104492\n",
      "\tLoss: 0.1450059711933136\n",
      "\tLoss: 0.11088252067565918\n",
      "\tLoss: 0.17449519038200378\n",
      "\tLoss: 0.09474574029445648\n",
      "\tLoss: 0.1423076093196869\n",
      "\tLoss: 0.11998901516199112\n",
      "\tLoss: 0.14500701427459717\n",
      "\tLoss: 0.10188958793878555\n",
      "\tLoss: 0.12443701922893524\n",
      "\tLoss: 0.2020079791545868\n",
      "\tLoss: 0.20335859060287476\n",
      "\tLoss: 0.1736241579055786\n",
      "\tLoss: 0.13323959708213806\n",
      "\tLoss: 0.1367289423942566\n",
      "\tLoss: 0.13466858863830566\n",
      "\tLoss: 0.10355406254529953\n",
      "\tLoss: 0.11278028041124344\n",
      "\tLoss: 0.18570011854171753\n",
      "\tLoss: 0.17162005603313446\n",
      "\tLoss: 0.14594250917434692\n",
      "\tLoss: 0.15261809527873993\n",
      "\tLoss: 0.13820955157279968\n",
      "\tLoss: 0.11636389046907425\n",
      "\tLoss: 0.19731032848358154\n",
      "\tLoss: 0.15442749857902527\n",
      "\tLoss: 0.14445239305496216\n",
      "\tLoss: 0.08775439113378525\n",
      "\tLoss: 0.19359426200389862\n",
      "\tLoss: 0.15571185946464539\n",
      "\tLoss: 0.12875811755657196\n",
      "\tLoss: 0.22323159873485565\n",
      "\tLoss: 0.14827246963977814\n",
      "\tLoss: 0.19217725098133087\n",
      "\tLoss: 0.17535988986492157\n",
      "\tLoss: 0.11408914625644684\n",
      "\tLoss: 0.16381561756134033\n",
      "\tLoss: 0.12776324152946472\n",
      "\tLoss: 0.17463931441307068\n",
      "\tLoss: 0.10933609306812286\n",
      "\tLoss: 0.12453053891658783\n",
      "\tLoss: 0.18703123927116394\n",
      "\tLoss: 0.14811009168624878\n",
      "\tLoss: 0.1674267053604126\n",
      "\tLoss: 0.1171235740184784\n",
      "\tLoss: 0.13045398890972137\n",
      "\tLoss: 0.17641106247901917\n",
      "\tLoss: 0.10505789518356323\n",
      "\tLoss: 0.16785717010498047\n",
      "\tLoss: 0.1985563188791275\n",
      "\tLoss: 0.19584372639656067\n",
      "\tLoss: 0.1730974018573761\n",
      "\tLoss: 0.18366578221321106\n",
      "\tLoss: 0.13155779242515564\n",
      "\tLoss: 0.12324005365371704\n",
      "\tLoss: 0.14901113510131836\n",
      "\tLoss: 0.15186768770217896\n",
      "\tLoss: 0.18418502807617188\n",
      "\tLoss: 0.10342562198638916\n",
      "\tLoss: 0.10169114172458649\n",
      "\tLoss: 0.1780797392129898\n",
      "\tLoss: 0.19609302282333374\n",
      "\tLoss: 0.1461680382490158\n",
      "\tLoss: 0.11394540965557098\n",
      "\tLoss: 0.16474223136901855\n",
      "\tLoss: 0.16851499676704407\n",
      "\tLoss: 0.12181299924850464\n",
      "\tLoss: 0.09189940989017487\n",
      "\tLoss: 0.17966897785663605\n",
      "\tLoss: 0.1748298853635788\n",
      "\tLoss: 0.1559242308139801\n",
      "\tLoss: 0.11312104761600494\n",
      "\tLoss: 0.1285276710987091\n",
      "\tLoss: 0.18055769801139832\n",
      "\tLoss: 0.16215716302394867\n",
      "\tLoss: 0.16894946992397308\n",
      "\tLoss: 0.11183330416679382\n",
      "\tLoss: 0.1303916722536087\n",
      "\tLoss: 0.0574786439538002\n",
      "\tLoss: 0.16097164154052734\n",
      "\tLoss: 0.148128479719162\n",
      "\tLoss: 0.14768758416175842\n",
      "\tLoss: 0.11228533089160919\n",
      "\tLoss: 0.12570720911026\n",
      "\tLoss: 0.1450396329164505\n",
      "\tLoss: 0.15602415800094604\n",
      "\tLoss: 0.15262198448181152\n",
      "\tLoss: 0.11878647655248642\n",
      "\tLoss: 0.12664036452770233\n",
      "\tLoss: 0.14865803718566895\n",
      "\tLoss: 0.1670134961605072\n",
      "\tLoss: 0.13850325345993042\n",
      "\tLoss: 0.13847172260284424\n",
      "\tLoss: 0.12676659226417542\n",
      "\tLoss: 0.12409687787294388\n",
      "\tLoss: 0.07336576282978058\n",
      "\tLoss: 0.1720922291278839\n",
      "[time] Epoch 6: 432.55114379990846s = 7.209185729998475m\n",
      "\n",
      "Epoch 7...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.07173885405063629\n",
      "\tLoss: 0.1495084911584854\n",
      "\tLoss: 0.15475904941558838\n",
      "\tLoss: 0.0916622057557106\n",
      "\tLoss: 0.13695909082889557\n",
      "\tLoss: 0.19341421127319336\n",
      "\tLoss: 0.12171906977891922\n",
      "\tLoss: 0.12338264286518097\n",
      "\tLoss: 0.10146375000476837\n",
      "\tLoss: 0.13003897666931152\n",
      "\tLoss: 0.10574183613061905\n",
      "\tLoss: 0.19583050906658173\n",
      "\tLoss: 0.17437148094177246\n",
      "\tLoss: 0.13652601838111877\n",
      "\tLoss: 0.11378593742847443\n",
      "\tLoss: 0.12797129154205322\n",
      "\tLoss: 0.1049860268831253\n",
      "\tLoss: 0.22431552410125732\n",
      "\tLoss: 0.23574024438858032\n",
      "\tLoss: 0.11583057045936584\n",
      "\tLoss: 0.15162435173988342\n",
      "\tLoss: 0.2181369662284851\n",
      "\tLoss: 0.09852941334247589\n",
      "\tLoss: 0.11057937890291214\n",
      "\tLoss: 0.13150407373905182\n",
      "\tLoss: 0.10412606596946716\n",
      "\tLoss: 0.1746065616607666\n",
      "\tLoss: 0.0814315676689148\n",
      "\tLoss: 0.14499585330486298\n",
      "\tLoss: 0.1638774573802948\n",
      "\tLoss: 0.13478241860866547\n",
      "\tLoss: 0.2115333378314972\n",
      "\tLoss: 0.11673817038536072\n",
      "\tLoss: 0.15778189897537231\n",
      "\tLoss: 0.14794036746025085\n",
      "\tLoss: 0.3080134987831116\n",
      "\tLoss: 0.23712749779224396\n",
      "\tLoss: 0.12216312438249588\n",
      "\tLoss: 0.17298977077007294\n",
      "\tLoss: 0.18771277368068695\n",
      "\tLoss: 0.103443443775177\n",
      "\tLoss: 0.17237800359725952\n",
      "\tLoss: 0.14728540182113647\n",
      "\tLoss: 0.1504679024219513\n",
      "\tLoss: 0.11556540429592133\n",
      "\tLoss: 0.09382568299770355\n",
      "\tLoss: 0.16754841804504395\n",
      "\tLoss: 0.14665833115577698\n",
      "\tLoss: 0.15687912702560425\n",
      "\tLoss: 0.18044783174991608\n",
      "\tLoss: 0.08332556486129761\n",
      "\tLoss: 0.14917974174022675\n",
      "\tLoss: 0.13567182421684265\n",
      "\tLoss: 0.19894252717494965\n",
      "\tLoss: 0.1770656704902649\n",
      "\tLoss: 0.13420632481575012\n",
      "\tLoss: 0.12465306371450424\n",
      "\tLoss: 0.11453051120042801\n",
      "\tLoss: 0.12441916018724442\n",
      "\tLoss: 0.17164725065231323\n",
      "\tLoss: 0.159598708152771\n",
      "\tLoss: 0.16704660654067993\n",
      "\tLoss: 0.16305087506771088\n",
      "\tLoss: 0.09468816965818405\n",
      "\tLoss: 0.17804038524627686\n",
      "\tLoss: 0.1201024204492569\n",
      "\tLoss: 0.13183914124965668\n",
      "\tLoss: 0.1829117238521576\n",
      "\tLoss: 0.12206342816352844\n",
      "\tLoss: 0.16835838556289673\n",
      "\tLoss: 0.11634959280490875\n",
      "\tLoss: 0.20368003845214844\n",
      "\tLoss: 0.20038814842700958\n",
      "\tLoss: 0.11056447774171829\n",
      "\tLoss: 0.15012815594673157\n",
      "\tLoss: 0.10170969367027283\n",
      "\tLoss: 0.1538994014263153\n",
      "\tLoss: 0.1386355608701706\n",
      "\tLoss: 0.1745426058769226\n",
      "\tLoss: 0.12836015224456787\n",
      "\tLoss: 0.10014212876558304\n",
      "\tLoss: 0.1340527981519699\n",
      "\tLoss: 0.09667143225669861\n",
      "\tLoss: 0.2211051732301712\n",
      "\tLoss: 0.11398565769195557\n",
      "\tLoss: 0.15610936284065247\n",
      "\tLoss: 0.07084689289331436\n",
      "\tLoss: 0.1257399618625641\n",
      "\tLoss: 0.1009741947054863\n",
      "\tLoss: 0.2122325897216797\n",
      "\tLoss: 0.08304397016763687\n",
      "\tLoss: 0.13351500034332275\n",
      "\tLoss: 0.1124865934252739\n",
      "\tLoss: 0.1722181737422943\n",
      "\tLoss: 0.21725738048553467\n",
      "\tLoss: 0.17089509963989258\n",
      "\tLoss: 0.11796523630619049\n",
      "\tLoss: 0.12941278517246246\n",
      "\tLoss: 0.16076235473155975\n",
      "\tLoss: 0.13117149472236633\n",
      "\tLoss: 0.15919682383537292\n",
      "\tLoss: 0.16699326038360596\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.12552675604820251\n",
      "\tLoss: 0.17536036670207977\n",
      "\tLoss: 0.19049468636512756\n",
      "\tLoss: 0.15993618965148926\n",
      "\tLoss: 0.14603179693222046\n",
      "\tLoss: 0.1355818510055542\n",
      "\tLoss: 0.20580023527145386\n",
      "\tLoss: 0.16135945916175842\n",
      "\tLoss: 0.06881134957075119\n",
      "\tLoss: 0.16370518505573273\n",
      "\tLoss: 0.1537570357322693\n",
      "\tLoss: 0.172105610370636\n",
      "\tLoss: 0.09483227133750916\n",
      "\tLoss: 0.16096344590187073\n",
      "\tLoss: 0.11769535392522812\n",
      "\tLoss: 0.13313281536102295\n",
      "\tLoss: 0.1545119732618332\n",
      "\tLoss: 0.11191436648368835\n",
      "\tLoss: 0.0925765112042427\n",
      "\tLoss: 0.12423963099718094\n",
      "\tLoss: 0.11561794579029083\n",
      "\tLoss: 0.09623326361179352\n",
      "\tLoss: 0.1121831014752388\n",
      "\tLoss: 0.12591804563999176\n",
      "\tLoss: 0.09788447618484497\n",
      "\tLoss: 0.14984816312789917\n",
      "\tLoss: 0.2166166603565216\n",
      "\tLoss: 0.18133991956710815\n",
      "\tLoss: 0.1726723611354828\n",
      "\tLoss: 0.12479673326015472\n",
      "\tLoss: 0.16917471587657928\n",
      "\tLoss: 0.14796048402786255\n",
      "\tLoss: 0.16982340812683105\n",
      "\tLoss: 0.20352721214294434\n",
      "\tLoss: 0.18809717893600464\n",
      "\tLoss: 0.14191934466362\n",
      "\tLoss: 0.12969344854354858\n",
      "\tLoss: 0.17581912875175476\n",
      "\tLoss: 0.15482985973358154\n",
      "\tLoss: 0.16951444745063782\n",
      "\tLoss: 0.13807040452957153\n",
      "\tLoss: 0.161306694149971\n",
      "\tLoss: 0.17912578582763672\n",
      "\tLoss: 0.14696110785007477\n",
      "\tLoss: 0.1652313768863678\n",
      "\tLoss: 0.14121896028518677\n",
      "\tLoss: 0.20211124420166016\n",
      "\tLoss: 0.20077553391456604\n",
      "\tLoss: 0.13890905678272247\n",
      "\tLoss: 0.12790338695049286\n",
      "\tLoss: 0.14649304747581482\n",
      "\tLoss: 0.13250687718391418\n",
      "\tLoss: 0.1309186816215515\n",
      "\tLoss: 0.17085936665534973\n",
      "\tLoss: 0.21610373258590698\n",
      "\tLoss: 0.14979435503482819\n",
      "\tLoss: 0.1526591032743454\n",
      "\tLoss: 0.19200114905834198\n",
      "\tLoss: 0.15210184454917908\n",
      "\tLoss: 0.23607292771339417\n",
      "\tLoss: 0.12918153405189514\n",
      "\tLoss: 0.10850758105516434\n",
      "\tLoss: 0.11125677824020386\n",
      "\tLoss: 0.15769432485103607\n",
      "\tLoss: 0.15489396452903748\n",
      "\tLoss: 0.08714522421360016\n",
      "\tLoss: 0.09084102511405945\n",
      "\tLoss: 0.13251416385173798\n",
      "\tLoss: 0.17560914158821106\n",
      "\tLoss: 0.1876191347837448\n",
      "\tLoss: 0.1305340826511383\n",
      "\tLoss: 0.13378609716892242\n",
      "\tLoss: 0.128823921084404\n",
      "\tLoss: 0.22855274379253387\n",
      "\tLoss: 0.14216019213199615\n",
      "\tLoss: 0.13991162180900574\n",
      "\tLoss: 0.1573062241077423\n",
      "\tLoss: 0.10326527059078217\n",
      "\tLoss: 0.14384722709655762\n",
      "\tLoss: 0.17003779113292694\n",
      "\tLoss: 0.15364065766334534\n",
      "\tLoss: 0.1478814035654068\n",
      "\tLoss: 0.1256929486989975\n",
      "\tLoss: 0.1577209085226059\n",
      "\tLoss: 0.12795789539813995\n",
      "\tLoss: 0.17502833902835846\n",
      "\tLoss: 0.18559402227401733\n",
      "\tLoss: 0.09043265879154205\n",
      "\tLoss: 0.15317018330097198\n",
      "\tLoss: 0.12151683866977692\n",
      "\tLoss: 0.16990956664085388\n",
      "\tLoss: 0.10890518128871918\n",
      "\tLoss: 0.16713875532150269\n",
      "\tLoss: 0.10544681549072266\n",
      "\tLoss: 0.09941370785236359\n",
      "\tLoss: 0.12347062677145004\n",
      "\tLoss: 0.1708609163761139\n",
      "\tLoss: 0.16237980127334595\n",
      "\tLoss: 0.2044602930545807\n",
      "\tLoss: 0.16862091422080994\n",
      "\tLoss: 0.20481163263320923\n",
      "\tLoss: 0.17260494828224182\n",
      "[time] Epoch 7: 428.5768427993171s = 7.142947379988618m\n",
      "\n",
      "Epoch 8...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.11261965334415436\n",
      "\tLoss: 0.10803309082984924\n",
      "\tLoss: 0.1399170458316803\n",
      "\tLoss: 0.246892511844635\n",
      "\tLoss: 0.09151789546012878\n",
      "\tLoss: 0.15575392544269562\n",
      "\tLoss: 0.105965256690979\n",
      "\tLoss: 0.0743718147277832\n",
      "\tLoss: 0.16119995713233948\n",
      "\tLoss: 0.12507042288780212\n",
      "\tLoss: 0.1679505705833435\n",
      "\tLoss: 0.1923465132713318\n",
      "\tLoss: 0.14096654951572418\n",
      "\tLoss: 0.12308429181575775\n",
      "\tLoss: 0.1505562663078308\n",
      "\tLoss: 0.15999621152877808\n",
      "\tLoss: 0.13800181448459625\n",
      "\tLoss: 0.11865897476673126\n",
      "\tLoss: 0.18201085925102234\n",
      "\tLoss: 0.15273146331310272\n",
      "\tLoss: 0.1916789412498474\n",
      "\tLoss: 0.18776938319206238\n",
      "\tLoss: 0.1372569352388382\n",
      "\tLoss: 0.1545129120349884\n",
      "\tLoss: 0.0956941545009613\n",
      "\tLoss: 0.16387391090393066\n",
      "\tLoss: 0.11063575744628906\n",
      "\tLoss: 0.11076445132493973\n",
      "\tLoss: 0.1573077142238617\n",
      "\tLoss: 0.16621538996696472\n",
      "\tLoss: 0.09498944878578186\n",
      "\tLoss: 0.17848631739616394\n",
      "\tLoss: 0.1336526870727539\n",
      "\tLoss: 0.1657387912273407\n",
      "\tLoss: 0.11802460253238678\n",
      "\tLoss: 0.1103326827287674\n",
      "\tLoss: 0.19325830042362213\n",
      "\tLoss: 0.18004518747329712\n",
      "\tLoss: 0.11733923852443695\n",
      "\tLoss: 0.10826016217470169\n",
      "\tLoss: 0.10129725188016891\n",
      "\tLoss: 0.1015508845448494\n",
      "\tLoss: 0.09963710606098175\n",
      "\tLoss: 0.17114953696727753\n",
      "\tLoss: 0.15194928646087646\n",
      "\tLoss: 0.15271729230880737\n",
      "\tLoss: 0.10910089313983917\n",
      "\tLoss: 0.1591828465461731\n",
      "\tLoss: 0.12321066111326218\n",
      "\tLoss: 0.15365546941757202\n",
      "\tLoss: 0.16417059302330017\n",
      "\tLoss: 0.08716265112161636\n",
      "\tLoss: 0.10431020706892014\n",
      "\tLoss: 0.15294641256332397\n",
      "\tLoss: 0.14746931195259094\n",
      "\tLoss: 0.12004537135362625\n",
      "\tLoss: 0.15363585948944092\n",
      "\tLoss: 0.21589291095733643\n",
      "\tLoss: 0.1112636849284172\n",
      "\tLoss: 0.19560976326465607\n",
      "\tLoss: 0.14733625948429108\n",
      "\tLoss: 0.09432460367679596\n",
      "\tLoss: 0.11706390231847763\n",
      "\tLoss: 0.18462839722633362\n",
      "\tLoss: 0.1788184940814972\n",
      "\tLoss: 0.1028246283531189\n",
      "\tLoss: 0.0925116315484047\n",
      "\tLoss: 0.08777077496051788\n",
      "\tLoss: 0.12628740072250366\n",
      "\tLoss: 0.11883808672428131\n",
      "\tLoss: 0.19717083871364594\n",
      "\tLoss: 0.09568676352500916\n",
      "\tLoss: 0.101490318775177\n",
      "\tLoss: 0.1476321816444397\n",
      "\tLoss: 0.14232730865478516\n",
      "\tLoss: 0.07620574533939362\n",
      "\tLoss: 0.17601348459720612\n",
      "\tLoss: 0.09642095863819122\n",
      "\tLoss: 0.16110335290431976\n",
      "\tLoss: 0.12158501148223877\n",
      "\tLoss: 0.16128449141979218\n",
      "\tLoss: 0.0709434226155281\n",
      "\tLoss: 0.1867193579673767\n",
      "\tLoss: 0.15030232071876526\n",
      "\tLoss: 0.13395428657531738\n",
      "\tLoss: 0.14707404375076294\n",
      "\tLoss: 0.1955910623073578\n",
      "\tLoss: 0.1400374472141266\n",
      "\tLoss: 0.10389368236064911\n",
      "\tLoss: 0.11441662907600403\n",
      "\tLoss: 0.16677483916282654\n",
      "\tLoss: 0.12431420385837555\n",
      "\tLoss: 0.1517925262451172\n",
      "\tLoss: 0.157478466629982\n",
      "\tLoss: 0.10162626206874847\n",
      "\tLoss: 0.15159441530704498\n",
      "\tLoss: 0.175648495554924\n",
      "\tLoss: 0.1390131264925003\n",
      "\tLoss: 0.12967798113822937\n",
      "\tLoss: 0.15257109701633453\n",
      "\tLoss: 0.20354606211185455\n",
      "\tLoss: 0.1436956822872162\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.16288438439369202\n",
      "\tLoss: 0.23729358613491058\n",
      "\tLoss: 0.12731340527534485\n",
      "\tLoss: 0.15375173091888428\n",
      "\tLoss: 0.16200201213359833\n",
      "\tLoss: 0.10976586490869522\n",
      "\tLoss: 0.16998796164989471\n",
      "\tLoss: 0.14011558890342712\n",
      "\tLoss: 0.108940988779068\n",
      "\tLoss: 0.158626526594162\n",
      "\tLoss: 0.12352222204208374\n",
      "\tLoss: 0.19999462366104126\n",
      "\tLoss: 0.17762812972068787\n",
      "\tLoss: 0.09760904312133789\n",
      "\tLoss: 0.16024354100227356\n",
      "\tLoss: 0.15048488974571228\n",
      "\tLoss: 0.0899352952837944\n",
      "\tLoss: 0.13677328824996948\n",
      "\tLoss: 0.14303573966026306\n",
      "\tLoss: 0.0909145399928093\n",
      "\tLoss: 0.1598621904850006\n",
      "\tLoss: 0.12215477228164673\n",
      "\tLoss: 0.1616438329219818\n",
      "\tLoss: 0.17090804874897003\n",
      "\tLoss: 0.11244925856590271\n",
      "\tLoss: 0.11390504240989685\n",
      "\tLoss: 0.12864121794700623\n",
      "\tLoss: 0.1627763956785202\n",
      "\tLoss: 0.11568780243396759\n",
      "\tLoss: 0.13045050203800201\n",
      "\tLoss: 0.15454772114753723\n",
      "\tLoss: 0.15827932953834534\n",
      "\tLoss: 0.10694712400436401\n",
      "\tLoss: 0.13814115524291992\n",
      "\tLoss: 0.12113736569881439\n",
      "\tLoss: 0.17102766036987305\n",
      "\tLoss: 0.09989260882139206\n",
      "\tLoss: 0.09688952565193176\n",
      "\tLoss: 0.1319202035665512\n",
      "\tLoss: 0.12565889954566956\n",
      "\tLoss: 0.19110450148582458\n",
      "\tLoss: 0.13873203098773956\n",
      "\tLoss: 0.13445337116718292\n",
      "\tLoss: 0.10066135227680206\n",
      "\tLoss: 0.13034804165363312\n",
      "\tLoss: 0.13876882195472717\n",
      "\tLoss: 0.15242528915405273\n",
      "\tLoss: 0.08654895424842834\n",
      "\tLoss: 0.15511012077331543\n",
      "\tLoss: 0.15702804923057556\n",
      "\tLoss: 0.14753097295761108\n",
      "\tLoss: 0.1676812618970871\n",
      "\tLoss: 0.09966453909873962\n",
      "\tLoss: 0.18600678443908691\n",
      "\tLoss: 0.15752851963043213\n",
      "\tLoss: 0.1038224846124649\n",
      "\tLoss: 0.20512676239013672\n",
      "\tLoss: 0.143576979637146\n",
      "\tLoss: 0.13821446895599365\n",
      "\tLoss: 0.14294399321079254\n",
      "\tLoss: 0.14673514664173126\n",
      "\tLoss: 0.1405159831047058\n",
      "\tLoss: 0.1254206895828247\n",
      "\tLoss: 0.12397458404302597\n",
      "\tLoss: 0.1419377326965332\n",
      "\tLoss: 0.1388055384159088\n",
      "\tLoss: 0.14743778109550476\n",
      "\tLoss: 0.16059157252311707\n",
      "\tLoss: 0.1065470427274704\n",
      "\tLoss: 0.13008013367652893\n",
      "\tLoss: 0.16003571450710297\n",
      "\tLoss: 0.17289316654205322\n",
      "\tLoss: 0.1297052949666977\n",
      "\tLoss: 0.17959126830101013\n",
      "\tLoss: 0.12838585674762726\n",
      "\tLoss: 0.12814518809318542\n",
      "\tLoss: 0.150706946849823\n",
      "\tLoss: 0.11663499474525452\n",
      "\tLoss: 0.12635555863380432\n",
      "\tLoss: 0.14948968589305878\n",
      "\tLoss: 0.14729750156402588\n",
      "\tLoss: 0.1427074670791626\n",
      "\tLoss: 0.11378492414951324\n",
      "\tLoss: 0.14900530874729156\n",
      "\tLoss: 0.165281742811203\n",
      "\tLoss: 0.168514221906662\n",
      "\tLoss: 0.15022431313991547\n",
      "\tLoss: 0.15278884768486023\n",
      "\tLoss: 0.0885130912065506\n",
      "\tLoss: 0.17921219766139984\n",
      "\tLoss: 0.14034797251224518\n",
      "\tLoss: 0.13218142092227936\n",
      "\tLoss: 0.08152510225772858\n",
      "\tLoss: 0.1373736560344696\n",
      "\tLoss: 0.141493558883667\n",
      "\tLoss: 0.09524554759263992\n",
      "\tLoss: 0.08273150026798248\n",
      "\tLoss: 0.19041740894317627\n",
      "\tLoss: 0.1359090954065323\n",
      "\tLoss: 0.1252935528755188\n",
      "\tLoss: 0.14465069770812988\n",
      "\tLoss: 0.1380087286233902\n",
      "[time] Epoch 8: 439.16275173518807s = 7.3193791955864675m\n",
      "\n",
      "Epoch 9...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.06370595097541809\n",
      "\tLoss: 0.10428192466497421\n",
      "\tLoss: 0.14067763090133667\n",
      "\tLoss: 0.12008306384086609\n",
      "\tLoss: 0.08761496841907501\n",
      "\tLoss: 0.141559898853302\n",
      "\tLoss: 0.16280749440193176\n",
      "\tLoss: 0.11154335737228394\n",
      "\tLoss: 0.14451996982097626\n",
      "\tLoss: 0.17348892986774445\n",
      "\tLoss: 0.14943143725395203\n",
      "\tLoss: 0.1253965198993683\n",
      "\tLoss: 0.17536292970180511\n",
      "\tLoss: 0.0669964849948883\n",
      "\tLoss: 0.16058050096035004\n",
      "\tLoss: 0.17375096678733826\n",
      "\tLoss: 0.14640554785728455\n",
      "\tLoss: 0.15327188372612\n",
      "\tLoss: 0.10999393463134766\n",
      "\tLoss: 0.19886131584644318\n",
      "\tLoss: 0.13998766243457794\n",
      "\tLoss: 0.1991061568260193\n",
      "\tLoss: 0.09893657267093658\n",
      "\tLoss: 0.13097821176052094\n",
      "\tLoss: 0.1548008918762207\n",
      "\tLoss: 0.1588350385427475\n",
      "\tLoss: 0.1460418403148651\n",
      "\tLoss: 0.09584423154592514\n",
      "\tLoss: 0.11556112766265869\n",
      "\tLoss: 0.1356402337551117\n",
      "\tLoss: 0.1505700647830963\n",
      "\tLoss: 0.15466991066932678\n",
      "\tLoss: 0.141964852809906\n",
      "\tLoss: 0.20038020610809326\n",
      "\tLoss: 0.08393295109272003\n",
      "\tLoss: 0.14887331426143646\n",
      "\tLoss: 0.12044092267751694\n",
      "\tLoss: 0.21813815832138062\n",
      "\tLoss: 0.14464601874351501\n",
      "\tLoss: 0.17135585844516754\n",
      "\tLoss: 0.1854049265384674\n",
      "\tLoss: 0.18974816799163818\n",
      "\tLoss: 0.14188626408576965\n",
      "\tLoss: 0.17075051367282867\n",
      "\tLoss: 0.14694340527057648\n",
      "\tLoss: 0.15594127774238586\n",
      "\tLoss: 0.12345170974731445\n",
      "\tLoss: 0.18285180628299713\n",
      "\tLoss: 0.11676555871963501\n",
      "\tLoss: 0.12745633721351624\n",
      "\tLoss: 0.16969819366931915\n",
      "\tLoss: 0.09530092030763626\n",
      "\tLoss: 0.10890734195709229\n",
      "\tLoss: 0.15946710109710693\n",
      "\tLoss: 0.1365993767976761\n",
      "\tLoss: 0.11430659890174866\n",
      "\tLoss: 0.09691347926855087\n",
      "\tLoss: 0.19420841336250305\n",
      "\tLoss: 0.18215978145599365\n",
      "\tLoss: 0.15586704015731812\n",
      "\tLoss: 0.18699562549591064\n",
      "\tLoss: 0.11008872091770172\n",
      "\tLoss: 0.12304285913705826\n",
      "\tLoss: 0.09861088544130325\n",
      "\tLoss: 0.19062674045562744\n",
      "\tLoss: 0.14897586405277252\n",
      "\tLoss: 0.15004664659500122\n",
      "\tLoss: 0.1569405198097229\n",
      "\tLoss: 0.1542530655860901\n",
      "\tLoss: 0.1782657355070114\n",
      "\tLoss: 0.16719871759414673\n",
      "\tLoss: 0.1437528431415558\n",
      "\tLoss: 0.15488728880882263\n",
      "\tLoss: 0.15809649229049683\n",
      "\tLoss: 0.14171341061592102\n",
      "\tLoss: 0.1889791041612625\n",
      "\tLoss: 0.15058064460754395\n",
      "\tLoss: 0.13465604186058044\n",
      "\tLoss: 0.16334615647792816\n",
      "\tLoss: 0.13167211413383484\n",
      "\tLoss: 0.1579016000032425\n",
      "\tLoss: 0.2055535912513733\n",
      "\tLoss: 0.161809504032135\n",
      "\tLoss: 0.1297633945941925\n",
      "\tLoss: 0.14158394932746887\n",
      "\tLoss: 0.1282428652048111\n",
      "\tLoss: 0.1461237519979477\n",
      "\tLoss: 0.1339680403470993\n",
      "\tLoss: 0.1566527634859085\n",
      "\tLoss: 0.08568218350410461\n",
      "\tLoss: 0.12489667534828186\n",
      "\tLoss: 0.15169057250022888\n",
      "\tLoss: 0.1311873197555542\n",
      "\tLoss: 0.14624884724617004\n",
      "\tLoss: 0.10579375922679901\n",
      "\tLoss: 0.18107420206069946\n",
      "\tLoss: 0.1278477907180786\n",
      "\tLoss: 0.11776208132505417\n",
      "\tLoss: 0.14504143595695496\n",
      "\tLoss: 0.1517086923122406\n",
      "\tLoss: 0.07587321102619171\n",
      "\tLoss: 0.1350506842136383\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.13682715594768524\n",
      "\tLoss: 0.1321842074394226\n",
      "\tLoss: 0.10279577225446701\n",
      "\tLoss: 0.13984277844429016\n",
      "\tLoss: 0.18287937343120575\n",
      "\tLoss: 0.1381697803735733\n",
      "\tLoss: 0.2088029980659485\n",
      "\tLoss: 0.12289255857467651\n",
      "\tLoss: 0.11772359907627106\n",
      "\tLoss: 0.09446687996387482\n",
      "\tLoss: 0.12013784795999527\n",
      "\tLoss: 0.1366027593612671\n",
      "\tLoss: 0.0665912926197052\n",
      "\tLoss: 0.1306084245443344\n",
      "\tLoss: 0.13194061815738678\n",
      "\tLoss: 0.10300855338573456\n",
      "\tLoss: 0.13493308424949646\n",
      "\tLoss: 0.12271178513765335\n",
      "\tLoss: 0.13425719738006592\n",
      "\tLoss: 0.1074482798576355\n",
      "\tLoss: 0.1215120181441307\n",
      "\tLoss: 0.14194689691066742\n",
      "\tLoss: 0.1705440878868103\n",
      "\tLoss: 0.10668589919805527\n",
      "\tLoss: 0.14482903480529785\n",
      "\tLoss: 0.1303822100162506\n",
      "\tLoss: 0.0803619772195816\n",
      "\tLoss: 0.1213451623916626\n",
      "\tLoss: 0.2247782051563263\n",
      "\tLoss: 0.21803295612335205\n",
      "\tLoss: 0.12216130644083023\n",
      "\tLoss: 0.10279138386249542\n",
      "\tLoss: 0.154018834233284\n",
      "\tLoss: 0.08624687045812607\n",
      "\tLoss: 0.11741321533918381\n",
      "\tLoss: 0.10265189409255981\n",
      "\tLoss: 0.12050259113311768\n",
      "\tLoss: 0.11091385781764984\n",
      "\tLoss: 0.10783737152814865\n",
      "\tLoss: 0.08826956152915955\n",
      "\tLoss: 0.11934933066368103\n",
      "\tLoss: 0.16101974248886108\n",
      "\tLoss: 0.10453884303569794\n",
      "\tLoss: 0.12112781405448914\n",
      "\tLoss: 0.10495388507843018\n",
      "\tLoss: 0.11138889193534851\n",
      "\tLoss: 0.13084086775779724\n",
      "\tLoss: 0.1265130639076233\n",
      "\tLoss: 0.15075376629829407\n",
      "\tLoss: 0.15209174156188965\n",
      "\tLoss: 0.07329012453556061\n",
      "\tLoss: 0.1403026133775711\n",
      "\tLoss: 0.15557453036308289\n",
      "\tLoss: 0.16090410947799683\n",
      "\tLoss: 0.13486695289611816\n",
      "\tLoss: 0.14741331338882446\n",
      "\tLoss: 0.15180209279060364\n",
      "\tLoss: 0.11053614318370819\n",
      "\tLoss: 0.11429691314697266\n",
      "\tLoss: 0.10983021557331085\n",
      "\tLoss: 0.16040104627609253\n",
      "\tLoss: 0.1304284632205963\n",
      "\tLoss: 0.11475737392902374\n",
      "\tLoss: 0.14604951441287994\n",
      "\tLoss: 0.1677451878786087\n",
      "\tLoss: 0.15563538670539856\n",
      "\tLoss: 0.1637992411851883\n",
      "\tLoss: 0.08275597542524338\n",
      "\tLoss: 0.10068875551223755\n",
      "\tLoss: 0.16095484793186188\n",
      "\tLoss: 0.09644407778978348\n",
      "\tLoss: 0.13084043562412262\n",
      "\tLoss: 0.1294356733560562\n",
      "\tLoss: 0.18293748795986176\n",
      "\tLoss: 0.15798914432525635\n",
      "\tLoss: 0.1113312691450119\n",
      "\tLoss: 0.17441359162330627\n",
      "\tLoss: 0.1614455282688141\n",
      "\tLoss: 0.12229152023792267\n",
      "\tLoss: 0.1510840803384781\n",
      "\tLoss: 0.1623336672782898\n",
      "\tLoss: 0.08667051792144775\n",
      "\tLoss: 0.17524001002311707\n",
      "\tLoss: 0.1147335022687912\n",
      "\tLoss: 0.12767377495765686\n",
      "\tLoss: 0.1449434459209442\n",
      "\tLoss: 0.11413933336734772\n",
      "\tLoss: 0.09282378107309341\n",
      "\tLoss: 0.1424904763698578\n",
      "\tLoss: 0.12432816624641418\n",
      "\tLoss: 0.16095948219299316\n",
      "\tLoss: 0.15257585048675537\n",
      "\tLoss: 0.1187342032790184\n",
      "\tLoss: 0.125772163271904\n",
      "\tLoss: 0.13361060619354248\n",
      "\tLoss: 0.1290157437324524\n",
      "\tLoss: 0.12757807970046997\n",
      "\tLoss: 0.19091424345970154\n",
      "\tLoss: 0.17215211689472198\n",
      "\tLoss: 0.14049823582172394\n",
      "\tLoss: 0.12916380167007446\n",
      "\tLoss: 0.09917205572128296\n",
      "[time] Epoch 9: 437.6479484317824s = 7.294132473863041m\n",
      "\n",
      "Epoch 10...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.1180909126996994\n",
      "\tLoss: 0.14583821594715118\n",
      "\tLoss: 0.13933512568473816\n",
      "\tLoss: 0.10234600305557251\n",
      "\tLoss: 0.1532079577445984\n",
      "\tLoss: 0.14705008268356323\n",
      "\tLoss: 0.1620214283466339\n",
      "\tLoss: 0.13421370089054108\n",
      "\tLoss: 0.09411182999610901\n",
      "\tLoss: 0.09124666452407837\n",
      "\tLoss: 0.208261176943779\n",
      "\tLoss: 0.17350360751152039\n",
      "\tLoss: 0.06928995251655579\n",
      "\tLoss: 0.13524354994297028\n",
      "\tLoss: 0.10756097733974457\n",
      "\tLoss: 0.1410190612077713\n",
      "\tLoss: 0.12271563708782196\n",
      "\tLoss: 0.10086563229560852\n",
      "\tLoss: 0.10561972856521606\n",
      "\tLoss: 0.1510343849658966\n",
      "\tLoss: 0.16496077179908752\n",
      "\tLoss: 0.10694073885679245\n",
      "\tLoss: 0.13224032521247864\n",
      "\tLoss: 0.12024778872728348\n",
      "\tLoss: 0.18002870678901672\n",
      "\tLoss: 0.15235884487628937\n",
      "\tLoss: 0.11339238286018372\n",
      "\tLoss: 0.13456495106220245\n",
      "\tLoss: 0.1512736678123474\n",
      "\tLoss: 0.20528501272201538\n",
      "\tLoss: 0.11817575991153717\n",
      "\tLoss: 0.14956729114055634\n",
      "\tLoss: 0.18800993263721466\n",
      "\tLoss: 0.12158724665641785\n",
      "\tLoss: 0.1289481818675995\n",
      "\tLoss: 0.07835019379854202\n",
      "\tLoss: 0.12207235395908356\n",
      "\tLoss: 0.20714011788368225\n",
      "\tLoss: 0.10993628948926926\n",
      "\tLoss: 0.16474699974060059\n",
      "\tLoss: 0.09423331171274185\n",
      "\tLoss: 0.10176003724336624\n",
      "\tLoss: 0.10609454661607742\n",
      "\tLoss: 0.11688262224197388\n",
      "\tLoss: 0.1645134687423706\n",
      "\tLoss: 0.10666787624359131\n",
      "\tLoss: 0.1047811508178711\n",
      "\tLoss: 0.13029253482818604\n",
      "\tLoss: 0.15027035772800446\n",
      "\tLoss: 0.1373445987701416\n",
      "\tLoss: 0.15466271340847015\n",
      "\tLoss: 0.1694575548171997\n",
      "\tLoss: 0.1255275160074234\n",
      "\tLoss: 0.20581088960170746\n",
      "\tLoss: 0.13472458720207214\n",
      "\tLoss: 0.14531579613685608\n",
      "\tLoss: 0.17094279825687408\n",
      "\tLoss: 0.13747096061706543\n",
      "\tLoss: 0.14083319902420044\n",
      "\tLoss: 0.1488770842552185\n",
      "\tLoss: 0.13676616549491882\n",
      "\tLoss: 0.11893846839666367\n",
      "\tLoss: 0.10428064316511154\n",
      "\tLoss: 0.12581557035446167\n",
      "\tLoss: 0.10211807489395142\n",
      "\tLoss: 0.10965041816234589\n",
      "\tLoss: 0.16280487179756165\n",
      "\tLoss: 0.1324019432067871\n",
      "\tLoss: 0.18674099445343018\n",
      "\tLoss: 0.1301082968711853\n",
      "\tLoss: 0.1335117369890213\n",
      "\tLoss: 0.14660578966140747\n",
      "\tLoss: 0.13725140690803528\n",
      "\tLoss: 0.12044233083724976\n",
      "\tLoss: 0.11099006980657578\n",
      "\tLoss: 0.1183515191078186\n",
      "\tLoss: 0.20199993252754211\n",
      "\tLoss: 0.11515063047409058\n",
      "\tLoss: 0.12300857156515121\n",
      "\tLoss: 0.10199904441833496\n",
      "\tLoss: 0.12155695259571075\n",
      "\tLoss: 0.11015059053897858\n",
      "\tLoss: 0.11947044730186462\n",
      "\tLoss: 0.18730658292770386\n",
      "\tLoss: 0.16753074526786804\n",
      "\tLoss: 0.151392862200737\n",
      "\tLoss: 0.09743774682283401\n",
      "\tLoss: 0.11956419050693512\n",
      "\tLoss: 0.15907587110996246\n",
      "\tLoss: 0.19841596484184265\n",
      "\tLoss: 0.14157478511333466\n",
      "\tLoss: 0.14808690547943115\n",
      "\tLoss: 0.12827059626579285\n",
      "\tLoss: 0.11482077836990356\n",
      "\tLoss: 0.14386864006519318\n",
      "\tLoss: 0.12971869111061096\n",
      "\tLoss: 0.13845227658748627\n",
      "\tLoss: 0.1476578563451767\n",
      "\tLoss: 0.12882274389266968\n",
      "\tLoss: 0.12196149677038193\n",
      "\tLoss: 0.1601717174053192\n",
      "\tLoss: 0.09829079359769821\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.12956559658050537\n",
      "\tLoss: 0.07731495797634125\n",
      "\tLoss: 0.20448622107505798\n",
      "\tLoss: 0.11055341362953186\n",
      "\tLoss: 0.19533443450927734\n",
      "\tLoss: 0.14689914882183075\n",
      "\tLoss: 0.14842291176319122\n",
      "\tLoss: 0.118918277323246\n",
      "\tLoss: 0.1205681562423706\n",
      "\tLoss: 0.1527654379606247\n",
      "\tLoss: 0.11023810505867004\n",
      "\tLoss: 0.13990390300750732\n",
      "\tLoss: 0.14979064464569092\n",
      "\tLoss: 0.13197055459022522\n",
      "\tLoss: 0.13302823901176453\n",
      "\tLoss: 0.17891821265220642\n",
      "\tLoss: 0.13634419441223145\n",
      "\tLoss: 0.18072077631950378\n",
      "\tLoss: 0.08648525178432465\n",
      "\tLoss: 0.13820186257362366\n",
      "\tLoss: 0.13412991166114807\n",
      "\tLoss: 0.17459417879581451\n",
      "\tLoss: 0.14180085062980652\n",
      "\tLoss: 0.07492756098508835\n",
      "\tLoss: 0.08278246223926544\n",
      "\tLoss: 0.09846942871809006\n",
      "\tLoss: 0.10970199108123779\n",
      "\tLoss: 0.14707745611667633\n",
      "\tLoss: 0.12700048089027405\n",
      "\tLoss: 0.13958053290843964\n",
      "\tLoss: 0.07650649547576904\n",
      "\tLoss: 0.12447689473628998\n",
      "\tLoss: 0.13623425364494324\n",
      "\tLoss: 0.08122800290584564\n",
      "\tLoss: 0.11743269860744476\n",
      "\tLoss: 0.08923547714948654\n",
      "\tLoss: 0.12758032977581024\n",
      "\tLoss: 0.13673937320709229\n",
      "\tLoss: 0.17826449871063232\n",
      "\tLoss: 0.08453062176704407\n",
      "\tLoss: 0.07626473903656006\n",
      "\tLoss: 0.10426643490791321\n",
      "\tLoss: 0.14760221540927887\n",
      "\tLoss: 0.1086350679397583\n",
      "\tLoss: 0.13426919281482697\n",
      "\tLoss: 0.1618906557559967\n",
      "\tLoss: 0.08182433247566223\n",
      "\tLoss: 0.13800594210624695\n",
      "\tLoss: 0.19744530320167542\n",
      "\tLoss: 0.09706321358680725\n",
      "\tLoss: 0.100396066904068\n",
      "\tLoss: 0.13852572441101074\n",
      "\tLoss: 0.16172802448272705\n",
      "\tLoss: 0.17764219641685486\n",
      "\tLoss: 0.192403182387352\n",
      "\tLoss: 0.1861969530582428\n",
      "\tLoss: 0.14395980536937714\n",
      "\tLoss: 0.0912545770406723\n",
      "\tLoss: 0.11100756376981735\n",
      "\tLoss: 0.1012127548456192\n",
      "\tLoss: 0.14192155003547668\n",
      "\tLoss: 0.13552182912826538\n",
      "\tLoss: 0.13466277718544006\n",
      "\tLoss: 0.13072888553142548\n",
      "\tLoss: 0.16723060607910156\n",
      "\tLoss: 0.08500657975673676\n",
      "\tLoss: 0.0896136462688446\n",
      "\tLoss: 0.12335669249296188\n",
      "\tLoss: 0.10047183930873871\n",
      "\tLoss: 0.1008884385228157\n",
      "\tLoss: 0.12714990973472595\n",
      "\tLoss: 0.061609212309122086\n",
      "\tLoss: 0.17892269790172577\n",
      "\tLoss: 0.1584768295288086\n",
      "\tLoss: 0.15464572608470917\n",
      "\tLoss: 0.07773180305957794\n",
      "\tLoss: 0.13121117651462555\n",
      "\tLoss: 0.12468230724334717\n",
      "\tLoss: 0.11029645055532455\n",
      "\tLoss: 0.13426542282104492\n",
      "\tLoss: 0.20584478974342346\n",
      "\tLoss: 0.09542683511972427\n",
      "\tLoss: 0.15716667473316193\n",
      "\tLoss: 0.16756363213062286\n",
      "\tLoss: 0.192879319190979\n",
      "\tLoss: 0.11520745605230331\n",
      "\tLoss: 0.15407606959342957\n",
      "\tLoss: 0.1684248447418213\n",
      "\tLoss: 0.12187805771827698\n",
      "\tLoss: 0.15765315294265747\n",
      "\tLoss: 0.09964504837989807\n",
      "\tLoss: 0.08275522291660309\n",
      "\tLoss: 0.0794544592499733\n",
      "\tLoss: 0.1162608340382576\n",
      "\tLoss: 0.1114204078912735\n",
      "\tLoss: 0.10708613693714142\n",
      "\tLoss: 0.13839563727378845\n",
      "\tLoss: 0.1518458127975464\n",
      "\tLoss: 0.2488601952791214\n",
      "\tLoss: 0.09902898222208023\n",
      "\tLoss: 0.1981639564037323\n",
      "\tLoss: 0.13209575414657593\n",
      "[time] Epoch 10: 435.5564300059341s = 7.259273833432235m\n",
      "\n",
      "Epoch 11...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.1559670865535736\n",
      "\tLoss: 0.08294396102428436\n",
      "\tLoss: 0.11163437366485596\n",
      "\tLoss: 0.1543998122215271\n",
      "\tLoss: 0.2727981209754944\n",
      "\tLoss: 0.17054224014282227\n",
      "\tLoss: 0.10892622172832489\n",
      "\tLoss: 0.11632758378982544\n",
      "\tLoss: 0.11405487358570099\n",
      "\tLoss: 0.18703213334083557\n",
      "\tLoss: 0.11051297187805176\n",
      "\tLoss: 0.1735130101442337\n",
      "\tLoss: 0.12524229288101196\n",
      "\tLoss: 0.12860792875289917\n",
      "\tLoss: 0.1859806627035141\n",
      "\tLoss: 0.11829657852649689\n",
      "\tLoss: 0.15495926141738892\n",
      "\tLoss: 0.09547753632068634\n",
      "\tLoss: 0.11586737632751465\n",
      "\tLoss: 0.13450711965560913\n",
      "\tLoss: 0.08807902038097382\n",
      "\tLoss: 0.16389763355255127\n",
      "\tLoss: 0.11402327567338943\n",
      "\tLoss: 0.17356735467910767\n",
      "\tLoss: 0.12124741077423096\n",
      "\tLoss: 0.18174615502357483\n",
      "\tLoss: 0.1684335321187973\n",
      "\tLoss: 0.16315260529518127\n",
      "\tLoss: 0.135462686419487\n",
      "\tLoss: 0.08056672662496567\n",
      "\tLoss: 0.08133847266435623\n",
      "\tLoss: 0.11512001603841782\n",
      "\tLoss: 0.13779640197753906\n",
      "\tLoss: 0.12958961725234985\n",
      "\tLoss: 0.12356754392385483\n",
      "\tLoss: 0.11147677898406982\n",
      "\tLoss: 0.18902912735939026\n",
      "\tLoss: 0.12911736965179443\n",
      "\tLoss: 0.15919068455696106\n",
      "\tLoss: 0.1782398372888565\n",
      "\tLoss: 0.11494006961584091\n",
      "\tLoss: 0.17986789345741272\n",
      "\tLoss: 0.12059097737073898\n",
      "\tLoss: 0.10377539694309235\n",
      "\tLoss: 0.1236351802945137\n",
      "\tLoss: 0.14656710624694824\n",
      "\tLoss: 0.10859271138906479\n",
      "\tLoss: 0.16149049997329712\n",
      "\tLoss: 0.11035637557506561\n",
      "\tLoss: 0.07434889674186707\n",
      "\tLoss: 0.12102369964122772\n",
      "\tLoss: 0.12156813591718674\n",
      "\tLoss: 0.12868092954158783\n",
      "\tLoss: 0.11717159301042557\n",
      "\tLoss: 0.15510399639606476\n",
      "\tLoss: 0.18381783366203308\n",
      "\tLoss: 0.1379476934671402\n",
      "\tLoss: 0.13239030539989471\n",
      "\tLoss: 0.08381279557943344\n",
      "\tLoss: 0.06754878163337708\n",
      "\tLoss: 0.1801537573337555\n",
      "\tLoss: 0.14779745042324066\n",
      "\tLoss: 0.15774016082286835\n",
      "\tLoss: 0.19754934310913086\n",
      "\tLoss: 0.15635356307029724\n",
      "\tLoss: 0.15328259766101837\n",
      "\tLoss: 0.09636345505714417\n",
      "\tLoss: 0.09607037156820297\n",
      "\tLoss: 0.10472619533538818\n",
      "\tLoss: 0.15471389889717102\n",
      "\tLoss: 0.13690891861915588\n",
      "\tLoss: 0.19257278740406036\n",
      "\tLoss: 0.15211175382137299\n",
      "\tLoss: 0.11741765588521957\n",
      "\tLoss: 0.10570234060287476\n",
      "\tLoss: 0.1845184862613678\n",
      "\tLoss: 0.17375275492668152\n",
      "\tLoss: 0.19717732071876526\n",
      "\tLoss: 0.14414136111736298\n",
      "\tLoss: 0.1668241173028946\n",
      "\tLoss: 0.14408817887306213\n",
      "\tLoss: 0.11930368840694427\n",
      "\tLoss: 0.13017350435256958\n",
      "\tLoss: 0.17221982777118683\n",
      "\tLoss: 0.14695365726947784\n",
      "\tLoss: 0.11408063024282455\n",
      "\tLoss: 0.15219944715499878\n",
      "\tLoss: 0.13653278350830078\n",
      "\tLoss: 0.07201497256755829\n",
      "\tLoss: 0.11811842024326324\n",
      "\tLoss: 0.1453789621591568\n",
      "\tLoss: 0.2089991271495819\n",
      "\tLoss: 0.19211314618587494\n",
      "\tLoss: 0.16760551929473877\n",
      "\tLoss: 0.18732884526252747\n",
      "\tLoss: 0.1310376226902008\n",
      "\tLoss: 0.10432055592536926\n",
      "\tLoss: 0.15426376461982727\n",
      "\tLoss: 0.12824076414108276\n",
      "\tLoss: 0.09400883316993713\n",
      "\tLoss: 0.12641170620918274\n",
      "\tLoss: 0.1947128176689148\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.10712289810180664\n",
      "\tLoss: 0.10134866088628769\n",
      "\tLoss: 0.09143359959125519\n",
      "\tLoss: 0.09040462970733643\n",
      "\tLoss: 0.1512930989265442\n",
      "\tLoss: 0.10660199820995331\n",
      "\tLoss: 0.1818505823612213\n",
      "\tLoss: 0.09282463788986206\n",
      "\tLoss: 0.13662229478359222\n",
      "\tLoss: 0.1307598352432251\n",
      "\tLoss: 0.09233767539262772\n",
      "\tLoss: 0.12638850510120392\n",
      "\tLoss: 0.16293388605117798\n",
      "\tLoss: 0.09172110259532928\n",
      "\tLoss: 0.16314426064491272\n",
      "\tLoss: 0.08689679205417633\n",
      "\tLoss: 0.1181107759475708\n",
      "\tLoss: 0.16140814125537872\n",
      "\tLoss: 0.09733525663614273\n",
      "\tLoss: 0.11880162358283997\n",
      "\tLoss: 0.14105713367462158\n",
      "\tLoss: 0.10505033284425735\n",
      "\tLoss: 0.10968421399593353\n",
      "\tLoss: 0.19762615859508514\n",
      "\tLoss: 0.08374685049057007\n",
      "\tLoss: 0.07783312350511551\n",
      "\tLoss: 0.09163839370012283\n",
      "\tLoss: 0.05698954313993454\n",
      "\tLoss: 0.11664839833974838\n",
      "\tLoss: 0.17715688049793243\n",
      "\tLoss: 0.10125450789928436\n",
      "\tLoss: 0.11273424327373505\n",
      "\tLoss: 0.20254047214984894\n",
      "\tLoss: 0.12237383425235748\n",
      "\tLoss: 0.1282646507024765\n",
      "\tLoss: 0.12162145972251892\n",
      "\tLoss: 0.1322600245475769\n",
      "\tLoss: 0.1139281615614891\n",
      "\tLoss: 0.1711180955171585\n",
      "\tLoss: 0.08371151238679886\n",
      "\tLoss: 0.12222699820995331\n",
      "\tLoss: 0.14647993445396423\n",
      "\tLoss: 0.15869536995887756\n",
      "\tLoss: 0.13624995946884155\n",
      "\tLoss: 0.11982657015323639\n",
      "\tLoss: 0.1030845195055008\n",
      "\tLoss: 0.10196919739246368\n",
      "\tLoss: 0.15142229199409485\n",
      "\tLoss: 0.07476530969142914\n",
      "\tLoss: 0.12160193920135498\n",
      "\tLoss: 0.19659028947353363\n",
      "\tLoss: 0.14007754623889923\n",
      "\tLoss: 0.1532372534275055\n",
      "\tLoss: 0.11783285439014435\n",
      "\tLoss: 0.10116171836853027\n",
      "\tLoss: 0.15539765357971191\n",
      "\tLoss: 0.15837013721466064\n",
      "\tLoss: 0.1580890715122223\n",
      "\tLoss: 0.07780434936285019\n",
      "\tLoss: 0.08962011337280273\n",
      "\tLoss: 0.09650260210037231\n",
      "\tLoss: 0.09490413963794708\n",
      "\tLoss: 0.18439818918704987\n",
      "\tLoss: 0.17060381174087524\n",
      "\tLoss: 0.10131648927927017\n",
      "\tLoss: 0.11420783400535583\n",
      "\tLoss: 0.15076260268688202\n",
      "\tLoss: 0.09638715535402298\n",
      "\tLoss: 0.1382957398891449\n",
      "\tLoss: 0.11148674786090851\n",
      "\tLoss: 0.09617135673761368\n",
      "\tLoss: 0.13648304343223572\n",
      "\tLoss: 0.1592240333557129\n",
      "\tLoss: 0.12119410932064056\n",
      "\tLoss: 0.11351532489061356\n",
      "\tLoss: 0.07632146030664444\n",
      "\tLoss: 0.13255132734775543\n",
      "\tLoss: 0.09386821836233139\n",
      "\tLoss: 0.10917265713214874\n",
      "\tLoss: 0.14642217755317688\n",
      "\tLoss: 0.1519240289926529\n",
      "\tLoss: 0.11992426216602325\n",
      "\tLoss: 0.12460434436798096\n",
      "\tLoss: 0.15297988057136536\n",
      "\tLoss: 0.10894101858139038\n",
      "\tLoss: 0.14920194447040558\n",
      "\tLoss: 0.1324523687362671\n",
      "\tLoss: 0.10778581351041794\n",
      "\tLoss: 0.11748707294464111\n",
      "\tLoss: 0.15927699208259583\n",
      "\tLoss: 0.10087697207927704\n",
      "\tLoss: 0.20209893584251404\n",
      "\tLoss: 0.12768636643886566\n",
      "\tLoss: 0.15752571821212769\n",
      "\tLoss: 0.08767161518335342\n",
      "\tLoss: 0.1565077006816864\n",
      "\tLoss: 0.144825279712677\n",
      "\tLoss: 0.18241943418979645\n",
      "\tLoss: 0.0807560533285141\n",
      "\tLoss: 0.16113325953483582\n",
      "\tLoss: 0.1487298309803009\n",
      "\tLoss: 0.1158761978149414\n",
      "[time] Epoch 11: 436.15771640185267s = 7.269295273364211m\n",
      "\n",
      "Epoch 12...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.04827796667814255\n",
      "\tLoss: 0.09998057782649994\n",
      "\tLoss: 0.16428230702877045\n",
      "\tLoss: 0.16463914513587952\n",
      "\tLoss: 0.08930625021457672\n",
      "\tLoss: 0.11441078037023544\n",
      "\tLoss: 0.14242184162139893\n",
      "\tLoss: 0.14461000263690948\n",
      "\tLoss: 0.133766770362854\n",
      "\tLoss: 0.1499451994895935\n",
      "\tLoss: 0.13798171281814575\n",
      "\tLoss: 0.10776974260807037\n",
      "\tLoss: 0.16965553164482117\n",
      "\tLoss: 0.07150445878505707\n",
      "\tLoss: 0.10814588516950607\n",
      "\tLoss: 0.148930162191391\n",
      "\tLoss: 0.12879452109336853\n",
      "\tLoss: 0.16654960811138153\n",
      "\tLoss: 0.11902760714292526\n",
      "\tLoss: 0.18068057298660278\n",
      "\tLoss: 0.1986459493637085\n",
      "\tLoss: 0.17016401886940002\n",
      "\tLoss: 0.1400931477546692\n",
      "\tLoss: 0.11494231224060059\n",
      "\tLoss: 0.15157964825630188\n",
      "\tLoss: 0.07629033923149109\n",
      "\tLoss: 0.12198100984096527\n",
      "\tLoss: 0.10964864492416382\n",
      "\tLoss: 0.14201179146766663\n",
      "\tLoss: 0.18371856212615967\n",
      "\tLoss: 0.11429508030414581\n",
      "\tLoss: 0.1168532744050026\n",
      "\tLoss: 0.090932697057724\n",
      "\tLoss: 0.18496331572532654\n",
      "\tLoss: 0.08421209454536438\n",
      "\tLoss: 0.10232718288898468\n",
      "\tLoss: 0.11699045449495316\n",
      "\tLoss: 0.1879577338695526\n",
      "\tLoss: 0.12909157574176788\n",
      "\tLoss: 0.19331219792366028\n",
      "\tLoss: 0.09544038772583008\n",
      "\tLoss: 0.06992653012275696\n",
      "\tLoss: 0.06986924260854721\n",
      "\tLoss: 0.12023724615573883\n",
      "\tLoss: 0.15660971403121948\n",
      "\tLoss: 0.16049852967262268\n",
      "\tLoss: 0.13136331737041473\n",
      "\tLoss: 0.1910303235054016\n",
      "\tLoss: 0.1510939598083496\n",
      "\tLoss: 0.1482807844877243\n",
      "\tLoss: 0.11606644839048386\n",
      "\tLoss: 0.08370980620384216\n",
      "\tLoss: 0.12020793557167053\n",
      "\tLoss: 0.15832707285881042\n",
      "\tLoss: 0.16946525871753693\n",
      "\tLoss: 0.0903751328587532\n",
      "\tLoss: 0.09738123416900635\n",
      "\tLoss: 0.12740007042884827\n",
      "\tLoss: 0.13075679540634155\n",
      "\tLoss: 0.15954414010047913\n",
      "\tLoss: 0.07619413733482361\n",
      "\tLoss: 0.1186799556016922\n",
      "\tLoss: 0.17431145906448364\n",
      "\tLoss: 0.11021655052900314\n",
      "\tLoss: 0.13847504556179047\n",
      "\tLoss: 0.10720515251159668\n",
      "\tLoss: 0.17248065769672394\n",
      "\tLoss: 0.13865607976913452\n",
      "\tLoss: 0.09625373780727386\n",
      "\tLoss: 0.14640140533447266\n",
      "\tLoss: 0.13994553685188293\n",
      "\tLoss: 0.09545391798019409\n",
      "\tLoss: 0.1217242032289505\n",
      "\tLoss: 0.132780984044075\n",
      "\tLoss: 0.18495863676071167\n",
      "\tLoss: 0.1434808373451233\n",
      "\tLoss: 0.11906000971794128\n",
      "\tLoss: 0.15348434448242188\n",
      "\tLoss: 0.09633694589138031\n",
      "\tLoss: 0.13791091740131378\n",
      "\tLoss: 0.1407940685749054\n",
      "\tLoss: 0.1424294114112854\n",
      "\tLoss: 0.06951041519641876\n",
      "\tLoss: 0.11203586310148239\n",
      "\tLoss: 0.13690362870693207\n",
      "\tLoss: 0.10069584846496582\n",
      "\tLoss: 0.1483202576637268\n",
      "\tLoss: 0.14814843237400055\n",
      "\tLoss: 0.09340652823448181\n",
      "\tLoss: 0.1725063920021057\n",
      "\tLoss: 0.1407867968082428\n",
      "\tLoss: 0.1358385980129242\n",
      "\tLoss: 0.09031732380390167\n",
      "\tLoss: 0.12557381391525269\n",
      "\tLoss: 0.1495850384235382\n",
      "\tLoss: 0.11946365237236023\n",
      "\tLoss: 0.11105716973543167\n",
      "\tLoss: 0.20540213584899902\n",
      "\tLoss: 0.10545933246612549\n",
      "\tLoss: 0.13890737295150757\n",
      "\tLoss: 0.11456115543842316\n",
      "\tLoss: 0.19124846160411835\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.09472690522670746\n",
      "\tLoss: 0.07209143042564392\n",
      "\tLoss: 0.12030241638422012\n",
      "\tLoss: 0.14726340770721436\n",
      "\tLoss: 0.12183339148759842\n",
      "\tLoss: 0.1862373650074005\n",
      "\tLoss: 0.11631888151168823\n",
      "\tLoss: 0.08698491752147675\n",
      "\tLoss: 0.16816776990890503\n",
      "\tLoss: 0.10758241266012192\n",
      "\tLoss: 0.10572324693202972\n",
      "\tLoss: 0.09718522429466248\n",
      "\tLoss: 0.07423030585050583\n",
      "\tLoss: 0.12508152425289154\n",
      "\tLoss: 0.15522632002830505\n",
      "\tLoss: 0.11518663913011551\n",
      "\tLoss: 0.17038625478744507\n",
      "\tLoss: 0.19404268264770508\n",
      "\tLoss: 0.16720226407051086\n",
      "\tLoss: 0.14285394549369812\n",
      "\tLoss: 0.10913718491792679\n",
      "\tLoss: 0.15165188908576965\n",
      "\tLoss: 0.14409850537776947\n",
      "\tLoss: 0.12498817592859268\n",
      "\tLoss: 0.12114755809307098\n",
      "\tLoss: 0.15598323941230774\n",
      "\tLoss: 0.10466203093528748\n",
      "\tLoss: 0.19856922328472137\n",
      "\tLoss: 0.21066361665725708\n",
      "\tLoss: 0.13603031635284424\n",
      "\tLoss: 0.12773868441581726\n",
      "\tLoss: 0.1392359882593155\n",
      "\tLoss: 0.09681510180234909\n",
      "\tLoss: 0.11140547692775726\n",
      "\tLoss: 0.1968388557434082\n",
      "\tLoss: 0.10434512794017792\n",
      "\tLoss: 0.21173077821731567\n",
      "\tLoss: 0.14771440625190735\n",
      "\tLoss: 0.1325538158416748\n",
      "\tLoss: 0.15169654786586761\n",
      "\tLoss: 0.1458955854177475\n",
      "\tLoss: 0.1410260647535324\n",
      "\tLoss: 0.2345992773771286\n",
      "\tLoss: 0.09511313587427139\n",
      "\tLoss: 0.13588422536849976\n",
      "\tLoss: 0.10212307423353195\n",
      "\tLoss: 0.06999142467975616\n",
      "\tLoss: 0.12415461987257004\n",
      "\tLoss: 0.11836604028940201\n",
      "\tLoss: 0.11417868733406067\n",
      "\tLoss: 0.11636088043451309\n",
      "\tLoss: 0.164523184299469\n",
      "\tLoss: 0.13686946034431458\n",
      "\tLoss: 0.14403854310512543\n",
      "\tLoss: 0.14193221926689148\n",
      "\tLoss: 0.09462237358093262\n",
      "\tLoss: 0.12898552417755127\n",
      "\tLoss: 0.13740617036819458\n",
      "\tLoss: 0.18829727172851562\n",
      "\tLoss: 0.12921559810638428\n",
      "\tLoss: 0.13574452698230743\n",
      "\tLoss: 0.09108953177928925\n",
      "\tLoss: 0.11453252285718918\n",
      "\tLoss: 0.1705482304096222\n",
      "\tLoss: 0.08540238440036774\n",
      "\tLoss: 0.1270400583744049\n",
      "\tLoss: 0.1767425537109375\n",
      "\tLoss: 0.1130673810839653\n",
      "\tLoss: 0.10895396023988724\n",
      "\tLoss: 0.16172802448272705\n",
      "\tLoss: 0.12569141387939453\n",
      "\tLoss: 0.15322038531303406\n",
      "\tLoss: 0.09930957853794098\n",
      "\tLoss: 0.14433881640434265\n",
      "\tLoss: 0.15680623054504395\n",
      "\tLoss: 0.08346925675868988\n",
      "\tLoss: 0.18847569823265076\n",
      "\tLoss: 0.20225784182548523\n",
      "\tLoss: 0.12633872032165527\n",
      "\tLoss: 0.1231495589017868\n",
      "\tLoss: 0.12887021899223328\n",
      "\tLoss: 0.15973007678985596\n",
      "\tLoss: 0.08209969103336334\n",
      "\tLoss: 0.18253274261951447\n",
      "\tLoss: 0.12423785775899887\n",
      "\tLoss: 0.15018555521965027\n",
      "\tLoss: 0.11429933458566666\n",
      "\tLoss: 0.11683247238397598\n",
      "\tLoss: 0.14815062284469604\n",
      "\tLoss: 0.15726608037948608\n",
      "\tLoss: 0.15643587708473206\n",
      "\tLoss: 0.10850706696510315\n",
      "\tLoss: 0.1243157610297203\n",
      "\tLoss: 0.09243154525756836\n",
      "\tLoss: 0.10964974015951157\n",
      "\tLoss: 0.07697557657957077\n",
      "\tLoss: 0.12105868756771088\n",
      "\tLoss: 0.17840923368930817\n",
      "\tLoss: 0.16598498821258545\n",
      "\tLoss: 0.13976913690567017\n",
      "\tLoss: 0.09829984605312347\n",
      "\tLoss: 0.1800239086151123\n",
      "[time] Epoch 12: 438.6460878578946s = 7.31076813096491m\n",
      "\n",
      "Epoch 13...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.13664737343788147\n",
      "\tLoss: 0.10097402334213257\n",
      "\tLoss: 0.1566508561372757\n",
      "\tLoss: 0.1345810890197754\n",
      "\tLoss: 0.1291668564081192\n",
      "\tLoss: 0.1050543338060379\n",
      "\tLoss: 0.190504789352417\n",
      "\tLoss: 0.08436979353427887\n",
      "\tLoss: 0.08698377758264542\n",
      "\tLoss: 0.18224212527275085\n",
      "\tLoss: 0.17103907465934753\n",
      "\tLoss: 0.12552696466445923\n",
      "\tLoss: 0.12634262442588806\n",
      "\tLoss: 0.12186875939369202\n",
      "\tLoss: 0.08997409045696259\n",
      "\tLoss: 0.10875262320041656\n",
      "\tLoss: 0.12604203820228577\n",
      "\tLoss: 0.1461246758699417\n",
      "\tLoss: 0.1376837193965912\n",
      "\tLoss: 0.11407977342605591\n",
      "\tLoss: 0.16854864358901978\n",
      "\tLoss: 0.16153475642204285\n",
      "\tLoss: 0.1745578646659851\n",
      "\tLoss: 0.1191524788737297\n",
      "\tLoss: 0.1587471067905426\n",
      "\tLoss: 0.16697953641414642\n",
      "\tLoss: 0.15699420869350433\n",
      "\tLoss: 0.19486922025680542\n",
      "\tLoss: 0.17054948210716248\n",
      "\tLoss: 0.14739862084388733\n",
      "\tLoss: 0.14139965176582336\n",
      "\tLoss: 0.09155184030532837\n",
      "\tLoss: 0.10740166157484055\n",
      "\tLoss: 0.13821634650230408\n",
      "\tLoss: 0.1406969130039215\n",
      "\tLoss: 0.13739919662475586\n",
      "\tLoss: 0.17051434516906738\n",
      "\tLoss: 0.1854981780052185\n",
      "\tLoss: 0.13666044175624847\n",
      "\tLoss: 0.15555283427238464\n",
      "\tLoss: 0.11953170597553253\n",
      "\tLoss: 0.12227710336446762\n",
      "\tLoss: 0.13781458139419556\n",
      "\tLoss: 0.07983498275279999\n",
      "\tLoss: 0.12223795056343079\n",
      "\tLoss: 0.12928509712219238\n",
      "\tLoss: 0.12634387612342834\n",
      "\tLoss: 0.14133189618587494\n",
      "\tLoss: 0.18311136960983276\n",
      "\tLoss: 0.15642985701560974\n",
      "\tLoss: 0.09455647319555283\n",
      "\tLoss: 0.14067763090133667\n",
      "\tLoss: 0.12258090078830719\n",
      "\tLoss: 0.10092872381210327\n",
      "\tLoss: 0.1338241547346115\n",
      "\tLoss: 0.1445445865392685\n",
      "\tLoss: 0.13454006612300873\n",
      "\tLoss: 0.1390068084001541\n",
      "\tLoss: 0.09484829008579254\n",
      "\tLoss: 0.1346634328365326\n",
      "\tLoss: 0.09625592082738876\n",
      "\tLoss: 0.0947633683681488\n",
      "\tLoss: 0.10862598568201065\n",
      "\tLoss: 0.11155825853347778\n",
      "\tLoss: 0.1146220713853836\n",
      "\tLoss: 0.10329857468605042\n",
      "\tLoss: 0.21272049844264984\n",
      "\tLoss: 0.13850325345993042\n",
      "\tLoss: 0.13956785202026367\n",
      "\tLoss: 0.11429806798696518\n",
      "\tLoss: 0.15823042392730713\n",
      "\tLoss: 0.1301266998052597\n",
      "\tLoss: 0.1019645407795906\n",
      "\tLoss: 0.08851396292448044\n",
      "\tLoss: 0.1554059237241745\n",
      "\tLoss: 0.11102592945098877\n",
      "\tLoss: 0.16043026745319366\n",
      "\tLoss: 0.1446956843137741\n",
      "\tLoss: 0.11473627388477325\n",
      "\tLoss: 0.12325428426265717\n",
      "\tLoss: 0.1442345678806305\n",
      "\tLoss: 0.13703417778015137\n",
      "\tLoss: 0.15675148367881775\n",
      "\tLoss: 0.12678730487823486\n",
      "\tLoss: 0.16141043603420258\n",
      "\tLoss: 0.07306718826293945\n",
      "\tLoss: 0.11361326277256012\n",
      "\tLoss: 0.10643699765205383\n",
      "\tLoss: 0.09200379252433777\n",
      "\tLoss: 0.0937158465385437\n",
      "\tLoss: 0.1549406349658966\n",
      "\tLoss: 0.10871405154466629\n",
      "\tLoss: 0.15348733961582184\n",
      "\tLoss: 0.07948553562164307\n",
      "\tLoss: 0.12400808185338974\n",
      "\tLoss: 0.13800525665283203\n",
      "\tLoss: 0.09635654091835022\n",
      "\tLoss: 0.16429373621940613\n",
      "\tLoss: 0.10279592126607895\n",
      "\tLoss: 0.08397683501243591\n",
      "\tLoss: 0.14417991042137146\n",
      "\tLoss: 0.10267684608697891\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.13128018379211426\n",
      "\tLoss: 0.1772259771823883\n",
      "\tLoss: 0.1272994428873062\n",
      "\tLoss: 0.19506865739822388\n",
      "\tLoss: 0.12266026437282562\n",
      "\tLoss: 0.156039297580719\n",
      "\tLoss: 0.10270123183727264\n",
      "\tLoss: 0.12430895864963531\n",
      "\tLoss: 0.09823153913021088\n",
      "\tLoss: 0.1575702428817749\n",
      "\tLoss: 0.13426189124584198\n",
      "\tLoss: 0.11413255333900452\n",
      "\tLoss: 0.1375337839126587\n",
      "\tLoss: 0.0855560377240181\n",
      "\tLoss: 0.11447808891534805\n",
      "\tLoss: 0.1267203390598297\n",
      "\tLoss: 0.13616609573364258\n",
      "\tLoss: 0.10126525163650513\n",
      "\tLoss: 0.146376833319664\n",
      "\tLoss: 0.12528257071971893\n",
      "\tLoss: 0.10707946866750717\n",
      "\tLoss: 0.1244567260146141\n",
      "\tLoss: 0.1853269636631012\n",
      "\tLoss: 0.17182594537734985\n",
      "\tLoss: 0.18525990843772888\n",
      "\tLoss: 0.0951637476682663\n",
      "\tLoss: 0.09610947966575623\n",
      "\tLoss: 0.11216823756694794\n",
      "\tLoss: 0.17345818877220154\n",
      "\tLoss: 0.16078180074691772\n",
      "\tLoss: 0.10009269416332245\n",
      "\tLoss: 0.112772636115551\n",
      "\tLoss: 0.04905034601688385\n",
      "\tLoss: 0.12569892406463623\n",
      "\tLoss: 0.21771448850631714\n",
      "\tLoss: 0.1204129308462143\n",
      "\tLoss: 0.1410515308380127\n",
      "\tLoss: 0.19380789995193481\n",
      "\tLoss: 0.14446473121643066\n",
      "\tLoss: 0.12843507528305054\n",
      "\tLoss: 0.15070180594921112\n",
      "\tLoss: 0.1532296985387802\n",
      "\tLoss: 0.19465409219264984\n",
      "\tLoss: 0.12284573167562485\n",
      "\tLoss: 0.12422510981559753\n",
      "\tLoss: 0.13219179213047028\n",
      "\tLoss: 0.10931602120399475\n",
      "\tLoss: 0.06701070815324783\n",
      "\tLoss: 0.08708271384239197\n",
      "\tLoss: 0.08851161599159241\n",
      "\tLoss: 0.117019422352314\n",
      "\tLoss: 0.14646422863006592\n",
      "\tLoss: 0.10910235345363617\n",
      "\tLoss: 0.08873655647039413\n",
      "\tLoss: 0.08868714421987534\n",
      "\tLoss: 0.1849927455186844\n",
      "\tLoss: 0.12383171916007996\n",
      "\tLoss: 0.1357247233390808\n",
      "\tLoss: 0.07958154380321503\n",
      "\tLoss: 0.11685938388109207\n",
      "\tLoss: 0.18437433242797852\n",
      "\tLoss: 0.18121649324893951\n",
      "\tLoss: 0.10279861092567444\n",
      "\tLoss: 0.09047459065914154\n",
      "\tLoss: 0.08071612566709518\n",
      "\tLoss: 0.1243424117565155\n",
      "\tLoss: 0.1528710424900055\n",
      "\tLoss: 0.09812447428703308\n",
      "\tLoss: 0.09032762050628662\n",
      "\tLoss: 0.19141247868537903\n",
      "\tLoss: 0.13819724321365356\n",
      "\tLoss: 0.18340426683425903\n",
      "\tLoss: 0.10329889506101608\n",
      "\tLoss: 0.1335446536540985\n",
      "\tLoss: 0.12451199442148209\n",
      "\tLoss: 0.09391443431377411\n",
      "\tLoss: 0.15235601365566254\n",
      "\tLoss: 0.166924387216568\n",
      "\tLoss: 0.1127627044916153\n",
      "\tLoss: 0.13798868656158447\n",
      "\tLoss: 0.10378678143024445\n",
      "\tLoss: 0.07322171330451965\n",
      "\tLoss: 0.11857920140028\n",
      "\tLoss: 0.15159761905670166\n",
      "\tLoss: 0.10003309696912766\n",
      "\tLoss: 0.08122093975543976\n",
      "\tLoss: 0.129118412733078\n",
      "\tLoss: 0.17028558254241943\n",
      "\tLoss: 0.09841131418943405\n",
      "\tLoss: 0.1147969514131546\n",
      "\tLoss: 0.1556786298751831\n",
      "\tLoss: 0.13251395523548126\n",
      "\tLoss: 0.16417312622070312\n",
      "\tLoss: 0.12416186183691025\n",
      "\tLoss: 0.13834862411022186\n",
      "\tLoss: 0.11096428334712982\n",
      "\tLoss: 0.1305253952741623\n",
      "\tLoss: 0.12507709860801697\n",
      "\tLoss: 0.17850394546985626\n",
      "\tLoss: 0.1344679445028305\n",
      "\tLoss: 0.12851333618164062\n",
      "\tLoss: 0.17059898376464844\n",
      "[time] Epoch 13: 435.7474803943187s = 7.262458006571978m\n",
      "\n",
      "Epoch 14...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.0892014428973198\n",
      "\tLoss: 0.10046611726284027\n",
      "\tLoss: 0.12164877355098724\n",
      "\tLoss: 0.07069379091262817\n",
      "\tLoss: 0.12010134011507034\n",
      "\tLoss: 0.10187113285064697\n",
      "\tLoss: 0.1613503396511078\n",
      "\tLoss: 0.14688187837600708\n",
      "\tLoss: 0.11536432057619095\n",
      "\tLoss: 0.18536308407783508\n",
      "\tLoss: 0.04007648676633835\n",
      "\tLoss: 0.17617851495742798\n",
      "\tLoss: 0.1129811629652977\n",
      "\tLoss: 0.11626004427671432\n",
      "\tLoss: 0.10983986407518387\n",
      "\tLoss: 0.20347130298614502\n",
      "\tLoss: 0.19379198551177979\n",
      "\tLoss: 0.15922236442565918\n",
      "\tLoss: 0.057662297040224075\n",
      "\tLoss: 0.07441908121109009\n",
      "\tLoss: 0.09840124845504761\n",
      "\tLoss: 0.15136539936065674\n",
      "\tLoss: 0.049104250967502594\n",
      "\tLoss: 0.14091701805591583\n",
      "\tLoss: 0.13284826278686523\n",
      "\tLoss: 0.13588972389698029\n",
      "\tLoss: 0.14088572561740875\n",
      "\tLoss: 0.11257138848304749\n",
      "\tLoss: 0.12808442115783691\n",
      "\tLoss: 0.12893202900886536\n",
      "\tLoss: 0.0913097932934761\n",
      "\tLoss: 0.1860024333000183\n",
      "\tLoss: 0.12517529726028442\n",
      "\tLoss: 0.1713700294494629\n",
      "\tLoss: 0.08069264888763428\n",
      "\tLoss: 0.14950813353061676\n",
      "\tLoss: 0.12078157812356949\n",
      "\tLoss: 0.1576838493347168\n",
      "\tLoss: 0.08439168334007263\n",
      "\tLoss: 0.10868804156780243\n",
      "\tLoss: 0.09857627749443054\n",
      "\tLoss: 0.10364405810832977\n",
      "\tLoss: 0.16112183034420013\n",
      "\tLoss: 0.09752285480499268\n",
      "\tLoss: 0.14331203699111938\n",
      "\tLoss: 0.16872569918632507\n",
      "\tLoss: 0.08363310247659683\n",
      "\tLoss: 0.12201486527919769\n",
      "\tLoss: 0.0801558643579483\n",
      "\tLoss: 0.15046972036361694\n",
      "\tLoss: 0.1695546656847\n",
      "\tLoss: 0.1284700632095337\n",
      "\tLoss: 0.110384002327919\n",
      "\tLoss: 0.08003804087638855\n",
      "\tLoss: 0.14922712743282318\n",
      "\tLoss: 0.10870590806007385\n",
      "\tLoss: 0.09171952307224274\n",
      "\tLoss: 0.1868707239627838\n",
      "\tLoss: 0.24422484636306763\n",
      "\tLoss: 0.10378533601760864\n",
      "\tLoss: 0.15496470034122467\n",
      "\tLoss: 0.0823865532875061\n",
      "\tLoss: 0.15507397055625916\n",
      "\tLoss: 0.11322160065174103\n",
      "\tLoss: 0.13043686747550964\n",
      "\tLoss: 0.12516987323760986\n",
      "\tLoss: 0.10793758183717728\n",
      "\tLoss: 0.10565941035747528\n",
      "\tLoss: 0.11951162666082382\n",
      "\tLoss: 0.06731486320495605\n",
      "\tLoss: 0.1431495100259781\n",
      "\tLoss: 0.18447455763816833\n",
      "\tLoss: 0.11553028970956802\n",
      "\tLoss: 0.16728800535202026\n",
      "\tLoss: 0.10569896548986435\n",
      "\tLoss: 0.10898716747760773\n",
      "\tLoss: 0.18641996383666992\n",
      "\tLoss: 0.15757574141025543\n",
      "\tLoss: 0.09472618252038956\n",
      "\tLoss: 0.07066145539283752\n",
      "\tLoss: 0.15897898375988007\n",
      "\tLoss: 0.17442956566810608\n",
      "\tLoss: 0.10546955466270447\n",
      "\tLoss: 0.13628336787223816\n",
      "\tLoss: 0.10479208827018738\n",
      "\tLoss: 0.1538061797618866\n",
      "\tLoss: 0.11995047330856323\n",
      "\tLoss: 0.0832909345626831\n",
      "\tLoss: 0.15113303065299988\n",
      "\tLoss: 0.12933941185474396\n",
      "\tLoss: 0.24410444498062134\n",
      "\tLoss: 0.11932909488677979\n",
      "\tLoss: 0.1227632686495781\n",
      "\tLoss: 0.1517658531665802\n",
      "\tLoss: 0.1719091534614563\n",
      "\tLoss: 0.15128467977046967\n",
      "\tLoss: 0.11772610247135162\n",
      "\tLoss: 0.16517820954322815\n",
      "\tLoss: 0.13854634761810303\n",
      "\tLoss: 0.1288660168647766\n",
      "\tLoss: 0.12084734439849854\n",
      "\tLoss: 0.07784683257341385\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.0986621230840683\n",
      "\tLoss: 0.1746610701084137\n",
      "\tLoss: 0.12751753628253937\n",
      "\tLoss: 0.16850785911083221\n",
      "\tLoss: 0.20184871554374695\n",
      "\tLoss: 0.08534267544746399\n",
      "\tLoss: 0.15260902047157288\n",
      "\tLoss: 0.20308126509189606\n",
      "\tLoss: 0.12906038761138916\n",
      "\tLoss: 0.08473537862300873\n",
      "\tLoss: 0.0816093236207962\n",
      "\tLoss: 0.13047850131988525\n",
      "\tLoss: 0.12020623683929443\n",
      "\tLoss: 0.09218072891235352\n",
      "\tLoss: 0.1789618283510208\n",
      "\tLoss: 0.17586052417755127\n",
      "\tLoss: 0.16133463382720947\n",
      "\tLoss: 0.10502971708774567\n",
      "\tLoss: 0.12172906845808029\n",
      "\tLoss: 0.14682863652706146\n",
      "\tLoss: 0.12367329746484756\n",
      "\tLoss: 0.14118054509162903\n",
      "\tLoss: 0.1339888721704483\n",
      "\tLoss: 0.152511864900589\n",
      "\tLoss: 0.1249508336186409\n",
      "\tLoss: 0.10276268422603607\n",
      "\tLoss: 0.1323302984237671\n",
      "\tLoss: 0.20551007986068726\n",
      "\tLoss: 0.0997103899717331\n",
      "\tLoss: 0.09426627308130264\n",
      "\tLoss: 0.11027543246746063\n",
      "\tLoss: 0.08178967982530594\n",
      "\tLoss: 0.12247679382562637\n",
      "\tLoss: 0.1521749496459961\n",
      "\tLoss: 0.09590913355350494\n",
      "\tLoss: 0.12701518833637238\n",
      "\tLoss: 0.15319105982780457\n",
      "\tLoss: 0.12645721435546875\n",
      "\tLoss: 0.08991684019565582\n",
      "\tLoss: 0.11547835171222687\n",
      "\tLoss: 0.13537666201591492\n",
      "\tLoss: 0.1282985806465149\n",
      "\tLoss: 0.13732323050498962\n",
      "\tLoss: 0.10874941200017929\n",
      "\tLoss: 0.1510961353778839\n",
      "\tLoss: 0.11807779967784882\n",
      "\tLoss: 0.0908852368593216\n",
      "\tLoss: 0.18462572991847992\n",
      "\tLoss: 0.07566608488559723\n",
      "\tLoss: 0.12378841638565063\n",
      "\tLoss: 0.0989864245057106\n",
      "\tLoss: 0.13524006307125092\n",
      "\tLoss: 0.16035637259483337\n",
      "\tLoss: 0.08069115877151489\n",
      "\tLoss: 0.11427532136440277\n",
      "\tLoss: 0.05673709511756897\n",
      "\tLoss: 0.1567002236843109\n",
      "\tLoss: 0.09422948956489563\n",
      "\tLoss: 0.09103232622146606\n",
      "\tLoss: 0.14412754774093628\n",
      "\tLoss: 0.11718083918094635\n",
      "\tLoss: 0.10903411358594894\n",
      "\tLoss: 0.12793444097042084\n",
      "\tLoss: 0.11009997129440308\n",
      "\tLoss: 0.08168736100196838\n",
      "\tLoss: 0.1396293342113495\n",
      "\tLoss: 0.10182418674230576\n",
      "\tLoss: 0.13466322422027588\n",
      "\tLoss: 0.13183045387268066\n",
      "\tLoss: 0.12978500127792358\n",
      "\tLoss: 0.21391145884990692\n",
      "\tLoss: 0.1535586267709732\n",
      "\tLoss: 0.08369994163513184\n",
      "\tLoss: 0.15743763744831085\n",
      "\tLoss: 0.13026054203510284\n",
      "\tLoss: 0.08958509564399719\n",
      "\tLoss: 0.19693249464035034\n",
      "\tLoss: 0.18592971563339233\n",
      "\tLoss: 0.07868414372205734\n",
      "\tLoss: 0.12875686585903168\n",
      "\tLoss: 0.11361765116453171\n",
      "\tLoss: 0.12731413543224335\n",
      "\tLoss: 0.15135832130908966\n",
      "\tLoss: 0.16000932455062866\n",
      "\tLoss: 0.10532936453819275\n",
      "\tLoss: 0.09969650208950043\n",
      "\tLoss: 0.08514910191297531\n",
      "\tLoss: 0.13374683260917664\n",
      "\tLoss: 0.09861361980438232\n",
      "\tLoss: 0.19936136901378632\n",
      "\tLoss: 0.09486314654350281\n",
      "\tLoss: 0.15097631514072418\n",
      "\tLoss: 0.10311323404312134\n",
      "\tLoss: 0.10452871769666672\n",
      "\tLoss: 0.07740724831819534\n",
      "\tLoss: 0.12680049240589142\n",
      "\tLoss: 0.11168041080236435\n",
      "\tLoss: 0.11485859006643295\n",
      "\tLoss: 0.1838740110397339\n",
      "\tLoss: 0.1379765272140503\n",
      "\tLoss: 0.12121950834989548\n",
      "\tLoss: 0.09604071825742722\n",
      "[time] Epoch 14: 432.40015068789944s = 7.206669178131658m\n",
      "\n",
      "Epoch 15...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.12305451184511185\n",
      "\tLoss: 0.1552753895521164\n",
      "\tLoss: 0.10644052177667618\n",
      "\tLoss: 0.14032620191574097\n",
      "\tLoss: 0.14599169790744781\n",
      "\tLoss: 0.12094748765230179\n",
      "\tLoss: 0.14499500393867493\n",
      "\tLoss: 0.1340876966714859\n",
      "\tLoss: 0.1091146171092987\n",
      "\tLoss: 0.10653787851333618\n",
      "\tLoss: 0.1309005618095398\n",
      "\tLoss: 0.1081196665763855\n",
      "\tLoss: 0.1313905268907547\n",
      "\tLoss: 0.21055665612220764\n",
      "\tLoss: 0.12804263830184937\n",
      "\tLoss: 0.14891444146633148\n",
      "\tLoss: 0.1277182698249817\n",
      "\tLoss: 0.15079843997955322\n",
      "\tLoss: 0.17312443256378174\n",
      "\tLoss: 0.16409006714820862\n",
      "\tLoss: 0.10345858335494995\n",
      "\tLoss: 0.13373792171478271\n",
      "\tLoss: 0.09038335084915161\n",
      "\tLoss: 0.1595153659582138\n",
      "\tLoss: 0.10598977655172348\n",
      "\tLoss: 0.16100364923477173\n",
      "\tLoss: 0.15270370244979858\n",
      "\tLoss: 0.1497259885072708\n",
      "\tLoss: 0.11063120514154434\n",
      "\tLoss: 0.1283685714006424\n",
      "\tLoss: 0.08412507176399231\n",
      "\tLoss: 0.0788237452507019\n",
      "\tLoss: 0.11159588396549225\n",
      "\tLoss: 0.15639840066432953\n",
      "\tLoss: 0.10546383261680603\n",
      "\tLoss: 0.19986800849437714\n",
      "\tLoss: 0.09548098593950272\n",
      "\tLoss: 0.07023347169160843\n",
      "\tLoss: 0.13696664571762085\n",
      "\tLoss: 0.18134844303131104\n",
      "\tLoss: 0.11790068447589874\n",
      "\tLoss: 0.1642252504825592\n",
      "\tLoss: 0.14650994539260864\n",
      "\tLoss: 0.1299574077129364\n",
      "\tLoss: 0.16015636920928955\n",
      "\tLoss: 0.0870126485824585\n",
      "\tLoss: 0.09921406954526901\n",
      "\tLoss: 0.07555380463600159\n",
      "\tLoss: 0.1430165022611618\n",
      "\tLoss: 0.16266316175460815\n",
      "\tLoss: 0.14292395114898682\n",
      "\tLoss: 0.17633678019046783\n",
      "\tLoss: 0.16058199107646942\n",
      "\tLoss: 0.12126076966524124\n",
      "\tLoss: 0.13516589999198914\n",
      "\tLoss: 0.149868905544281\n",
      "\tLoss: 0.1082075834274292\n",
      "\tLoss: 0.13738180696964264\n",
      "\tLoss: 0.18004803359508514\n",
      "\tLoss: 0.10866718739271164\n",
      "\tLoss: 0.1314254254102707\n",
      "\tLoss: 0.19792023301124573\n",
      "\tLoss: 0.14765676856040955\n",
      "\tLoss: 0.14056450128555298\n",
      "\tLoss: 0.07225842773914337\n",
      "\tLoss: 0.1604495346546173\n",
      "\tLoss: 0.08576451241970062\n",
      "\tLoss: 0.16490769386291504\n",
      "\tLoss: 0.18015262484550476\n",
      "\tLoss: 0.12985876202583313\n",
      "\tLoss: 0.11797081679105759\n",
      "\tLoss: 0.10806238651275635\n",
      "\tLoss: 0.1295396387577057\n",
      "\tLoss: 0.1422908753156662\n",
      "\tLoss: 0.10436785221099854\n",
      "\tLoss: 0.1307394951581955\n",
      "\tLoss: 0.09503281116485596\n",
      "\tLoss: 0.11494379490613937\n",
      "\tLoss: 0.15181849896907806\n",
      "\tLoss: 0.15591397881507874\n",
      "\tLoss: 0.09454528242349625\n",
      "\tLoss: 0.09044855833053589\n",
      "\tLoss: 0.16594576835632324\n",
      "\tLoss: 0.10821875929832458\n",
      "\tLoss: 0.09330980479717255\n",
      "\tLoss: 0.1397150754928589\n",
      "\tLoss: 0.10196055471897125\n",
      "\tLoss: 0.11408862471580505\n",
      "\tLoss: 0.11809761822223663\n",
      "\tLoss: 0.10032416135072708\n",
      "\tLoss: 0.1212105005979538\n",
      "\tLoss: 0.14456047117710114\n",
      "\tLoss: 0.11734461039304733\n",
      "\tLoss: 0.1548929661512375\n",
      "\tLoss: 0.10749324411153793\n",
      "\tLoss: 0.08725405484437943\n",
      "\tLoss: 0.1441536396741867\n",
      "\tLoss: 0.12709900736808777\n",
      "\tLoss: 0.1328766942024231\n",
      "\tLoss: 0.09342863410711288\n",
      "\tLoss: 0.1635901927947998\n",
      "\tLoss: 0.12513649463653564\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.08743197470903397\n",
      "\tLoss: 0.11782829463481903\n",
      "\tLoss: 0.15104031562805176\n",
      "\tLoss: 0.18797770142555237\n",
      "\tLoss: 0.11969756335020065\n",
      "\tLoss: 0.10562032461166382\n",
      "\tLoss: 0.17806145548820496\n",
      "\tLoss: 0.20230504870414734\n",
      "\tLoss: 0.16179466247558594\n",
      "\tLoss: 0.11278365552425385\n",
      "\tLoss: 0.10541871190071106\n",
      "\tLoss: 0.09106223285198212\n",
      "\tLoss: 0.1380402147769928\n",
      "\tLoss: 0.09577854722738266\n",
      "\tLoss: 0.11250743269920349\n",
      "\tLoss: 0.15167903900146484\n",
      "\tLoss: 0.11257641017436981\n",
      "\tLoss: 0.11074624210596085\n",
      "\tLoss: 0.12040546536445618\n",
      "\tLoss: 0.12742258608341217\n",
      "\tLoss: 0.14066335558891296\n",
      "\tLoss: 0.1449497640132904\n",
      "\tLoss: 0.11660483479499817\n",
      "\tLoss: 0.1213173121213913\n",
      "\tLoss: 0.17997857928276062\n",
      "\tLoss: 0.10478463023900986\n",
      "\tLoss: 0.08378508687019348\n",
      "\tLoss: 0.12682728469371796\n",
      "\tLoss: 0.15866753458976746\n",
      "\tLoss: 0.13379794359207153\n",
      "\tLoss: 0.14064228534698486\n",
      "\tLoss: 0.1618153154850006\n",
      "\tLoss: 0.13851425051689148\n",
      "\tLoss: 0.12715861201286316\n",
      "\tLoss: 0.08617885410785675\n",
      "\tLoss: 0.07757106423377991\n",
      "\tLoss: 0.13168060779571533\n",
      "\tLoss: 0.15506407618522644\n",
      "\tLoss: 0.13220348954200745\n",
      "\tLoss: 0.10995227098464966\n",
      "\tLoss: 0.11379304528236389\n",
      "\tLoss: 0.16776826977729797\n",
      "\tLoss: 0.12941518425941467\n",
      "\tLoss: 0.08891980350017548\n",
      "\tLoss: 0.0833098441362381\n",
      "\tLoss: 0.09857043623924255\n",
      "\tLoss: 0.12100844085216522\n",
      "\tLoss: 0.17662841081619263\n",
      "\tLoss: 0.14812377095222473\n",
      "\tLoss: 0.08001673221588135\n",
      "\tLoss: 0.1104389876127243\n",
      "\tLoss: 0.10739003866910934\n",
      "\tLoss: 0.10037868469953537\n",
      "\tLoss: 0.13998612761497498\n",
      "\tLoss: 0.19234323501586914\n",
      "\tLoss: 0.11160662770271301\n",
      "\tLoss: 0.0814264565706253\n",
      "\tLoss: 0.11128386110067368\n",
      "\tLoss: 0.11845371127128601\n",
      "\tLoss: 0.14191913604736328\n",
      "\tLoss: 0.0975639745593071\n",
      "\tLoss: 0.1420609951019287\n",
      "\tLoss: 0.06403765827417374\n",
      "\tLoss: 0.1739531308412552\n",
      "\tLoss: 0.058812208473682404\n",
      "\tLoss: 0.16877353191375732\n",
      "\tLoss: 0.2050904929637909\n",
      "\tLoss: 0.11390665173530579\n",
      "\tLoss: 0.13271057605743408\n",
      "\tLoss: 0.0831693634390831\n",
      "\tLoss: 0.1272459179162979\n",
      "\tLoss: 0.1372300535440445\n",
      "\tLoss: 0.1380159854888916\n",
      "\tLoss: 0.11893171072006226\n",
      "\tLoss: 0.05228371173143387\n",
      "\tLoss: 0.11129232496023178\n",
      "\tLoss: 0.143110454082489\n",
      "\tLoss: 0.11401866376399994\n",
      "\tLoss: 0.16929033398628235\n",
      "\tLoss: 0.14244544506072998\n",
      "\tLoss: 0.1404334157705307\n",
      "\tLoss: 0.1292046308517456\n",
      "\tLoss: 0.1289450228214264\n",
      "\tLoss: 0.12378891557455063\n",
      "\tLoss: 0.16602277755737305\n",
      "\tLoss: 0.12575271725654602\n",
      "\tLoss: 0.11066821217536926\n",
      "\tLoss: 0.16244414448738098\n",
      "\tLoss: 0.0891570895910263\n",
      "\tLoss: 0.12291765213012695\n",
      "\tLoss: 0.0983714908361435\n",
      "\tLoss: 0.13907021284103394\n",
      "\tLoss: 0.120390884578228\n",
      "\tLoss: 0.11107593774795532\n",
      "\tLoss: 0.1911899745464325\n",
      "\tLoss: 0.13808560371398926\n",
      "\tLoss: 0.09847208857536316\n",
      "\tLoss: 0.25770896673202515\n",
      "\tLoss: 0.12628990411758423\n",
      "\tLoss: 0.11970996111631393\n",
      "\tLoss: 0.08721042424440384\n",
      "\tLoss: 0.15268638730049133\n",
      "[time] Epoch 15: 433.65208028210327s = 7.227534671368388m\n",
      "\n",
      "Epoch 16...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.11982735246419907\n",
      "\tLoss: 0.16314446926116943\n",
      "\tLoss: 0.1384778469800949\n",
      "\tLoss: 0.15242570638656616\n",
      "\tLoss: 0.08386161923408508\n",
      "\tLoss: 0.16494427621364594\n",
      "\tLoss: 0.11380824446678162\n",
      "\tLoss: 0.1311720907688141\n",
      "\tLoss: 0.11187790334224701\n",
      "\tLoss: 0.12840500473976135\n",
      "\tLoss: 0.14352890849113464\n",
      "\tLoss: 0.18090340495109558\n",
      "\tLoss: 0.13548442721366882\n",
      "\tLoss: 0.11562420427799225\n",
      "\tLoss: 0.11153943836688995\n",
      "\tLoss: 0.137928768992424\n",
      "\tLoss: 0.14905233681201935\n",
      "\tLoss: 0.11009366810321808\n",
      "\tLoss: 0.12077860534191132\n",
      "\tLoss: 0.07355688512325287\n",
      "\tLoss: 0.1274704486131668\n",
      "\tLoss: 0.1361706256866455\n",
      "\tLoss: 0.12174269556999207\n",
      "\tLoss: 0.10333362221717834\n",
      "\tLoss: 0.08505908399820328\n",
      "\tLoss: 0.10135000199079514\n",
      "\tLoss: 0.0711597353219986\n",
      "\tLoss: 0.11184270679950714\n",
      "\tLoss: 0.11795216053724289\n",
      "\tLoss: 0.11535961925983429\n",
      "\tLoss: 0.13707134127616882\n",
      "\tLoss: 0.09715599566698074\n",
      "\tLoss: 0.09844924509525299\n",
      "\tLoss: 0.10668282955884933\n",
      "\tLoss: 0.14677153527736664\n",
      "\tLoss: 0.08913379162549973\n",
      "\tLoss: 0.11543150246143341\n",
      "\tLoss: 0.1381000131368637\n",
      "\tLoss: 0.08644551783800125\n",
      "\tLoss: 0.10227450728416443\n",
      "\tLoss: 0.1531941443681717\n",
      "\tLoss: 0.13184717297554016\n",
      "\tLoss: 0.14058206975460052\n",
      "\tLoss: 0.1341075599193573\n",
      "\tLoss: 0.12301452457904816\n",
      "\tLoss: 0.129157155752182\n",
      "\tLoss: 0.10324371606111526\n",
      "\tLoss: 0.15337179601192474\n",
      "\tLoss: 0.21818482875823975\n",
      "\tLoss: 0.13966773450374603\n",
      "\tLoss: 0.12493438273668289\n",
      "\tLoss: 0.18037432432174683\n",
      "\tLoss: 0.20941826701164246\n",
      "\tLoss: 0.14834222197532654\n",
      "\tLoss: 0.13687625527381897\n",
      "\tLoss: 0.15526729822158813\n",
      "\tLoss: 0.07483009248971939\n",
      "\tLoss: 0.17212074995040894\n",
      "\tLoss: 0.22439855337142944\n",
      "\tLoss: 0.13565632700920105\n",
      "\tLoss: 0.08607244491577148\n",
      "\tLoss: 0.10641983151435852\n",
      "\tLoss: 0.10442042350769043\n",
      "\tLoss: 0.17056581377983093\n",
      "\tLoss: 0.09685578942298889\n",
      "\tLoss: 0.17461347579956055\n",
      "\tLoss: 0.0926780253648758\n",
      "\tLoss: 0.1441565901041031\n",
      "\tLoss: 0.09454503655433655\n",
      "\tLoss: 0.17732460796833038\n",
      "\tLoss: 0.13110321760177612\n",
      "\tLoss: 0.10755661875009537\n",
      "\tLoss: 0.11388646811246872\n",
      "\tLoss: 0.10457581281661987\n",
      "\tLoss: 0.12446409463882446\n",
      "\tLoss: 0.14829006791114807\n",
      "\tLoss: 0.142283096909523\n",
      "\tLoss: 0.12033173441886902\n",
      "\tLoss: 0.1393108069896698\n",
      "\tLoss: 0.10112375020980835\n",
      "\tLoss: 0.131367489695549\n",
      "\tLoss: 0.1280735433101654\n",
      "\tLoss: 0.12243586033582687\n",
      "\tLoss: 0.11342217773199081\n",
      "\tLoss: 0.13047285377979279\n",
      "\tLoss: 0.15417389571666718\n",
      "\tLoss: 0.1220206692814827\n",
      "\tLoss: 0.12583830952644348\n",
      "\tLoss: 0.1284375935792923\n",
      "\tLoss: 0.07986316829919815\n",
      "\tLoss: 0.10978085547685623\n",
      "\tLoss: 0.13103598356246948\n",
      "\tLoss: 0.16112299263477325\n",
      "\tLoss: 0.08985923230648041\n",
      "\tLoss: 0.1098361387848854\n",
      "\tLoss: 0.08681147545576096\n",
      "\tLoss: 0.14091524481773376\n",
      "\tLoss: 0.17126934230327606\n",
      "\tLoss: 0.09731253236532211\n",
      "\tLoss: 0.1204662024974823\n",
      "\tLoss: 0.18922686576843262\n",
      "\tLoss: 0.09109339118003845\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.10078881680965424\n",
      "\tLoss: 0.11924734711647034\n",
      "\tLoss: 0.11212240159511566\n",
      "\tLoss: 0.1407174915075302\n",
      "\tLoss: 0.13028377294540405\n",
      "\tLoss: 0.15804718434810638\n",
      "\tLoss: 0.18485915660858154\n",
      "\tLoss: 0.10809049010276794\n",
      "\tLoss: 0.061703357845544815\n",
      "\tLoss: 0.08271730691194534\n",
      "\tLoss: 0.12632225453853607\n",
      "\tLoss: 0.1478809267282486\n",
      "\tLoss: 0.1372276395559311\n",
      "\tLoss: 0.12683773040771484\n",
      "\tLoss: 0.1372925490140915\n",
      "\tLoss: 0.09353587031364441\n",
      "\tLoss: 0.11774160712957382\n",
      "\tLoss: 0.13961559534072876\n",
      "\tLoss: 0.11040712893009186\n",
      "\tLoss: 0.16219188272953033\n",
      "\tLoss: 0.1553938239812851\n",
      "\tLoss: 0.1437882035970688\n",
      "\tLoss: 0.13001516461372375\n",
      "\tLoss: 0.16774851083755493\n",
      "\tLoss: 0.08447199314832687\n",
      "\tLoss: 0.16158345341682434\n",
      "\tLoss: 0.09313280135393143\n",
      "\tLoss: 0.13885650038719177\n",
      "\tLoss: 0.2433129996061325\n",
      "\tLoss: 0.08524321019649506\n",
      "\tLoss: 0.09459187090396881\n",
      "\tLoss: 0.0614946112036705\n",
      "\tLoss: 0.15810707211494446\n",
      "\tLoss: 0.13252298533916473\n",
      "\tLoss: 0.1418570578098297\n",
      "\tLoss: 0.15017756819725037\n",
      "\tLoss: 0.1478503942489624\n",
      "\tLoss: 0.1587764024734497\n",
      "\tLoss: 0.18251945078372955\n",
      "\tLoss: 0.1619269996881485\n",
      "\tLoss: 0.07979363203048706\n",
      "\tLoss: 0.09687670320272446\n",
      "\tLoss: 0.1402260661125183\n",
      "\tLoss: 0.10476319491863251\n",
      "\tLoss: 0.1400890350341797\n",
      "\tLoss: 0.12058320641517639\n",
      "\tLoss: 0.17738114297389984\n",
      "\tLoss: 0.1642078161239624\n",
      "\tLoss: 0.12047753483057022\n",
      "\tLoss: 0.12461967021226883\n",
      "\tLoss: 0.12121579796075821\n",
      "\tLoss: 0.2059093415737152\n",
      "\tLoss: 0.13926580548286438\n",
      "\tLoss: 0.20416811108589172\n",
      "\tLoss: 0.13675084710121155\n",
      "\tLoss: 0.15926676988601685\n",
      "\tLoss: 0.1789872646331787\n",
      "\tLoss: 0.12225893884897232\n",
      "\tLoss: 0.11266626417636871\n",
      "\tLoss: 0.07808497548103333\n",
      "\tLoss: 0.10271598398685455\n",
      "\tLoss: 0.14788982272148132\n",
      "\tLoss: 0.12250353395938873\n",
      "\tLoss: 0.1248856633901596\n",
      "\tLoss: 0.12251488864421844\n",
      "\tLoss: 0.14282241463661194\n",
      "\tLoss: 0.24028250575065613\n",
      "\tLoss: 0.10537847876548767\n",
      "\tLoss: 0.07825217396020889\n",
      "\tLoss: 0.19293524324893951\n",
      "\tLoss: 0.20456714928150177\n",
      "\tLoss: 0.1643674075603485\n",
      "\tLoss: 0.07270991802215576\n",
      "\tLoss: 0.25416651368141174\n",
      "\tLoss: 0.19466525316238403\n",
      "\tLoss: 0.09606105834245682\n",
      "\tLoss: 0.09634458273649216\n",
      "\tLoss: 0.07231490314006805\n",
      "\tLoss: 0.13868919014930725\n",
      "\tLoss: 0.18521727621555328\n",
      "\tLoss: 0.12771035730838776\n",
      "\tLoss: 0.14180800318717957\n",
      "\tLoss: 0.1163790374994278\n",
      "\tLoss: 0.14821484684944153\n",
      "\tLoss: 0.11137771606445312\n",
      "\tLoss: 0.1137651801109314\n",
      "\tLoss: 0.15488162636756897\n",
      "\tLoss: 0.13685190677642822\n",
      "\tLoss: 0.12730205059051514\n",
      "\tLoss: 0.09929871559143066\n",
      "\tLoss: 0.11492882668972015\n",
      "\tLoss: 0.09935169667005539\n",
      "\tLoss: 0.15510082244873047\n",
      "\tLoss: 0.07831787317991257\n",
      "\tLoss: 0.093767911195755\n",
      "\tLoss: 0.1495610624551773\n",
      "\tLoss: 0.09709838032722473\n",
      "\tLoss: 0.12060639262199402\n",
      "\tLoss: 0.15474829077720642\n",
      "\tLoss: 0.1461242437362671\n",
      "\tLoss: 0.1659746766090393\n",
      "\tLoss: 0.1200857162475586\n",
      "[time] Epoch 16: 429.07167926989496s = 7.15119465449825m\n",
      "\n",
      "Epoch 17...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.08979685604572296\n",
      "\tLoss: 0.10523556172847748\n",
      "\tLoss: 0.15571823716163635\n",
      "\tLoss: 0.09767328202724457\n",
      "\tLoss: 0.128051295876503\n",
      "\tLoss: 0.1511690616607666\n",
      "\tLoss: 0.04501911997795105\n",
      "\tLoss: 0.16208648681640625\n",
      "\tLoss: 0.18324822187423706\n",
      "\tLoss: 0.15311801433563232\n",
      "\tLoss: 0.1359018087387085\n",
      "\tLoss: 0.07822299003601074\n",
      "\tLoss: 0.09920404106378555\n",
      "\tLoss: 0.10028193891048431\n",
      "\tLoss: 0.10189992189407349\n",
      "\tLoss: 0.13369086384773254\n",
      "\tLoss: 0.12408576905727386\n",
      "\tLoss: 0.16062647104263306\n",
      "\tLoss: 0.10145338624715805\n",
      "\tLoss: 0.10392941534519196\n",
      "\tLoss: 0.07953700423240662\n",
      "\tLoss: 0.1314544975757599\n",
      "\tLoss: 0.16280770301818848\n",
      "\tLoss: 0.14316117763519287\n",
      "\tLoss: 0.09743022918701172\n",
      "\tLoss: 0.18268141150474548\n",
      "\tLoss: 0.12629154324531555\n",
      "\tLoss: 0.11147160828113556\n",
      "\tLoss: 0.1041618287563324\n",
      "\tLoss: 0.09472563117742538\n",
      "\tLoss: 0.06610507518053055\n",
      "\tLoss: 0.16068321466445923\n",
      "\tLoss: 0.16859586536884308\n",
      "\tLoss: 0.11361010372638702\n",
      "\tLoss: 0.1319018006324768\n",
      "\tLoss: 0.16333910822868347\n",
      "\tLoss: 0.11732993274927139\n",
      "\tLoss: 0.11467055976390839\n",
      "\tLoss: 0.13533449172973633\n",
      "\tLoss: 0.0676630511879921\n",
      "\tLoss: 0.08339900523424149\n",
      "\tLoss: 0.12535816431045532\n",
      "\tLoss: 0.1568496823310852\n",
      "\tLoss: 0.11013059318065643\n",
      "\tLoss: 0.11204571276903152\n",
      "\tLoss: 0.10849695652723312\n",
      "\tLoss: 0.09220301359891891\n",
      "\tLoss: 0.12914223968982697\n",
      "\tLoss: 0.12650826573371887\n",
      "\tLoss: 0.1263032853603363\n",
      "\tLoss: 0.144730806350708\n",
      "\tLoss: 0.14631026983261108\n",
      "\tLoss: 0.08013199269771576\n",
      "\tLoss: 0.11966182291507721\n",
      "\tLoss: 0.12263042479753494\n",
      "\tLoss: 0.21193519234657288\n",
      "\tLoss: 0.20165300369262695\n",
      "\tLoss: 0.109127938747406\n",
      "\tLoss: 0.1251278966665268\n",
      "\tLoss: 0.05982835218310356\n",
      "\tLoss: 0.15123875439167023\n",
      "\tLoss: 0.07088780403137207\n",
      "\tLoss: 0.10920218378305435\n",
      "\tLoss: 0.09404154121875763\n",
      "\tLoss: 0.13347548246383667\n",
      "\tLoss: 0.16080212593078613\n",
      "\tLoss: 0.22073312103748322\n",
      "\tLoss: 0.13472144305706024\n",
      "\tLoss: 0.12774775922298431\n",
      "\tLoss: 0.04628797620534897\n",
      "\tLoss: 0.08110573887825012\n",
      "\tLoss: 0.10694288462400436\n",
      "\tLoss: 0.14459137618541718\n",
      "\tLoss: 0.12087710201740265\n",
      "\tLoss: 0.1618439108133316\n",
      "\tLoss: 0.16064515709877014\n",
      "\tLoss: 0.10542887449264526\n",
      "\tLoss: 0.17068099975585938\n",
      "\tLoss: 0.11162859201431274\n",
      "\tLoss: 0.12620128691196442\n",
      "\tLoss: 0.09515637159347534\n",
      "\tLoss: 0.15620869398117065\n",
      "\tLoss: 0.11963848769664764\n",
      "\tLoss: 0.203754261136055\n",
      "\tLoss: 0.08451134711503983\n",
      "\tLoss: 0.10561226308345795\n",
      "\tLoss: 0.12631429731845856\n",
      "\tLoss: 0.11073896288871765\n",
      "\tLoss: 0.10597644746303558\n",
      "\tLoss: 0.1335805058479309\n",
      "\tLoss: 0.12517213821411133\n",
      "\tLoss: 0.14196991920471191\n",
      "\tLoss: 0.18861451745033264\n",
      "\tLoss: 0.1374409794807434\n",
      "\tLoss: 0.11302980035543442\n",
      "\tLoss: 0.06864916533231735\n",
      "\tLoss: 0.16881594061851501\n",
      "\tLoss: 0.1234186664223671\n",
      "\tLoss: 0.11032040417194366\n",
      "\tLoss: 0.08409489691257477\n",
      "\tLoss: 0.1579875946044922\n",
      "\tLoss: 0.1324421763420105\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.05603110045194626\n",
      "\tLoss: 0.08546248078346252\n",
      "\tLoss: 0.12269070744514465\n",
      "\tLoss: 0.09219396114349365\n",
      "\tLoss: 0.15340149402618408\n",
      "\tLoss: 0.16459649801254272\n",
      "\tLoss: 0.14201578497886658\n",
      "\tLoss: 0.1223779022693634\n",
      "\tLoss: 0.08651378750801086\n",
      "\tLoss: 0.08825507014989853\n",
      "\tLoss: 0.11543591320514679\n",
      "\tLoss: 0.11867193877696991\n",
      "\tLoss: 0.12984082102775574\n",
      "\tLoss: 0.03767774626612663\n",
      "\tLoss: 0.09654397517442703\n",
      "\tLoss: 0.11654245853424072\n",
      "\tLoss: 0.09913000464439392\n",
      "\tLoss: 0.14336466789245605\n",
      "\tLoss: 0.10304545611143112\n",
      "\tLoss: 0.08970017731189728\n",
      "\tLoss: 0.19794528186321259\n",
      "\tLoss: 0.11082428693771362\n",
      "\tLoss: 0.12956014275550842\n",
      "\tLoss: 0.1810453236103058\n",
      "\tLoss: 0.08983208239078522\n",
      "\tLoss: 0.12292401492595673\n",
      "\tLoss: 0.10694067180156708\n",
      "\tLoss: 0.10829412192106247\n",
      "\tLoss: 0.14331266283988953\n",
      "\tLoss: 0.15085771679878235\n",
      "\tLoss: 0.14247293770313263\n",
      "\tLoss: 0.09626559913158417\n",
      "\tLoss: 0.12556785345077515\n",
      "\tLoss: 0.12123160064220428\n",
      "\tLoss: 0.09271277487277985\n",
      "\tLoss: 0.10171353071928024\n",
      "\tLoss: 0.13221578299999237\n",
      "\tLoss: 0.1309100240468979\n",
      "\tLoss: 0.0877576470375061\n",
      "\tLoss: 0.13583281636238098\n",
      "\tLoss: 0.10970581322908401\n",
      "\tLoss: 0.18067646026611328\n",
      "\tLoss: 0.08393844217061996\n",
      "\tLoss: 0.1154298186302185\n",
      "\tLoss: 0.11459571123123169\n",
      "\tLoss: 0.17141208052635193\n",
      "\tLoss: 0.09847690165042877\n",
      "\tLoss: 0.08998987078666687\n",
      "\tLoss: 0.09878867864608765\n",
      "\tLoss: 0.1576218158006668\n",
      "\tLoss: 0.16367343068122864\n",
      "\tLoss: 0.13801690936088562\n",
      "\tLoss: 0.15511974692344666\n",
      "\tLoss: 0.12701202929019928\n",
      "\tLoss: 0.1157858744263649\n",
      "\tLoss: 0.10388294607400894\n",
      "\tLoss: 0.1298774927854538\n",
      "\tLoss: 0.14295589923858643\n",
      "\tLoss: 0.10152442753314972\n",
      "\tLoss: 0.14849644899368286\n",
      "\tLoss: 0.11206661909818649\n",
      "\tLoss: 0.14116239547729492\n",
      "\tLoss: 0.07894089818000793\n",
      "\tLoss: 0.12189709395170212\n",
      "\tLoss: 0.17109109461307526\n",
      "\tLoss: 0.13753756880760193\n",
      "\tLoss: 0.10797271877527237\n",
      "\tLoss: 0.1118370071053505\n",
      "\tLoss: 0.08831557631492615\n",
      "\tLoss: 0.1809486746788025\n",
      "\tLoss: 0.061395421624183655\n",
      "\tLoss: 0.09425200521945953\n",
      "\tLoss: 0.16486750543117523\n",
      "\tLoss: 0.07969703525304794\n",
      "\tLoss: 0.07421933114528656\n",
      "\tLoss: 0.09744417667388916\n",
      "\tLoss: 0.16204848885536194\n",
      "\tLoss: 0.12255385518074036\n",
      "\tLoss: 0.09574022889137268\n",
      "\tLoss: 0.14305920898914337\n",
      "\tLoss: 0.12166373431682587\n",
      "\tLoss: 0.13621529936790466\n",
      "\tLoss: 0.14853981137275696\n",
      "\tLoss: 0.11853352189064026\n",
      "\tLoss: 0.09881982207298279\n",
      "\tLoss: 0.11266063153743744\n",
      "\tLoss: 0.15574252605438232\n",
      "\tLoss: 0.09070401638746262\n",
      "\tLoss: 0.11460112780332565\n",
      "\tLoss: 0.1244037002325058\n",
      "\tLoss: 0.11504463106393814\n",
      "\tLoss: 0.10727672278881073\n",
      "\tLoss: 0.10535401105880737\n",
      "\tLoss: 0.13485392928123474\n",
      "\tLoss: 0.11337258666753769\n",
      "\tLoss: 0.14904573559761047\n",
      "\tLoss: 0.09593525528907776\n",
      "\tLoss: 0.08979252725839615\n",
      "\tLoss: 0.13724300265312195\n",
      "\tLoss: 0.12533894181251526\n",
      "\tLoss: 0.13750392198562622\n",
      "\tLoss: 0.1495584100484848\n",
      "[time] Epoch 17: 435.681391405873s = 7.261356523431217m\n",
      "\n",
      "Epoch 18...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.10919808596372604\n",
      "\tLoss: 0.12324608862400055\n",
      "\tLoss: 0.10260391235351562\n",
      "\tLoss: 0.09567981958389282\n",
      "\tLoss: 0.166807621717453\n",
      "\tLoss: 0.20040367543697357\n",
      "\tLoss: 0.12687182426452637\n",
      "\tLoss: 0.17407365143299103\n",
      "\tLoss: 0.13855242729187012\n",
      "\tLoss: 0.16390147805213928\n",
      "\tLoss: 0.08462263643741608\n",
      "\tLoss: 0.10256880521774292\n",
      "\tLoss: 0.10209944099187851\n",
      "\tLoss: 0.11799483001232147\n",
      "\tLoss: 0.07986564189195633\n",
      "\tLoss: 0.12156899273395538\n",
      "\tLoss: 0.118223175406456\n",
      "\tLoss: 0.11492294073104858\n",
      "\tLoss: 0.09014791995286942\n",
      "\tLoss: 0.13714471459388733\n",
      "\tLoss: 0.058212071657180786\n",
      "\tLoss: 0.14329937100410461\n",
      "\tLoss: 0.12039093673229218\n",
      "\tLoss: 0.10689336806535721\n",
      "\tLoss: 0.14744551479816437\n",
      "\tLoss: 0.12900283932685852\n",
      "\tLoss: 0.1173558309674263\n",
      "\tLoss: 0.11289778351783752\n",
      "\tLoss: 0.0881795883178711\n",
      "\tLoss: 0.12969732284545898\n",
      "\tLoss: 0.1394168585538864\n",
      "\tLoss: 0.11554190516471863\n",
      "\tLoss: 0.1158643439412117\n",
      "\tLoss: 0.0985691249370575\n",
      "\tLoss: 0.14649823307991028\n",
      "\tLoss: 0.09308482706546783\n",
      "\tLoss: 0.15055784583091736\n",
      "\tLoss: 0.10252196341753006\n",
      "\tLoss: 0.07301600277423859\n",
      "\tLoss: 0.13639011979103088\n",
      "\tLoss: 0.09935720264911652\n",
      "\tLoss: 0.08658190816640854\n",
      "\tLoss: 0.07624716311693192\n",
      "\tLoss: 0.13368654251098633\n",
      "\tLoss: 0.14735791087150574\n",
      "\tLoss: 0.1434318721294403\n",
      "\tLoss: 0.12624824047088623\n",
      "\tLoss: 0.1664121448993683\n",
      "\tLoss: 0.09786435961723328\n",
      "\tLoss: 0.1276797652244568\n",
      "\tLoss: 0.08233609795570374\n",
      "\tLoss: 0.11246904730796814\n",
      "\tLoss: 0.1632964313030243\n",
      "\tLoss: 0.15137186646461487\n",
      "\tLoss: 0.15961626172065735\n",
      "\tLoss: 0.11179226636886597\n",
      "\tLoss: 0.19097097218036652\n",
      "\tLoss: 0.14622226357460022\n",
      "\tLoss: 0.217512845993042\n",
      "\tLoss: 0.17473354935646057\n",
      "\tLoss: 0.14754945039749146\n",
      "\tLoss: 0.09694742411375046\n",
      "\tLoss: 0.15332090854644775\n",
      "\tLoss: 0.16129767894744873\n",
      "\tLoss: 0.07698702067136765\n",
      "\tLoss: 0.08430490642786026\n",
      "\tLoss: 0.09770572185516357\n",
      "\tLoss: 0.09600131958723068\n",
      "\tLoss: 0.12939177453517914\n",
      "\tLoss: 0.11260345578193665\n",
      "\tLoss: 0.19238071143627167\n",
      "\tLoss: 0.22270670533180237\n",
      "\tLoss: 0.1495778113603592\n",
      "\tLoss: 0.14157280325889587\n",
      "\tLoss: 0.11998909711837769\n",
      "\tLoss: 0.15038025379180908\n",
      "\tLoss: 0.18254461884498596\n",
      "\tLoss: 0.09921573102474213\n",
      "\tLoss: 0.14203491806983948\n",
      "\tLoss: 0.15172168612480164\n",
      "\tLoss: 0.12028209120035172\n",
      "\tLoss: 0.13798564672470093\n",
      "\tLoss: 0.1278897523880005\n",
      "\tLoss: 0.1237538531422615\n",
      "\tLoss: 0.13754291832447052\n",
      "\tLoss: 0.13729965686798096\n",
      "\tLoss: 0.14943410456180573\n",
      "\tLoss: 0.06805253028869629\n",
      "\tLoss: 0.20998390018939972\n",
      "\tLoss: 0.11118348687887192\n",
      "\tLoss: 0.15363390743732452\n",
      "\tLoss: 0.07342780381441116\n",
      "\tLoss: 0.14840111136436462\n",
      "\tLoss: 0.14950717985630035\n",
      "\tLoss: 0.18584555387496948\n",
      "\tLoss: 0.16628742218017578\n",
      "\tLoss: 0.19378918409347534\n",
      "\tLoss: 0.08712070435285568\n",
      "\tLoss: 0.19185757637023926\n",
      "\tLoss: 0.11528830975294113\n",
      "\tLoss: 0.10169878602027893\n",
      "\tLoss: 0.13117468357086182\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.11382618546485901\n",
      "\tLoss: 0.14606410264968872\n",
      "\tLoss: 0.07401897758245468\n",
      "\tLoss: 0.16424116492271423\n",
      "\tLoss: 0.15055346488952637\n",
      "\tLoss: 0.13860055804252625\n",
      "\tLoss: 0.15419116616249084\n",
      "\tLoss: 0.1498633474111557\n",
      "\tLoss: 0.15519294142723083\n",
      "\tLoss: 0.09352146089076996\n",
      "\tLoss: 0.11468446254730225\n",
      "\tLoss: 0.15864227712154388\n",
      "\tLoss: 0.1271834373474121\n",
      "\tLoss: 0.18188878893852234\n",
      "\tLoss: 0.09134714305400848\n",
      "\tLoss: 0.13098062574863434\n",
      "\tLoss: 0.12366512417793274\n",
      "\tLoss: 0.1425211876630783\n",
      "\tLoss: 0.1332656741142273\n",
      "\tLoss: 0.11106899380683899\n",
      "\tLoss: 0.10119660198688507\n",
      "\tLoss: 0.2082064151763916\n",
      "\tLoss: 0.09100458025932312\n",
      "\tLoss: 0.16279399394989014\n",
      "\tLoss: 0.1011851504445076\n",
      "\tLoss: 0.16493183374404907\n",
      "\tLoss: 0.11574619263410568\n",
      "\tLoss: 0.18411430716514587\n",
      "\tLoss: 0.15024811029434204\n",
      "\tLoss: 0.12952344119548798\n",
      "\tLoss: 0.08814931660890579\n",
      "\tLoss: 0.15115489065647125\n",
      "\tLoss: 0.14940407872200012\n",
      "\tLoss: 0.15536361932754517\n",
      "\tLoss: 0.16529428958892822\n",
      "\tLoss: 0.15853267908096313\n",
      "\tLoss: 0.13227298855781555\n",
      "\tLoss: 0.12894919514656067\n",
      "\tLoss: 0.09531813114881516\n",
      "\tLoss: 0.1468428671360016\n",
      "\tLoss: 0.08000627160072327\n",
      "\tLoss: 0.17249929904937744\n",
      "\tLoss: 0.09411723166704178\n",
      "\tLoss: 0.1033458411693573\n",
      "\tLoss: 0.12495759129524231\n",
      "\tLoss: 0.16648517549037933\n",
      "\tLoss: 0.15353211760520935\n",
      "\tLoss: 0.12672249972820282\n",
      "\tLoss: 0.07020314037799835\n",
      "\tLoss: 0.12567651271820068\n",
      "\tLoss: 0.14801102876663208\n",
      "\tLoss: 0.12973812222480774\n",
      "\tLoss: 0.10454637557268143\n",
      "\tLoss: 0.12640391290187836\n",
      "\tLoss: 0.09774637222290039\n",
      "\tLoss: 0.18155333399772644\n",
      "\tLoss: 0.146347314119339\n",
      "\tLoss: 0.06919191777706146\n",
      "\tLoss: 0.11680693924427032\n",
      "\tLoss: 0.14261466264724731\n",
      "\tLoss: 0.18204225599765778\n",
      "\tLoss: 0.14989469945430756\n",
      "\tLoss: 0.1345839500427246\n",
      "\tLoss: 0.1188255101442337\n",
      "\tLoss: 0.14606639742851257\n",
      "\tLoss: 0.10515989363193512\n",
      "\tLoss: 0.1251649260520935\n",
      "\tLoss: 0.09912724792957306\n",
      "\tLoss: 0.13698852062225342\n",
      "\tLoss: 0.10849705338478088\n",
      "\tLoss: 0.12822973728179932\n",
      "\tLoss: 0.13026192784309387\n",
      "\tLoss: 0.14948606491088867\n",
      "\tLoss: 0.1060081347823143\n",
      "\tLoss: 0.1449304223060608\n",
      "\tLoss: 0.167106494307518\n",
      "\tLoss: 0.10053358227014542\n",
      "\tLoss: 0.1252211630344391\n",
      "\tLoss: 0.12218465656042099\n",
      "\tLoss: 0.12086333334445953\n",
      "\tLoss: 0.11987560987472534\n",
      "\tLoss: 0.14363496005535126\n",
      "\tLoss: 0.13101865351200104\n",
      "\tLoss: 0.12139816582202911\n",
      "\tLoss: 0.12177080661058426\n",
      "\tLoss: 0.11439335346221924\n",
      "\tLoss: 0.07860617339611053\n",
      "\tLoss: 0.11423155665397644\n",
      "\tLoss: 0.12551936507225037\n",
      "\tLoss: 0.15854433178901672\n",
      "\tLoss: 0.13960228860378265\n",
      "\tLoss: 0.13201963901519775\n",
      "\tLoss: 0.12196557968854904\n",
      "\tLoss: 0.10128727555274963\n",
      "\tLoss: 0.10802092403173447\n",
      "\tLoss: 0.057630494236946106\n",
      "\tLoss: 0.13170158863067627\n",
      "\tLoss: 0.14924407005310059\n",
      "\tLoss: 0.12746566534042358\n",
      "\tLoss: 0.17240968346595764\n",
      "\tLoss: 0.11201518028974533\n",
      "\tLoss: 0.18992866575717926\n",
      "[time] Epoch 18: 426.718100389s = 7.111968339816667m\n",
      "\n",
      "Epoch 19...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.07102343440055847\n",
      "\tLoss: 0.09222883731126785\n",
      "\tLoss: 0.13409726321697235\n",
      "\tLoss: 0.14367547631263733\n",
      "\tLoss: 0.08090945333242416\n",
      "\tLoss: 0.13943636417388916\n",
      "\tLoss: 0.146953284740448\n",
      "\tLoss: 0.1345137059688568\n",
      "\tLoss: 0.11365490406751633\n",
      "\tLoss: 0.13136598467826843\n",
      "\tLoss: 0.14358755946159363\n",
      "\tLoss: 0.0874854326248169\n",
      "\tLoss: 0.22030363976955414\n",
      "\tLoss: 0.1683601438999176\n",
      "\tLoss: 0.10520729422569275\n",
      "\tLoss: 0.14594806730747223\n",
      "\tLoss: 0.09216980636119843\n",
      "\tLoss: 0.07785262167453766\n",
      "\tLoss: 0.16881127655506134\n",
      "\tLoss: 0.10893256962299347\n",
      "\tLoss: 0.09626727551221848\n",
      "\tLoss: 0.0859227329492569\n",
      "\tLoss: 0.1219349205493927\n",
      "\tLoss: 0.10339228063821793\n",
      "\tLoss: 0.12453754246234894\n",
      "\tLoss: 0.16104914247989655\n",
      "\tLoss: 0.077082559466362\n",
      "\tLoss: 0.15177038311958313\n",
      "\tLoss: 0.09775489568710327\n",
      "\tLoss: 0.10744617879390717\n",
      "\tLoss: 0.12736991047859192\n",
      "\tLoss: 0.10782651603221893\n",
      "\tLoss: 0.12928584218025208\n",
      "\tLoss: 0.1360345184803009\n",
      "\tLoss: 0.12600864470005035\n",
      "\tLoss: 0.12506097555160522\n",
      "\tLoss: 0.10299871861934662\n",
      "\tLoss: 0.1144663393497467\n",
      "\tLoss: 0.09349796921014786\n",
      "\tLoss: 0.13985991477966309\n",
      "\tLoss: 0.14366918802261353\n",
      "\tLoss: 0.0930313766002655\n",
      "\tLoss: 0.15099689364433289\n",
      "\tLoss: 0.11052167415618896\n",
      "\tLoss: 0.12546700239181519\n",
      "\tLoss: 0.11016245931386948\n",
      "\tLoss: 0.13333618640899658\n",
      "\tLoss: 0.11096760630607605\n",
      "\tLoss: 0.12311609089374542\n",
      "\tLoss: 0.09585171192884445\n",
      "\tLoss: 0.15441778302192688\n",
      "\tLoss: 0.12514536082744598\n",
      "\tLoss: 0.1237359568476677\n",
      "\tLoss: 0.10610999166965485\n",
      "\tLoss: 0.10805107653141022\n",
      "\tLoss: 0.13943783938884735\n",
      "\tLoss: 0.20492008328437805\n",
      "\tLoss: 0.1664162576198578\n",
      "\tLoss: 0.15398012101650238\n",
      "\tLoss: 0.1239924505352974\n",
      "\tLoss: 0.14128604531288147\n",
      "\tLoss: 0.15986782312393188\n",
      "\tLoss: 0.10774670541286469\n",
      "\tLoss: 0.11642669886350632\n",
      "\tLoss: 0.11896568536758423\n",
      "\tLoss: 0.13058090209960938\n",
      "\tLoss: 0.10757707059383392\n",
      "\tLoss: 0.14832809567451477\n",
      "\tLoss: 0.11430861055850983\n",
      "\tLoss: 0.13127818703651428\n",
      "\tLoss: 0.09197694808244705\n",
      "\tLoss: 0.1711156964302063\n",
      "\tLoss: 0.10846340656280518\n",
      "\tLoss: 0.11110373586416245\n",
      "\tLoss: 0.2216484248638153\n",
      "\tLoss: 0.11534015089273453\n",
      "\tLoss: 0.06295879930257797\n",
      "\tLoss: 0.08494335412979126\n",
      "\tLoss: 0.1464349329471588\n",
      "\tLoss: 0.1460450291633606\n",
      "\tLoss: 0.07094795256853104\n",
      "\tLoss: 0.06813882291316986\n",
      "\tLoss: 0.11966463923454285\n",
      "\tLoss: 0.10715332627296448\n",
      "\tLoss: 0.13733381032943726\n",
      "\tLoss: 0.1013592854142189\n",
      "\tLoss: 0.08673858642578125\n",
      "\tLoss: 0.1488354504108429\n",
      "\tLoss: 0.058712176978588104\n",
      "\tLoss: 0.16260205209255219\n",
      "\tLoss: 0.0797211229801178\n",
      "\tLoss: 0.1120709776878357\n",
      "\tLoss: 0.12480790168046951\n",
      "\tLoss: 0.15130272507667542\n",
      "\tLoss: 0.08449643850326538\n",
      "\tLoss: 0.11310119181871414\n",
      "\tLoss: 0.15652787685394287\n",
      "\tLoss: 0.16008102893829346\n",
      "\tLoss: 0.08904147148132324\n",
      "\tLoss: 0.16851139068603516\n",
      "\tLoss: 0.13413475453853607\n",
      "\tLoss: 0.1478959023952484\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.11286373436450958\n",
      "\tLoss: 0.1718328893184662\n",
      "\tLoss: 0.15598469972610474\n",
      "\tLoss: 0.16149991750717163\n",
      "\tLoss: 0.18076330423355103\n",
      "\tLoss: 0.08525820821523666\n",
      "\tLoss: 0.08522059768438339\n",
      "\tLoss: 0.09119705110788345\n",
      "\tLoss: 0.12157722562551498\n",
      "\tLoss: 0.12963762879371643\n",
      "\tLoss: 0.12030613422393799\n",
      "\tLoss: 0.1572737693786621\n",
      "\tLoss: 0.12284502387046814\n",
      "\tLoss: 0.0787510946393013\n",
      "\tLoss: 0.09752091765403748\n",
      "\tLoss: 0.15400703251361847\n",
      "\tLoss: 0.0967549979686737\n",
      "\tLoss: 0.08230022341012955\n",
      "\tLoss: 0.18211963772773743\n",
      "\tLoss: 0.1474258005619049\n",
      "\tLoss: 0.10967627912759781\n",
      "\tLoss: 0.1348610520362854\n",
      "\tLoss: 0.14107055962085724\n",
      "\tLoss: 0.0867106020450592\n",
      "\tLoss: 0.13654835522174835\n",
      "\tLoss: 0.14647114276885986\n",
      "\tLoss: 0.13882726430892944\n",
      "\tLoss: 0.16010168194770813\n",
      "\tLoss: 0.15008080005645752\n",
      "\tLoss: 0.11602999269962311\n",
      "\tLoss: 0.10756250470876694\n",
      "\tLoss: 0.14317390322685242\n",
      "\tLoss: 0.11958511173725128\n",
      "\tLoss: 0.12898968160152435\n",
      "\tLoss: 0.166518896818161\n",
      "\tLoss: 0.13188570737838745\n",
      "\tLoss: 0.0969260185956955\n",
      "\tLoss: 0.15784671902656555\n",
      "\tLoss: 0.11027785390615463\n",
      "\tLoss: 0.11996196210384369\n",
      "\tLoss: 0.10842521488666534\n",
      "\tLoss: 0.12878865003585815\n",
      "\tLoss: 0.1699153184890747\n",
      "\tLoss: 0.1283026486635208\n",
      "\tLoss: 0.09395233541727066\n",
      "\tLoss: 0.12099753320217133\n",
      "\tLoss: 0.09398557245731354\n",
      "\tLoss: 0.11919652670621872\n",
      "\tLoss: 0.15380534529685974\n",
      "\tLoss: 0.13766387104988098\n",
      "\tLoss: 0.0984000712633133\n",
      "\tLoss: 0.13754525780677795\n",
      "\tLoss: 0.166446715593338\n",
      "\tLoss: 0.11887696385383606\n",
      "\tLoss: 0.11163287609815598\n",
      "\tLoss: 0.11309921741485596\n",
      "\tLoss: 0.16876408457756042\n",
      "\tLoss: 0.13001427054405212\n",
      "\tLoss: 0.14537043869495392\n",
      "\tLoss: 0.1959524154663086\n",
      "\tLoss: 0.1305890530347824\n",
      "\tLoss: 0.17914387583732605\n",
      "\tLoss: 0.1142316609621048\n",
      "\tLoss: 0.06322519481182098\n",
      "\tLoss: 0.1242792159318924\n",
      "\tLoss: 0.13796845078468323\n",
      "\tLoss: 0.12508973479270935\n",
      "\tLoss: 0.13129201531410217\n",
      "\tLoss: 0.09813261032104492\n",
      "\tLoss: 0.06424207985401154\n",
      "\tLoss: 0.09128381311893463\n",
      "\tLoss: 0.13158583641052246\n",
      "\tLoss: 0.09441354125738144\n",
      "\tLoss: 0.12673848867416382\n",
      "\tLoss: 0.10983306169509888\n",
      "\tLoss: 0.05121302604675293\n",
      "\tLoss: 0.14098399877548218\n",
      "\tLoss: 0.13813653588294983\n",
      "\tLoss: 0.1045103520154953\n",
      "\tLoss: 0.15037888288497925\n",
      "\tLoss: 0.11553750932216644\n",
      "\tLoss: 0.12801732122898102\n",
      "\tLoss: 0.15584129095077515\n",
      "\tLoss: 0.1657925248146057\n",
      "\tLoss: 0.10819395631551743\n",
      "\tLoss: 0.13387008011341095\n",
      "\tLoss: 0.12893050909042358\n",
      "\tLoss: 0.1361272931098938\n",
      "\tLoss: 0.19818884134292603\n",
      "\tLoss: 0.17768892645835876\n",
      "\tLoss: 0.08422022312879562\n",
      "\tLoss: 0.08206629753112793\n",
      "\tLoss: 0.12112834304571152\n",
      "\tLoss: 0.10925281047821045\n",
      "\tLoss: 0.19529259204864502\n",
      "\tLoss: 0.09898175299167633\n",
      "\tLoss: 0.1565319299697876\n",
      "\tLoss: 0.04908730089664459\n",
      "\tLoss: 0.1129215732216835\n",
      "\tLoss: 0.06849080324172974\n",
      "\tLoss: 0.1221771240234375\n",
      "\tLoss: 0.13735303282737732\n",
      "[time] Epoch 19: 431.4325125836767s = 7.1905418763946125m\n",
      "\n",
      "Epoch 20...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.14441050589084625\n",
      "\tLoss: 0.10145693272352219\n",
      "\tLoss: 0.132735937833786\n",
      "\tLoss: 0.09169063717126846\n",
      "\tLoss: 0.1382855772972107\n",
      "\tLoss: 0.17524373531341553\n",
      "\tLoss: 0.100575752556324\n",
      "\tLoss: 0.11974845826625824\n",
      "\tLoss: 0.11265121400356293\n",
      "\tLoss: 0.1583513617515564\n",
      "\tLoss: 0.1080462858080864\n",
      "\tLoss: 0.14118048548698425\n",
      "\tLoss: 0.1101372241973877\n",
      "\tLoss: 0.07701025903224945\n",
      "\tLoss: 0.11257077753543854\n",
      "\tLoss: 0.16304178535938263\n",
      "\tLoss: 0.09169057011604309\n",
      "\tLoss: 0.0949254259467125\n",
      "\tLoss: 0.10555315017700195\n",
      "\tLoss: 0.07315760850906372\n",
      "\tLoss: 0.20103654265403748\n",
      "\tLoss: 0.08869539946317673\n",
      "\tLoss: 0.11368419229984283\n",
      "\tLoss: 0.18630480766296387\n",
      "\tLoss: 0.10662403702735901\n",
      "\tLoss: 0.09736265987157822\n",
      "\tLoss: 0.1498742252588272\n",
      "\tLoss: 0.10114653408527374\n",
      "\tLoss: 0.1951773464679718\n",
      "\tLoss: 0.09141705930233002\n",
      "\tLoss: 0.0801122635602951\n",
      "\tLoss: 0.15052828192710876\n",
      "\tLoss: 0.08090578019618988\n",
      "\tLoss: 0.16368533670902252\n",
      "\tLoss: 0.16995494067668915\n",
      "\tLoss: 0.19999949634075165\n",
      "\tLoss: 0.13476864993572235\n",
      "\tLoss: 0.14244775474071503\n",
      "\tLoss: 0.12353762239217758\n",
      "\tLoss: 0.12907695770263672\n",
      "\tLoss: 0.07856789231300354\n",
      "\tLoss: 0.13910891115665436\n",
      "\tLoss: 0.15093645453453064\n",
      "\tLoss: 0.1316957175731659\n",
      "\tLoss: 0.11566542088985443\n",
      "\tLoss: 0.11172035336494446\n",
      "\tLoss: 0.1006179004907608\n",
      "\tLoss: 0.08955463767051697\n",
      "\tLoss: 0.05542200431227684\n",
      "\tLoss: 0.12875233590602875\n",
      "\tLoss: 0.13145220279693604\n",
      "\tLoss: 0.14459611475467682\n",
      "\tLoss: 0.11330555379390717\n",
      "\tLoss: 0.15824131667613983\n",
      "\tLoss: 0.08027860522270203\n",
      "\tLoss: 0.1720140278339386\n",
      "\tLoss: 0.0915411040186882\n",
      "\tLoss: 0.10156471282243729\n",
      "\tLoss: 0.1043243408203125\n",
      "\tLoss: 0.13439536094665527\n",
      "\tLoss: 0.09919088333845139\n",
      "\tLoss: 0.21164342761039734\n",
      "\tLoss: 0.1366337388753891\n",
      "\tLoss: 0.11104200780391693\n",
      "\tLoss: 0.1276339292526245\n",
      "\tLoss: 0.09213230013847351\n",
      "\tLoss: 0.12275716662406921\n",
      "\tLoss: 0.16853925585746765\n",
      "\tLoss: 0.1777186095714569\n",
      "\tLoss: 0.13121283054351807\n",
      "\tLoss: 0.13325046002864838\n",
      "\tLoss: 0.12352296710014343\n",
      "\tLoss: 0.13768377900123596\n",
      "\tLoss: 0.15750998258590698\n",
      "\tLoss: 0.08299992233514786\n",
      "\tLoss: 0.137911856174469\n",
      "\tLoss: 0.13541167974472046\n",
      "\tLoss: 0.17708182334899902\n",
      "\tLoss: 0.10569799691438675\n",
      "\tLoss: 0.13162246346473694\n",
      "\tLoss: 0.097849041223526\n",
      "\tLoss: 0.18193945288658142\n",
      "\tLoss: 0.08529055118560791\n",
      "\tLoss: 0.15560302138328552\n",
      "\tLoss: 0.11713829636573792\n",
      "\tLoss: 0.15387681126594543\n",
      "\tLoss: 0.17162421345710754\n",
      "\tLoss: 0.14932116866111755\n",
      "\tLoss: 0.13658571243286133\n",
      "\tLoss: 0.10315181314945221\n",
      "\tLoss: 0.17754337191581726\n",
      "\tLoss: 0.08338330686092377\n",
      "\tLoss: 0.114876389503479\n",
      "\tLoss: 0.1757914423942566\n",
      "\tLoss: 0.13315829634666443\n",
      "\tLoss: 0.07464402168989182\n",
      "\tLoss: 0.12581607699394226\n",
      "\tLoss: 0.14330542087554932\n",
      "\tLoss: 0.09065540134906769\n",
      "\tLoss: 0.1039019376039505\n",
      "\tLoss: 0.07224743068218231\n",
      "\tLoss: 0.1743365228176117\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.08167975395917892\n",
      "\tLoss: 0.06865491718053818\n",
      "\tLoss: 0.13180193305015564\n",
      "\tLoss: 0.14957290887832642\n",
      "\tLoss: 0.09414525330066681\n",
      "\tLoss: 0.09378199279308319\n",
      "\tLoss: 0.09328882396221161\n",
      "\tLoss: 0.16977371275424957\n",
      "\tLoss: 0.1257537603378296\n",
      "\tLoss: 0.10186861455440521\n",
      "\tLoss: 0.1398773193359375\n",
      "\tLoss: 0.1124759167432785\n",
      "\tLoss: 0.10584640502929688\n",
      "\tLoss: 0.13039708137512207\n",
      "\tLoss: 0.09935271739959717\n",
      "\tLoss: 0.10848850011825562\n",
      "\tLoss: 0.14037269353866577\n",
      "\tLoss: 0.1700647473335266\n",
      "\tLoss: 0.14686691761016846\n",
      "\tLoss: 0.13585534691810608\n",
      "\tLoss: 0.12416420131921768\n",
      "\tLoss: 0.11431318521499634\n",
      "\tLoss: 0.12942270934581757\n",
      "\tLoss: 0.06882564723491669\n",
      "\tLoss: 0.15987195074558258\n",
      "\tLoss: 0.0987498015165329\n",
      "\tLoss: 0.11184116452932358\n",
      "\tLoss: 0.15131737291812897\n",
      "\tLoss: 0.15179389715194702\n",
      "\tLoss: 0.13073304295539856\n",
      "\tLoss: 0.14478296041488647\n",
      "\tLoss: 0.14015047252178192\n",
      "\tLoss: 0.08610205352306366\n",
      "\tLoss: 0.16114890575408936\n",
      "\tLoss: 0.15342161059379578\n",
      "\tLoss: 0.09080132842063904\n",
      "\tLoss: 0.11513689905405045\n",
      "\tLoss: 0.08059828728437424\n",
      "\tLoss: 0.12408901005983353\n",
      "\tLoss: 0.15835131704807281\n",
      "\tLoss: 0.13024836778640747\n",
      "\tLoss: 0.1406320184469223\n",
      "\tLoss: 0.20522290468215942\n",
      "\tLoss: 0.07458382844924927\n",
      "\tLoss: 0.1125851422548294\n",
      "\tLoss: 0.08660419285297394\n",
      "\tLoss: 0.1151355728507042\n",
      "\tLoss: 0.09993551671504974\n",
      "\tLoss: 0.16609559953212738\n",
      "\tLoss: 0.09574095904827118\n",
      "\tLoss: 0.10436458885669708\n",
      "\tLoss: 0.12201529741287231\n",
      "\tLoss: 0.11628083884716034\n",
      "\tLoss: 0.12792301177978516\n",
      "\tLoss: 0.0881766676902771\n",
      "\tLoss: 0.14434799551963806\n",
      "\tLoss: 0.1609632968902588\n",
      "\tLoss: 0.14592184126377106\n",
      "\tLoss: 0.14728251099586487\n",
      "\tLoss: 0.09819579124450684\n",
      "\tLoss: 0.2030092179775238\n",
      "\tLoss: 0.16078335046768188\n",
      "\tLoss: 0.10252144932746887\n",
      "\tLoss: 0.15527719259262085\n",
      "\tLoss: 0.1556522697210312\n",
      "\tLoss: 0.09773734956979752\n",
      "\tLoss: 0.1748526245355606\n",
      "\tLoss: 0.08832354843616486\n",
      "\tLoss: 0.07784495502710342\n",
      "\tLoss: 0.14554882049560547\n",
      "\tLoss: 0.0774526298046112\n",
      "\tLoss: 0.1271299570798874\n",
      "\tLoss: 0.09960050880908966\n",
      "\tLoss: 0.21331730484962463\n",
      "\tLoss: 0.14492318034172058\n",
      "\tLoss: 0.10156356543302536\n",
      "\tLoss: 0.1396799236536026\n",
      "\tLoss: 0.11309238523244858\n",
      "\tLoss: 0.09332649409770966\n",
      "\tLoss: 0.16758817434310913\n",
      "\tLoss: 0.12074834853410721\n",
      "\tLoss: 0.12743252515792847\n",
      "\tLoss: 0.15314927697181702\n",
      "\tLoss: 0.10915935039520264\n",
      "\tLoss: 0.09711508452892303\n",
      "\tLoss: 0.1659315824508667\n",
      "\tLoss: 0.13442741334438324\n",
      "\tLoss: 0.1794496476650238\n",
      "\tLoss: 0.12345845997333527\n",
      "\tLoss: 0.14943894743919373\n",
      "\tLoss: 0.10875984281301498\n",
      "\tLoss: 0.12750035524368286\n",
      "\tLoss: 0.0530405268073082\n",
      "\tLoss: 0.08013533055782318\n",
      "\tLoss: 0.16372618079185486\n",
      "\tLoss: 0.12934167683124542\n",
      "\tLoss: 0.09389986097812653\n",
      "\tLoss: 0.11056333035230637\n",
      "\tLoss: 0.10619135946035385\n",
      "\tLoss: 0.12761354446411133\n",
      "\tLoss: 0.13171735405921936\n",
      "\tLoss: 0.1375809758901596\n",
      "[time] Epoch 20: 432.9057343346067s = 7.215095572243444m\n",
      "\n",
      "Epoch 21...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.07403580844402313\n",
      "\tLoss: 0.09624134004116058\n",
      "\tLoss: 0.10991893708705902\n",
      "\tLoss: 0.18405240774154663\n",
      "\tLoss: 0.12398044764995575\n",
      "\tLoss: 0.1329927146434784\n",
      "\tLoss: 0.12004033476114273\n",
      "\tLoss: 0.09084539115428925\n",
      "\tLoss: 0.04800177738070488\n",
      "\tLoss: 0.10980943590402603\n",
      "\tLoss: 0.12831148505210876\n",
      "\tLoss: 0.09990782290697098\n",
      "\tLoss: 0.14309729635715485\n",
      "\tLoss: 0.10404874384403229\n",
      "\tLoss: 0.11474361270666122\n",
      "\tLoss: 0.1494935005903244\n",
      "\tLoss: 0.1059112548828125\n",
      "\tLoss: 0.1339537799358368\n",
      "\tLoss: 0.106183722615242\n",
      "\tLoss: 0.1399618685245514\n",
      "\tLoss: 0.14513005316257477\n",
      "\tLoss: 0.15678651630878448\n",
      "\tLoss: 0.15236464142799377\n",
      "\tLoss: 0.12921090424060822\n",
      "\tLoss: 0.1416654735803604\n",
      "\tLoss: 0.1172032505273819\n",
      "\tLoss: 0.13234835863113403\n",
      "\tLoss: 0.16835087537765503\n",
      "\tLoss: 0.14756377041339874\n",
      "\tLoss: 0.12609268724918365\n",
      "\tLoss: 0.11456166207790375\n",
      "\tLoss: 0.10361447930335999\n",
      "\tLoss: 0.1359107941389084\n",
      "\tLoss: 0.11296931654214859\n",
      "\tLoss: 0.0792793408036232\n",
      "\tLoss: 0.07080961018800735\n",
      "\tLoss: 0.08659124374389648\n",
      "\tLoss: 0.14134010672569275\n",
      "\tLoss: 0.16930802166461945\n",
      "\tLoss: 0.11415277421474457\n",
      "\tLoss: 0.1884945183992386\n",
      "\tLoss: 0.09299275279045105\n",
      "\tLoss: 0.12775152921676636\n",
      "\tLoss: 0.1496792435646057\n",
      "\tLoss: 0.09779977798461914\n",
      "\tLoss: 0.1678667813539505\n",
      "\tLoss: 0.0929308831691742\n",
      "\tLoss: 0.15328504145145416\n",
      "\tLoss: 0.15994912385940552\n",
      "\tLoss: 0.16467984020709991\n",
      "\tLoss: 0.13744333386421204\n",
      "\tLoss: 0.11953425407409668\n",
      "\tLoss: 0.07041151076555252\n",
      "\tLoss: 0.14963656663894653\n",
      "\tLoss: 0.1252463161945343\n",
      "\tLoss: 0.12008319795131683\n",
      "\tLoss: 0.10431642830371857\n",
      "\tLoss: 0.09579958766698837\n",
      "\tLoss: 0.1327175348997116\n",
      "\tLoss: 0.16240796446800232\n",
      "\tLoss: 0.13823969662189484\n",
      "\tLoss: 0.12458530068397522\n",
      "\tLoss: 0.17852360010147095\n",
      "\tLoss: 0.12284402549266815\n",
      "\tLoss: 0.10906491428613663\n",
      "\tLoss: 0.08165660500526428\n",
      "\tLoss: 0.15756817162036896\n",
      "\tLoss: 0.10993833839893341\n",
      "\tLoss: 0.07153856009244919\n",
      "\tLoss: 0.1339106410741806\n",
      "\tLoss: 0.14236196875572205\n",
      "\tLoss: 0.10341279208660126\n",
      "\tLoss: 0.13004830479621887\n",
      "\tLoss: 0.2026194930076599\n",
      "\tLoss: 0.1483142375946045\n",
      "\tLoss: 0.10107560455799103\n",
      "\tLoss: 0.09519767761230469\n",
      "\tLoss: 0.11055831611156464\n",
      "\tLoss: 0.041464000940322876\n",
      "\tLoss: 0.14575324952602386\n",
      "\tLoss: 0.15171702206134796\n",
      "\tLoss: 0.11953238397836685\n",
      "\tLoss: 0.09727639704942703\n",
      "\tLoss: 0.09070854634046555\n",
      "\tLoss: 0.12756434082984924\n",
      "\tLoss: 0.09557504951953888\n",
      "\tLoss: 0.11049861460924149\n",
      "\tLoss: 0.1283123791217804\n",
      "\tLoss: 0.07792317122220993\n",
      "\tLoss: 0.11367637664079666\n",
      "\tLoss: 0.17684239149093628\n",
      "\tLoss: 0.16217507421970367\n",
      "\tLoss: 0.13801902532577515\n",
      "\tLoss: 0.1612854152917862\n",
      "\tLoss: 0.12629516422748566\n",
      "\tLoss: 0.08726733922958374\n",
      "\tLoss: 0.08312651515007019\n",
      "\tLoss: 0.07686051726341248\n",
      "\tLoss: 0.1647467315196991\n",
      "\tLoss: 0.13647645711898804\n",
      "\tLoss: 0.1385042369365692\n",
      "\tLoss: 0.10900691151618958\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.09738186001777649\n",
      "\tLoss: 0.12906020879745483\n",
      "\tLoss: 0.15358932316303253\n",
      "\tLoss: 0.13944096863269806\n",
      "\tLoss: 0.09448511898517609\n",
      "\tLoss: 0.09762442111968994\n",
      "\tLoss: 0.1301945298910141\n",
      "\tLoss: 0.07454430311918259\n",
      "\tLoss: 0.08273512870073318\n",
      "\tLoss: 0.10073442757129669\n",
      "\tLoss: 0.14049550890922546\n",
      "\tLoss: 0.17639967799186707\n",
      "\tLoss: 0.08976656198501587\n",
      "\tLoss: 0.10942910611629486\n",
      "\tLoss: 0.17155572772026062\n",
      "\tLoss: 0.15890496969223022\n",
      "\tLoss: 0.15493056178092957\n",
      "\tLoss: 0.10110324621200562\n",
      "\tLoss: 0.06795748323202133\n",
      "\tLoss: 0.12440036237239838\n",
      "\tLoss: 0.1231093555688858\n",
      "\tLoss: 0.12365515530109406\n",
      "\tLoss: 0.1325191855430603\n",
      "\tLoss: 0.09936106204986572\n",
      "\tLoss: 0.11167801171541214\n",
      "\tLoss: 0.07337138056755066\n",
      "\tLoss: 0.14535072445869446\n",
      "\tLoss: 0.13407504558563232\n",
      "\tLoss: 0.12627564370632172\n",
      "\tLoss: 0.1059826910495758\n",
      "\tLoss: 0.12845100462436676\n",
      "\tLoss: 0.13208970427513123\n",
      "\tLoss: 0.1379614770412445\n",
      "\tLoss: 0.146025151014328\n",
      "\tLoss: 0.11203508824110031\n",
      "\tLoss: 0.13649947941303253\n",
      "\tLoss: 0.09726375341415405\n",
      "\tLoss: 0.06643257290124893\n",
      "\tLoss: 0.17820660769939423\n",
      "\tLoss: 0.13526587188243866\n",
      "\tLoss: 0.11812816560268402\n",
      "\tLoss: 0.13739889860153198\n",
      "\tLoss: 0.08532358705997467\n",
      "\tLoss: 0.12333393841981888\n",
      "\tLoss: 0.13596954941749573\n",
      "\tLoss: 0.0974382609128952\n",
      "\tLoss: 0.08758246153593063\n",
      "\tLoss: 0.11788048595190048\n",
      "\tLoss: 0.0855100080370903\n",
      "\tLoss: 0.08512059599161148\n",
      "\tLoss: 0.19465836882591248\n",
      "\tLoss: 0.14606931805610657\n",
      "\tLoss: 0.11306456476449966\n",
      "\tLoss: 0.10862253606319427\n",
      "\tLoss: 0.1652906835079193\n",
      "\tLoss: 0.12567800283432007\n",
      "\tLoss: 0.17576086521148682\n",
      "\tLoss: 0.09247362613677979\n",
      "\tLoss: 0.12966810166835785\n",
      "\tLoss: 0.09357980638742447\n",
      "\tLoss: 0.08265283703804016\n",
      "\tLoss: 0.10317514836788177\n",
      "\tLoss: 0.26367664337158203\n",
      "\tLoss: 0.1141558364033699\n",
      "\tLoss: 0.11690876632928848\n",
      "\tLoss: 0.08916755765676498\n",
      "\tLoss: 0.13845056295394897\n",
      "\tLoss: 0.12246623635292053\n",
      "\tLoss: 0.07837972044944763\n",
      "\tLoss: 0.0960780680179596\n",
      "\tLoss: 0.12005282193422318\n",
      "\tLoss: 0.1308971643447876\n",
      "\tLoss: 0.14975383877754211\n",
      "\tLoss: 0.12373475730419159\n",
      "\tLoss: 0.07434910535812378\n",
      "\tLoss: 0.1709745079278946\n",
      "\tLoss: 0.1151573434472084\n",
      "\tLoss: 0.1269281804561615\n",
      "\tLoss: 0.13326868414878845\n",
      "\tLoss: 0.10214406251907349\n",
      "\tLoss: 0.11563259363174438\n",
      "\tLoss: 0.0902639776468277\n",
      "\tLoss: 0.19980007410049438\n",
      "\tLoss: 0.11454262584447861\n",
      "\tLoss: 0.09044793248176575\n",
      "\tLoss: 0.10298524051904678\n",
      "\tLoss: 0.12431667745113373\n",
      "\tLoss: 0.13277742266654968\n",
      "\tLoss: 0.09725721925497055\n",
      "\tLoss: 0.09473054111003876\n",
      "\tLoss: 0.10246297717094421\n",
      "\tLoss: 0.07772526144981384\n",
      "\tLoss: 0.16375350952148438\n",
      "\tLoss: 0.1334981471300125\n",
      "\tLoss: 0.07838422060012817\n",
      "\tLoss: 0.16810007393360138\n",
      "\tLoss: 0.11777842789888382\n",
      "\tLoss: 0.14945146441459656\n",
      "\tLoss: 0.10437554121017456\n",
      "\tLoss: 0.07471784204244614\n",
      "\tLoss: 0.08339361846446991\n",
      "\tLoss: 0.13892951607704163\n",
      "[time] Epoch 21: 432.8462721211836s = 7.2141045353530595m\n",
      "\n",
      "Epoch 22...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.1419970840215683\n",
      "\tLoss: 0.15051379799842834\n",
      "\tLoss: 0.12848606705665588\n",
      "\tLoss: 0.12059710919857025\n",
      "\tLoss: 0.06054287031292915\n",
      "\tLoss: 0.1322738230228424\n",
      "\tLoss: 0.13338498771190643\n",
      "\tLoss: 0.14376944303512573\n",
      "\tLoss: 0.13766613602638245\n",
      "\tLoss: 0.17104023694992065\n",
      "\tLoss: 0.14297035336494446\n",
      "\tLoss: 0.10065357387065887\n",
      "\tLoss: 0.06622730940580368\n",
      "\tLoss: 0.15474393963813782\n",
      "\tLoss: 0.141776904463768\n",
      "\tLoss: 0.10476410388946533\n",
      "\tLoss: 0.1527268886566162\n",
      "\tLoss: 0.15946778655052185\n",
      "\tLoss: 0.07034426927566528\n",
      "\tLoss: 0.09440910071134567\n",
      "\tLoss: 0.12217868119478226\n",
      "\tLoss: 0.08682022988796234\n",
      "\tLoss: 0.16221857070922852\n",
      "\tLoss: 0.13094112277030945\n",
      "\tLoss: 0.17854857444763184\n",
      "\tLoss: 0.13392630219459534\n",
      "\tLoss: 0.1358066350221634\n",
      "\tLoss: 0.13508829474449158\n",
      "\tLoss: 0.1345413327217102\n",
      "\tLoss: 0.19080984592437744\n",
      "\tLoss: 0.10152283310890198\n",
      "\tLoss: 0.11565458029508591\n",
      "\tLoss: 0.16584444046020508\n",
      "\tLoss: 0.0894116461277008\n",
      "\tLoss: 0.0916946530342102\n",
      "\tLoss: 0.0698128193616867\n",
      "\tLoss: 0.10884144902229309\n",
      "\tLoss: 0.16754312813282013\n",
      "\tLoss: 0.1524442732334137\n",
      "\tLoss: 0.10884560644626617\n",
      "\tLoss: 0.17483700811862946\n",
      "\tLoss: 0.12281852960586548\n",
      "\tLoss: 0.1367875635623932\n",
      "\tLoss: 0.11624110490083694\n",
      "\tLoss: 0.10691104829311371\n",
      "\tLoss: 0.10406024008989334\n",
      "\tLoss: 0.13289430737495422\n",
      "\tLoss: 0.09175144881010056\n",
      "\tLoss: 0.14011648297309875\n",
      "\tLoss: 0.09313773363828659\n",
      "\tLoss: 0.1537909209728241\n",
      "\tLoss: 0.1030949205160141\n",
      "\tLoss: 0.18520577251911163\n",
      "\tLoss: 0.12303654849529266\n",
      "\tLoss: 0.11192677915096283\n",
      "\tLoss: 0.13064107298851013\n",
      "\tLoss: 0.13459621369838715\n",
      "\tLoss: 0.1427503526210785\n",
      "\tLoss: 0.10107198357582092\n",
      "\tLoss: 0.1398075670003891\n",
      "\tLoss: 0.11017362773418427\n",
      "\tLoss: 0.11082465946674347\n",
      "\tLoss: 0.11023683100938797\n",
      "\tLoss: 0.11644316464662552\n",
      "\tLoss: 0.16662847995758057\n",
      "\tLoss: 0.13471341133117676\n",
      "\tLoss: 0.18678084015846252\n",
      "\tLoss: 0.08892472088336945\n",
      "\tLoss: 0.08108842372894287\n",
      "\tLoss: 0.2067878544330597\n",
      "\tLoss: 0.1290488839149475\n",
      "\tLoss: 0.09447440505027771\n",
      "\tLoss: 0.21817220747470856\n",
      "\tLoss: 0.1589556634426117\n",
      "\tLoss: 0.14424031972885132\n",
      "\tLoss: 0.09983111172914505\n",
      "\tLoss: 0.11766963452100754\n",
      "\tLoss: 0.14014750719070435\n",
      "\tLoss: 0.1171879917383194\n",
      "\tLoss: 0.16043125092983246\n",
      "\tLoss: 0.12585508823394775\n",
      "\tLoss: 0.18392552435398102\n",
      "\tLoss: 0.17760908603668213\n",
      "\tLoss: 0.14185109734535217\n",
      "\tLoss: 0.11117120832204819\n",
      "\tLoss: 0.15378841757774353\n",
      "\tLoss: 0.14382532238960266\n",
      "\tLoss: 0.13298462331295013\n",
      "\tLoss: 0.18023492395877838\n",
      "\tLoss: 0.10835357010364532\n",
      "\tLoss: 0.1863354742527008\n",
      "\tLoss: 0.1269530951976776\n",
      "\tLoss: 0.1577531397342682\n",
      "\tLoss: 0.10546784847974777\n",
      "\tLoss: 0.1184891015291214\n",
      "\tLoss: 0.09547169506549835\n",
      "\tLoss: 0.168653205037117\n",
      "\tLoss: 0.11344137787818909\n",
      "\tLoss: 0.1413373053073883\n",
      "\tLoss: 0.16976352035999298\n",
      "\tLoss: 0.0924038514494896\n",
      "\tLoss: 0.13461995124816895\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.10736300051212311\n",
      "\tLoss: 0.23113517463207245\n",
      "\tLoss: 0.137032151222229\n",
      "\tLoss: 0.1676732897758484\n",
      "\tLoss: 0.12256856262683868\n",
      "\tLoss: 0.11842553317546844\n",
      "\tLoss: 0.1036791130900383\n",
      "\tLoss: 0.08771596103906631\n",
      "\tLoss: 0.1467161476612091\n",
      "\tLoss: 0.09230923652648926\n",
      "\tLoss: 0.12052998691797256\n",
      "\tLoss: 0.097262442111969\n",
      "\tLoss: 0.12490229308605194\n",
      "\tLoss: 0.07061062008142471\n",
      "\tLoss: 0.09065181016921997\n",
      "\tLoss: 0.1337113082408905\n",
      "\tLoss: 0.1362447291612625\n",
      "\tLoss: 0.11918839812278748\n",
      "\tLoss: 0.14011655747890472\n",
      "\tLoss: 0.08552934229373932\n",
      "\tLoss: 0.1207786425948143\n",
      "\tLoss: 0.14592987298965454\n",
      "\tLoss: 0.09230391681194305\n",
      "\tLoss: 0.1575162559747696\n",
      "\tLoss: 0.10560767352581024\n",
      "\tLoss: 0.07680986821651459\n",
      "\tLoss: 0.18185588717460632\n",
      "\tLoss: 0.09398060292005539\n",
      "\tLoss: 0.13540947437286377\n",
      "\tLoss: 0.09571295976638794\n",
      "\tLoss: 0.0959717333316803\n",
      "\tLoss: 0.14651250839233398\n",
      "\tLoss: 0.12276481091976166\n",
      "\tLoss: 0.10026812553405762\n",
      "\tLoss: 0.09354077279567719\n",
      "\tLoss: 0.1437387764453888\n",
      "\tLoss: 0.09954894334077835\n",
      "\tLoss: 0.1267833262681961\n",
      "\tLoss: 0.1302177608013153\n",
      "\tLoss: 0.1289844661951065\n",
      "\tLoss: 0.12647832930088043\n",
      "\tLoss: 0.10580439865589142\n",
      "\tLoss: 0.11536683142185211\n",
      "\tLoss: 0.12021555006504059\n",
      "\tLoss: 0.10346079617738724\n",
      "\tLoss: 0.10831785202026367\n",
      "\tLoss: 0.0905071571469307\n",
      "\tLoss: 0.08456294238567352\n",
      "\tLoss: 0.11732658743858337\n",
      "\tLoss: 0.1343667060136795\n",
      "\tLoss: 0.1113176941871643\n",
      "\tLoss: 0.07341724634170532\n",
      "\tLoss: 0.13109587132930756\n",
      "\tLoss: 0.10788562148809433\n",
      "\tLoss: 0.1077650636434555\n",
      "\tLoss: 0.06036629527807236\n",
      "\tLoss: 0.19929659366607666\n",
      "\tLoss: 0.1687462478876114\n",
      "\tLoss: 0.16209734976291656\n",
      "\tLoss: 0.12609615921974182\n",
      "\tLoss: 0.16928499937057495\n",
      "\tLoss: 0.12420879304409027\n",
      "\tLoss: 0.08955975621938705\n",
      "\tLoss: 0.100120410323143\n",
      "\tLoss: 0.1182253286242485\n",
      "\tLoss: 0.1636505424976349\n",
      "\tLoss: 0.12759876251220703\n",
      "\tLoss: 0.10930850356817245\n",
      "\tLoss: 0.16673535108566284\n",
      "\tLoss: 0.14021025598049164\n",
      "\tLoss: 0.1142403855919838\n",
      "\tLoss: 0.15836110711097717\n",
      "\tLoss: 0.1171611100435257\n",
      "\tLoss: 0.1499723196029663\n",
      "\tLoss: 0.10963284224271774\n",
      "\tLoss: 0.1214122325181961\n",
      "\tLoss: 0.082039475440979\n",
      "\tLoss: 0.148006409406662\n",
      "\tLoss: 0.157685786485672\n",
      "\tLoss: 0.09258410334587097\n",
      "\tLoss: 0.17252598702907562\n",
      "\tLoss: 0.12556913495063782\n",
      "\tLoss: 0.10650714486837387\n",
      "\tLoss: 0.1385490894317627\n",
      "\tLoss: 0.16804435849189758\n",
      "\tLoss: 0.2107388973236084\n",
      "\tLoss: 0.14007487893104553\n",
      "\tLoss: 0.09833640605211258\n",
      "\tLoss: 0.1432097852230072\n",
      "\tLoss: 0.14656102657318115\n",
      "\tLoss: 0.1389986127614975\n",
      "\tLoss: 0.09718842059373856\n",
      "\tLoss: 0.09237884730100632\n",
      "\tLoss: 0.13934831321239471\n",
      "\tLoss: 0.07863412797451019\n",
      "\tLoss: 0.12802338600158691\n",
      "\tLoss: 0.11706458032131195\n",
      "\tLoss: 0.1306963860988617\n",
      "\tLoss: 0.17164424061775208\n",
      "\tLoss: 0.11426210403442383\n",
      "\tLoss: 0.0876857340335846\n",
      "\tLoss: 0.10780959576368332\n",
      "[time] Epoch 22: 432.04531442793086s = 7.200755240465514m\n",
      "\n",
      "Epoch 23...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.07973608374595642\n",
      "\tLoss: 0.15685641765594482\n",
      "\tLoss: 0.08609817922115326\n",
      "\tLoss: 0.1252431422472\n",
      "\tLoss: 0.1040244847536087\n",
      "\tLoss: 0.12202008068561554\n",
      "\tLoss: 0.08128052204847336\n",
      "\tLoss: 0.11518800258636475\n",
      "\tLoss: 0.1687801480293274\n",
      "\tLoss: 0.07327156513929367\n",
      "\tLoss: 0.11511284112930298\n",
      "\tLoss: 0.13163572549819946\n",
      "\tLoss: 0.10660292953252792\n",
      "\tLoss: 0.06914166361093521\n",
      "\tLoss: 0.1617274135351181\n",
      "\tLoss: 0.0737152025103569\n",
      "\tLoss: 0.07796333730220795\n",
      "\tLoss: 0.1277771145105362\n",
      "\tLoss: 0.12935870885849\n",
      "\tLoss: 0.13571763038635254\n",
      "\tLoss: 0.09991707652807236\n",
      "\tLoss: 0.08613581955432892\n",
      "\tLoss: 0.16702574491500854\n",
      "\tLoss: 0.17510859668254852\n",
      "\tLoss: 0.1269863098859787\n",
      "\tLoss: 0.19250471889972687\n",
      "\tLoss: 0.09053674340248108\n",
      "\tLoss: 0.08394975960254669\n",
      "\tLoss: 0.14987590909004211\n",
      "\tLoss: 0.21179789304733276\n",
      "\tLoss: 0.12078474462032318\n",
      "\tLoss: 0.09509781002998352\n",
      "\tLoss: 0.09704748541116714\n",
      "\tLoss: 0.12072080373764038\n",
      "\tLoss: 0.12502938508987427\n",
      "\tLoss: 0.16831369698047638\n",
      "\tLoss: 0.11300820112228394\n",
      "\tLoss: 0.15434470772743225\n",
      "\tLoss: 0.08739512413740158\n",
      "\tLoss: 0.09925645589828491\n",
      "\tLoss: 0.0802471861243248\n",
      "\tLoss: 0.14344051480293274\n",
      "\tLoss: 0.11747127771377563\n",
      "\tLoss: 0.1037936881184578\n",
      "\tLoss: 0.06617539376020432\n",
      "\tLoss: 0.14398802816867828\n",
      "\tLoss: 0.09007512032985687\n",
      "\tLoss: 0.14131620526313782\n",
      "\tLoss: 0.12280783802270889\n",
      "\tLoss: 0.12178893387317657\n",
      "\tLoss: 0.14235155284404755\n",
      "\tLoss: 0.15167146921157837\n",
      "\tLoss: 0.13509637117385864\n",
      "\tLoss: 0.15041714906692505\n",
      "\tLoss: 0.08284759521484375\n",
      "\tLoss: 0.135358989238739\n",
      "\tLoss: 0.11762000620365143\n",
      "\tLoss: 0.1297636479139328\n",
      "\tLoss: 0.09881319105625153\n",
      "\tLoss: 0.0717724859714508\n",
      "\tLoss: 0.12595346570014954\n",
      "\tLoss: 0.1164039671421051\n",
      "\tLoss: 0.14592203497886658\n",
      "\tLoss: 0.09227949380874634\n",
      "\tLoss: 0.07924128323793411\n",
      "\tLoss: 0.08906835317611694\n",
      "\tLoss: 0.1697942167520523\n",
      "\tLoss: 0.1344756931066513\n",
      "\tLoss: 0.12025774270296097\n",
      "\tLoss: 0.10937566310167313\n",
      "\tLoss: 0.08249986916780472\n",
      "\tLoss: 0.11579369008541107\n",
      "\tLoss: 0.13812339305877686\n",
      "\tLoss: 0.06539636850357056\n",
      "\tLoss: 0.11740686744451523\n",
      "\tLoss: 0.1761903166770935\n",
      "\tLoss: 0.12944909930229187\n",
      "\tLoss: 0.1659158170223236\n",
      "\tLoss: 0.1935756802558899\n",
      "\tLoss: 0.16542193293571472\n",
      "\tLoss: 0.08700625598430634\n",
      "\tLoss: 0.11460401117801666\n",
      "\tLoss: 0.10279371589422226\n",
      "\tLoss: 0.12450946867465973\n",
      "\tLoss: 0.11046675592660904\n",
      "\tLoss: 0.06538788229227066\n",
      "\tLoss: 0.1566920280456543\n",
      "\tLoss: 0.14942798018455505\n",
      "\tLoss: 0.1639357954263687\n",
      "\tLoss: 0.21615971624851227\n",
      "\tLoss: 0.15448421239852905\n",
      "\tLoss: 0.12101823091506958\n",
      "\tLoss: 0.11927905678749084\n",
      "\tLoss: 0.13524065911769867\n",
      "\tLoss: 0.12398211658000946\n",
      "\tLoss: 0.11562490463256836\n",
      "\tLoss: 0.10801111161708832\n",
      "\tLoss: 0.13140574097633362\n",
      "\tLoss: 0.12572303414344788\n",
      "\tLoss: 0.13618722558021545\n",
      "\tLoss: 0.09829895198345184\n",
      "\tLoss: 0.08327062427997589\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.06235115975141525\n",
      "\tLoss: 0.13981550931930542\n",
      "\tLoss: 0.11229722201824188\n",
      "\tLoss: 0.10428749024868011\n",
      "\tLoss: 0.15912815928459167\n",
      "\tLoss: 0.12693317234516144\n",
      "\tLoss: 0.08082517981529236\n",
      "\tLoss: 0.1025313287973404\n",
      "\tLoss: 0.08629299700260162\n",
      "\tLoss: 0.12159228324890137\n",
      "\tLoss: 0.1305728554725647\n",
      "\tLoss: 0.1921767145395279\n",
      "\tLoss: 0.10655508190393448\n",
      "\tLoss: 0.0894489735364914\n",
      "\tLoss: 0.1857961118221283\n",
      "\tLoss: 0.08355753868818283\n",
      "\tLoss: 0.09315783530473709\n",
      "\tLoss: 0.13676612079143524\n",
      "\tLoss: 0.10426507145166397\n",
      "\tLoss: 0.15765449404716492\n",
      "\tLoss: 0.08771736174821854\n",
      "\tLoss: 0.12964048981666565\n",
      "\tLoss: 0.17511339485645294\n",
      "\tLoss: 0.15765641629695892\n",
      "\tLoss: 0.08296184241771698\n",
      "\tLoss: 0.13587136566638947\n",
      "\tLoss: 0.12945324182510376\n",
      "\tLoss: 0.12246716022491455\n",
      "\tLoss: 0.1847778558731079\n",
      "\tLoss: 0.10902746766805649\n",
      "\tLoss: 0.10740689933300018\n",
      "\tLoss: 0.06181349605321884\n",
      "\tLoss: 0.11642922461032867\n",
      "\tLoss: 0.09230875968933105\n",
      "\tLoss: 0.11747577041387558\n",
      "\tLoss: 0.13977184891700745\n",
      "\tLoss: 0.08607694506645203\n",
      "\tLoss: 0.14805874228477478\n",
      "\tLoss: 0.16527920961380005\n",
      "\tLoss: 0.15500599145889282\n",
      "\tLoss: 0.08596691489219666\n",
      "\tLoss: 0.1294000893831253\n",
      "\tLoss: 0.10362650454044342\n",
      "\tLoss: 0.09832318872213364\n",
      "\tLoss: 0.08837190270423889\n",
      "\tLoss: 0.20457524061203003\n",
      "\tLoss: 0.18862125277519226\n",
      "\tLoss: 0.16903936862945557\n",
      "\tLoss: 0.17778092622756958\n",
      "\tLoss: 0.11199124157428741\n",
      "\tLoss: 0.13688021898269653\n",
      "\tLoss: 0.12207528948783875\n",
      "\tLoss: 0.13562577962875366\n",
      "\tLoss: 0.19304457306861877\n",
      "\tLoss: 0.1315872073173523\n",
      "\tLoss: 0.13922689855098724\n",
      "\tLoss: 0.1654658019542694\n",
      "\tLoss: 0.18043409287929535\n",
      "\tLoss: 0.09988897293806076\n",
      "\tLoss: 0.16101016104221344\n",
      "\tLoss: 0.1737941950559616\n",
      "\tLoss: 0.07930983603000641\n",
      "\tLoss: 0.06652706116437912\n",
      "\tLoss: 0.15159302949905396\n",
      "\tLoss: 0.06274573504924774\n",
      "\tLoss: 0.1071014553308487\n",
      "\tLoss: 0.15825265645980835\n",
      "\tLoss: 0.11023139208555222\n",
      "\tLoss: 0.18622314929962158\n",
      "\tLoss: 0.12400668859481812\n",
      "\tLoss: 0.15204890072345734\n",
      "\tLoss: 0.12417417764663696\n",
      "\tLoss: 0.09113471210002899\n",
      "\tLoss: 0.16295942664146423\n",
      "\tLoss: 0.09056581556797028\n",
      "\tLoss: 0.07874180376529694\n",
      "\tLoss: 0.16158388555049896\n",
      "\tLoss: 0.176116943359375\n",
      "\tLoss: 0.14256882667541504\n",
      "\tLoss: 0.12915398180484772\n",
      "\tLoss: 0.12093659490346909\n",
      "\tLoss: 0.10565971583127975\n",
      "\tLoss: 0.13338428735733032\n",
      "\tLoss: 0.14763528108596802\n",
      "\tLoss: 0.09319688379764557\n",
      "\tLoss: 0.11233019083738327\n",
      "\tLoss: 0.1243124008178711\n",
      "\tLoss: 0.1786476969718933\n",
      "\tLoss: 0.10096743702888489\n",
      "\tLoss: 0.1521383821964264\n",
      "\tLoss: 0.15804682672023773\n",
      "\tLoss: 0.13577741384506226\n",
      "\tLoss: 0.12737737596035004\n",
      "\tLoss: 0.12637940049171448\n",
      "\tLoss: 0.15825755894184113\n",
      "\tLoss: 0.07554695010185242\n",
      "\tLoss: 0.10809896886348724\n",
      "\tLoss: 0.10311011970043182\n",
      "\tLoss: 0.13510102033615112\n",
      "\tLoss: 0.15363119542598724\n",
      "\tLoss: 0.18927592039108276\n",
      "\tLoss: 0.13435068726539612\n",
      "[time] Epoch 23: 426.9820007351227s = 7.116366678918712m\n",
      "\n",
      "Epoch 24...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.10166820883750916\n",
      "\tLoss: 0.12450143694877625\n",
      "\tLoss: 0.09137241542339325\n",
      "\tLoss: 0.12214994430541992\n",
      "\tLoss: 0.11283176392316818\n",
      "\tLoss: 0.12216641753911972\n",
      "\tLoss: 0.08831723034381866\n",
      "\tLoss: 0.1438274085521698\n",
      "\tLoss: 0.13130804896354675\n",
      "\tLoss: 0.14573442935943604\n",
      "\tLoss: 0.148616224527359\n",
      "\tLoss: 0.1675940454006195\n",
      "\tLoss: 0.13539236783981323\n",
      "\tLoss: 0.0861487090587616\n",
      "\tLoss: 0.20095723867416382\n",
      "\tLoss: 0.11717481911182404\n",
      "\tLoss: 0.12499357759952545\n",
      "\tLoss: 0.04680868238210678\n",
      "\tLoss: 0.14285385608673096\n",
      "\tLoss: 0.13533632457256317\n",
      "\tLoss: 0.1985149085521698\n",
      "\tLoss: 0.1849949061870575\n",
      "\tLoss: 0.09128738194704056\n",
      "\tLoss: 0.1296483874320984\n",
      "\tLoss: 0.08241211622953415\n",
      "\tLoss: 0.16443969309329987\n",
      "\tLoss: 0.11000956594944\n",
      "\tLoss: 0.0760168582201004\n",
      "\tLoss: 0.10711304098367691\n",
      "\tLoss: 0.16950267553329468\n",
      "\tLoss: 0.11157497763633728\n",
      "\tLoss: 0.12732544541358948\n",
      "\tLoss: 0.09090898931026459\n",
      "\tLoss: 0.09934782981872559\n",
      "\tLoss: 0.1424216330051422\n",
      "\tLoss: 0.15706640481948853\n",
      "\tLoss: 0.16235603392124176\n",
      "\tLoss: 0.13702909648418427\n",
      "\tLoss: 0.09192357957363129\n",
      "\tLoss: 0.14019924402236938\n",
      "\tLoss: 0.12342406809329987\n",
      "\tLoss: 0.1276175081729889\n",
      "\tLoss: 0.11875640600919724\n",
      "\tLoss: 0.08962971717119217\n",
      "\tLoss: 0.08846583962440491\n",
      "\tLoss: 0.11748412996530533\n",
      "\tLoss: 0.1000351533293724\n",
      "\tLoss: 0.16071531176567078\n",
      "\tLoss: 0.17232996225357056\n",
      "\tLoss: 0.11909123510122299\n",
      "\tLoss: 0.11456577479839325\n",
      "\tLoss: 0.09331619739532471\n",
      "\tLoss: 0.12022223323583603\n",
      "\tLoss: 0.0850241482257843\n",
      "\tLoss: 0.12867134809494019\n",
      "\tLoss: 0.08595166355371475\n",
      "\tLoss: 0.08361893892288208\n",
      "\tLoss: 0.07493600249290466\n",
      "\tLoss: 0.09153987467288971\n",
      "\tLoss: 0.12469163537025452\n",
      "\tLoss: 0.14912402629852295\n",
      "\tLoss: 0.12815706431865692\n",
      "\tLoss: 0.13502931594848633\n",
      "\tLoss: 0.1322290003299713\n",
      "\tLoss: 0.08011080324649811\n",
      "\tLoss: 0.12246543169021606\n",
      "\tLoss: 0.12040366232395172\n",
      "\tLoss: 0.15137851238250732\n",
      "\tLoss: 0.12303382903337479\n",
      "\tLoss: 0.0815407931804657\n",
      "\tLoss: 0.0620618611574173\n",
      "\tLoss: 0.11071117222309113\n",
      "\tLoss: 0.09730099141597748\n",
      "\tLoss: 0.15860477089881897\n",
      "\tLoss: 0.12594887614250183\n",
      "\tLoss: 0.14740832149982452\n",
      "\tLoss: 0.10403671860694885\n",
      "\tLoss: 0.05315645784139633\n",
      "\tLoss: 0.07179144769906998\n",
      "\tLoss: 0.13333463668823242\n",
      "\tLoss: 0.12326601147651672\n",
      "\tLoss: 0.07291671633720398\n",
      "\tLoss: 0.16302341222763062\n",
      "\tLoss: 0.09077970683574677\n",
      "\tLoss: 0.08899444341659546\n",
      "\tLoss: 0.12653402984142303\n",
      "\tLoss: 0.12180395424365997\n",
      "\tLoss: 0.0976111963391304\n",
      "\tLoss: 0.12057369947433472\n",
      "\tLoss: 0.11176667362451553\n",
      "\tLoss: 0.09763512760400772\n",
      "\tLoss: 0.1696396768093109\n",
      "\tLoss: 0.14080330729484558\n",
      "\tLoss: 0.10433249175548553\n",
      "\tLoss: 0.17373257875442505\n",
      "\tLoss: 0.13318952918052673\n",
      "\tLoss: 0.18686962127685547\n",
      "\tLoss: 0.12374243140220642\n",
      "\tLoss: 0.08792747557163239\n",
      "\tLoss: 0.13434042036533356\n",
      "\tLoss: 0.093865767121315\n",
      "\tLoss: 0.14101722836494446\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.09232904762029648\n",
      "\tLoss: 0.13028809428215027\n",
      "\tLoss: 0.15678656101226807\n",
      "\tLoss: 0.12018938362598419\n",
      "\tLoss: 0.14378540217876434\n",
      "\tLoss: 0.14069226384162903\n",
      "\tLoss: 0.07077788561582565\n",
      "\tLoss: 0.11288133263587952\n",
      "\tLoss: 0.12000525742769241\n",
      "\tLoss: 0.11883960664272308\n",
      "\tLoss: 0.08328654617071152\n",
      "\tLoss: 0.12723493576049805\n",
      "\tLoss: 0.17696046829223633\n",
      "\tLoss: 0.1328376829624176\n",
      "\tLoss: 0.1569446325302124\n",
      "\tLoss: 0.11332754045724869\n",
      "\tLoss: 0.10045875608921051\n",
      "\tLoss: 0.1195216029882431\n",
      "\tLoss: 0.10671471804380417\n",
      "\tLoss: 0.172183096408844\n",
      "\tLoss: 0.160578191280365\n",
      "\tLoss: 0.13728633522987366\n",
      "\tLoss: 0.11304172873497009\n",
      "\tLoss: 0.073829784989357\n",
      "\tLoss: 0.07778698205947876\n",
      "\tLoss: 0.11403465270996094\n",
      "\tLoss: 0.07491114735603333\n",
      "\tLoss: 0.15045292675495148\n",
      "\tLoss: 0.12241287529468536\n",
      "\tLoss: 0.11902276426553726\n",
      "\tLoss: 0.08940714597702026\n",
      "\tLoss: 0.10095703601837158\n",
      "\tLoss: 0.1469222903251648\n",
      "\tLoss: 0.09462778270244598\n",
      "\tLoss: 0.1498744785785675\n",
      "\tLoss: 0.10334460437297821\n",
      "\tLoss: 0.15967553853988647\n",
      "\tLoss: 0.15782810747623444\n",
      "\tLoss: 0.11007718741893768\n",
      "\tLoss: 0.11600001156330109\n",
      "\tLoss: 0.0988575667142868\n",
      "\tLoss: 0.19144970178604126\n",
      "\tLoss: 0.107826828956604\n",
      "\tLoss: 0.11088359355926514\n",
      "\tLoss: 0.10221931338310242\n",
      "\tLoss: 0.11007112264633179\n",
      "\tLoss: 0.10470467805862427\n",
      "\tLoss: 0.17469091713428497\n",
      "\tLoss: 0.09943826496601105\n",
      "\tLoss: 0.12651985883712769\n",
      "\tLoss: 0.12193064391613007\n",
      "\tLoss: 0.07840291410684586\n",
      "\tLoss: 0.15924489498138428\n",
      "\tLoss: 0.14819782972335815\n",
      "\tLoss: 0.14051848649978638\n",
      "\tLoss: 0.10978758335113525\n",
      "\tLoss: 0.1130860298871994\n",
      "\tLoss: 0.14465084671974182\n",
      "\tLoss: 0.13133075833320618\n",
      "\tLoss: 0.08931797742843628\n",
      "\tLoss: 0.12983021140098572\n",
      "\tLoss: 0.11333940923213959\n",
      "\tLoss: 0.08984830975532532\n",
      "\tLoss: 0.11422760784626007\n",
      "\tLoss: 0.10228811204433441\n",
      "\tLoss: 0.15268999338150024\n",
      "\tLoss: 0.09547337889671326\n",
      "\tLoss: 0.1230144277215004\n",
      "\tLoss: 0.13588126003742218\n",
      "\tLoss: 0.113894522190094\n",
      "\tLoss: 0.10455265641212463\n",
      "\tLoss: 0.07321180403232574\n",
      "\tLoss: 0.08954115957021713\n",
      "\tLoss: 0.14203178882598877\n",
      "\tLoss: 0.055081743746995926\n",
      "\tLoss: 0.08291037380695343\n",
      "\tLoss: 0.1271069347858429\n",
      "\tLoss: 0.10780023783445358\n",
      "\tLoss: 0.13859823346138\n",
      "\tLoss: 0.17134739458560944\n",
      "\tLoss: 0.09545828402042389\n",
      "\tLoss: 0.11310884356498718\n",
      "\tLoss: 0.13634492456912994\n",
      "\tLoss: 0.15513457357883453\n",
      "\tLoss: 0.15685537457466125\n",
      "\tLoss: 0.2010713517665863\n",
      "\tLoss: 0.15418663620948792\n",
      "\tLoss: 0.15472587943077087\n",
      "\tLoss: 0.21599073708057404\n",
      "\tLoss: 0.11256525665521622\n",
      "\tLoss: 0.1738215684890747\n",
      "\tLoss: 0.13636337220668793\n",
      "\tLoss: 0.16560515761375427\n",
      "\tLoss: 0.09578574448823929\n",
      "\tLoss: 0.14289763569831848\n",
      "\tLoss: 0.07564623653888702\n",
      "\tLoss: 0.16120770573616028\n",
      "\tLoss: 0.09188129752874374\n",
      "\tLoss: 0.17012463510036469\n",
      "\tLoss: 0.09140364080667496\n",
      "\tLoss: 0.10935654491186142\n",
      "\tLoss: 0.09555474668741226\n",
      "[time] Epoch 24: 428.1856181672774s = 7.136426969454623m\n",
      "\n",
      "Epoch 25...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.08073113858699799\n",
      "\tLoss: 0.15620291233062744\n",
      "\tLoss: 0.08046084642410278\n",
      "\tLoss: 0.1011023074388504\n",
      "\tLoss: 0.11093810200691223\n",
      "\tLoss: 0.1538744568824768\n",
      "\tLoss: 0.1285715103149414\n",
      "\tLoss: 0.1270856112241745\n",
      "\tLoss: 0.12589842081069946\n",
      "\tLoss: 0.17295341193675995\n",
      "\tLoss: 0.14255391061306\n",
      "\tLoss: 0.09422712028026581\n",
      "\tLoss: 0.10512866824865341\n",
      "\tLoss: 0.07062578201293945\n",
      "\tLoss: 0.10240942984819412\n",
      "\tLoss: 0.15269102156162262\n",
      "\tLoss: 0.12134765088558197\n",
      "\tLoss: 0.11096622794866562\n",
      "\tLoss: 0.11127112805843353\n",
      "\tLoss: 0.13155758380889893\n",
      "\tLoss: 0.11336822807788849\n",
      "\tLoss: 0.1338534951210022\n",
      "\tLoss: 0.13269473612308502\n",
      "\tLoss: 0.10677123069763184\n",
      "\tLoss: 0.09829024970531464\n",
      "\tLoss: 0.1089560016989708\n",
      "\tLoss: 0.12747983634471893\n",
      "\tLoss: 0.18481463193893433\n",
      "\tLoss: 0.09484872967004776\n",
      "\tLoss: 0.07194159924983978\n",
      "\tLoss: 0.09845026582479477\n",
      "\tLoss: 0.17810118198394775\n",
      "\tLoss: 0.11470533907413483\n",
      "\tLoss: 0.10713856667280197\n",
      "\tLoss: 0.12673573195934296\n",
      "\tLoss: 0.16070756316184998\n",
      "\tLoss: 0.10743770003318787\n",
      "\tLoss: 0.07468406856060028\n",
      "\tLoss: 0.12043716013431549\n",
      "\tLoss: 0.1306132972240448\n",
      "\tLoss: 0.10552963614463806\n",
      "\tLoss: 0.15473894774913788\n",
      "\tLoss: 0.1082669198513031\n",
      "\tLoss: 0.1428520679473877\n",
      "\tLoss: 0.1499553918838501\n",
      "\tLoss: 0.0869549885392189\n",
      "\tLoss: 0.13393352925777435\n",
      "\tLoss: 0.11014463007450104\n",
      "\tLoss: 0.1962440013885498\n",
      "\tLoss: 0.07701140642166138\n",
      "\tLoss: 0.08187255263328552\n",
      "\tLoss: 0.1170714795589447\n",
      "\tLoss: 0.1777060627937317\n",
      "\tLoss: 0.10169480741024017\n",
      "\tLoss: 0.10318960249423981\n",
      "\tLoss: 0.13001848757266998\n",
      "\tLoss: 0.09966928511857986\n",
      "\tLoss: 0.06648360192775726\n",
      "\tLoss: 0.18521347641944885\n",
      "\tLoss: 0.11171742528676987\n",
      "\tLoss: 0.07915964722633362\n",
      "\tLoss: 0.1255986988544464\n",
      "\tLoss: 0.1113242506980896\n",
      "\tLoss: 0.13678213953971863\n",
      "\tLoss: 0.11602607369422913\n",
      "\tLoss: 0.09363744407892227\n",
      "\tLoss: 0.16151444613933563\n",
      "\tLoss: 0.06736717373132706\n",
      "\tLoss: 0.09912289679050446\n",
      "\tLoss: 0.09965014457702637\n",
      "\tLoss: 0.11515458673238754\n",
      "\tLoss: 0.10756009072065353\n",
      "\tLoss: 0.09908295422792435\n",
      "\tLoss: 0.19967111945152283\n",
      "\tLoss: 0.14023417234420776\n",
      "\tLoss: 0.11443153023719788\n",
      "\tLoss: 0.15735331177711487\n",
      "\tLoss: 0.08739708364009857\n",
      "\tLoss: 0.15464681386947632\n",
      "\tLoss: 0.06368166953325272\n",
      "\tLoss: 0.17023511230945587\n",
      "\tLoss: 0.09171541035175323\n",
      "\tLoss: 0.18234442174434662\n",
      "\tLoss: 0.14831241965293884\n",
      "\tLoss: 0.08486013114452362\n",
      "\tLoss: 0.1149396151304245\n",
      "\tLoss: 0.13475821912288666\n",
      "\tLoss: 0.16402854025363922\n",
      "\tLoss: 0.08657011389732361\n",
      "\tLoss: 0.13996198773384094\n",
      "\tLoss: 0.20402401685714722\n",
      "\tLoss: 0.0479707308113575\n",
      "\tLoss: 0.12064199894666672\n",
      "\tLoss: 0.11661557853221893\n",
      "\tLoss: 0.1407497525215149\n",
      "\tLoss: 0.11252292990684509\n",
      "\tLoss: 0.09810443222522736\n",
      "\tLoss: 0.11242997646331787\n",
      "\tLoss: 0.1253555566072464\n",
      "\tLoss: 0.07804301381111145\n",
      "\tLoss: 0.12280938029289246\n",
      "\tLoss: 0.16096800565719604\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.11022067815065384\n",
      "\tLoss: 0.09885784983634949\n",
      "\tLoss: 0.13598006963729858\n",
      "\tLoss: 0.1096574068069458\n",
      "\tLoss: 0.11345333606004715\n",
      "\tLoss: 0.1468537151813507\n",
      "\tLoss: 0.14564192295074463\n",
      "\tLoss: 0.14554017782211304\n",
      "\tLoss: 0.12972627580165863\n",
      "\tLoss: 0.0963284820318222\n",
      "\tLoss: 0.14883294701576233\n",
      "\tLoss: 0.10003581643104553\n",
      "\tLoss: 0.12777476012706757\n",
      "\tLoss: 0.12379218637943268\n",
      "\tLoss: 0.10565058141946793\n",
      "\tLoss: 0.11284927278757095\n",
      "\tLoss: 0.0731973648071289\n",
      "\tLoss: 0.06872411072254181\n",
      "\tLoss: 0.0732421949505806\n",
      "\tLoss: 0.17084968090057373\n",
      "\tLoss: 0.1214570552110672\n",
      "\tLoss: 0.12739695608615875\n",
      "\tLoss: 0.1427241563796997\n",
      "\tLoss: 0.12140575051307678\n",
      "\tLoss: 0.11067678034305573\n",
      "\tLoss: 0.15546804666519165\n",
      "\tLoss: 0.17587895691394806\n",
      "\tLoss: 0.12787574529647827\n",
      "\tLoss: 0.09769406914710999\n",
      "\tLoss: 0.06656955182552338\n",
      "\tLoss: 0.09778106212615967\n",
      "\tLoss: 0.13724437355995178\n",
      "\tLoss: 0.1237439438700676\n",
      "\tLoss: 0.06371940672397614\n",
      "\tLoss: 0.07930050045251846\n",
      "\tLoss: 0.11215066909790039\n",
      "\tLoss: 0.1658092737197876\n",
      "\tLoss: 0.10493169724941254\n",
      "\tLoss: 0.05827433243393898\n",
      "\tLoss: 0.10330329835414886\n",
      "\tLoss: 0.10440514981746674\n",
      "\tLoss: 0.15652544796466827\n",
      "\tLoss: 0.08372931182384491\n",
      "\tLoss: 0.13444110751152039\n",
      "\tLoss: 0.09923207014799118\n",
      "\tLoss: 0.12498808652162552\n",
      "\tLoss: 0.1297742873430252\n",
      "\tLoss: 0.15389877557754517\n",
      "\tLoss: 0.1024271547794342\n",
      "\tLoss: 0.13727280497550964\n",
      "\tLoss: 0.1496899425983429\n",
      "\tLoss: 0.12256923317909241\n",
      "\tLoss: 0.10360062122344971\n",
      "\tLoss: 0.11576934158802032\n",
      "\tLoss: 0.15906648337841034\n",
      "\tLoss: 0.10474076867103577\n",
      "\tLoss: 0.12288320809602737\n",
      "\tLoss: 0.10114294290542603\n",
      "\tLoss: 0.0896197259426117\n",
      "\tLoss: 0.07839865237474442\n",
      "\tLoss: 0.08671873807907104\n",
      "\tLoss: 0.13493970036506653\n",
      "\tLoss: 0.08896481990814209\n",
      "\tLoss: 0.06149965152144432\n",
      "\tLoss: 0.15445801615715027\n",
      "\tLoss: 0.1615360826253891\n",
      "\tLoss: 0.15454328060150146\n",
      "\tLoss: 0.1147417426109314\n",
      "\tLoss: 0.12217355519533157\n",
      "\tLoss: 0.07975335419178009\n",
      "\tLoss: 0.09466105699539185\n",
      "\tLoss: 0.18117737770080566\n",
      "\tLoss: 0.12648501992225647\n",
      "\tLoss: 0.132941335439682\n",
      "\tLoss: 0.1079857274889946\n",
      "\tLoss: 0.09513244032859802\n",
      "\tLoss: 0.10865727812051773\n",
      "\tLoss: 0.14643928408622742\n",
      "\tLoss: 0.12373500317335129\n",
      "\tLoss: 0.1180308610200882\n",
      "\tLoss: 0.0931544303894043\n",
      "\tLoss: 0.1726946234703064\n",
      "\tLoss: 0.15243589878082275\n",
      "\tLoss: 0.13544411957263947\n",
      "\tLoss: 0.17505906522274017\n",
      "\tLoss: 0.1066858172416687\n",
      "\tLoss: 0.0801708996295929\n",
      "\tLoss: 0.0978098064661026\n",
      "\tLoss: 0.06931065022945404\n",
      "\tLoss: 0.181031733751297\n",
      "\tLoss: 0.1436263918876648\n",
      "\tLoss: 0.10678718239068985\n",
      "\tLoss: 0.16372781991958618\n",
      "\tLoss: 0.12752364575862885\n",
      "\tLoss: 0.14473441243171692\n",
      "\tLoss: 0.09887737035751343\n",
      "\tLoss: 0.08470793813467026\n",
      "\tLoss: 0.1140364408493042\n",
      "\tLoss: 0.07282920926809311\n",
      "\tLoss: 0.0684412345290184\n",
      "\tLoss: 0.06271550059318542\n",
      "\tLoss: 0.1491159200668335\n",
      "[time] Epoch 25: 432.58514874987304s = 7.209752479164551m\n",
      "\n",
      "Epoch 26...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.13703593611717224\n",
      "\tLoss: 0.05763738602399826\n",
      "\tLoss: 0.13837240636348724\n",
      "\tLoss: 0.1821555495262146\n",
      "\tLoss: 0.1259901523590088\n",
      "\tLoss: 0.1505413055419922\n",
      "\tLoss: 0.13198041915893555\n",
      "\tLoss: 0.134230837225914\n",
      "\tLoss: 0.07975293695926666\n",
      "\tLoss: 0.172476664185524\n",
      "\tLoss: 0.09106124937534332\n",
      "\tLoss: 0.10162345319986343\n",
      "\tLoss: 0.16254417598247528\n",
      "\tLoss: 0.06517665088176727\n",
      "\tLoss: 0.1471409797668457\n",
      "\tLoss: 0.13548459112644196\n",
      "\tLoss: 0.13776257634162903\n",
      "\tLoss: 0.040828436613082886\n",
      "\tLoss: 0.10589233785867691\n",
      "\tLoss: 0.1029755026102066\n",
      "\tLoss: 0.09391549229621887\n",
      "\tLoss: 0.06567059457302094\n",
      "\tLoss: 0.16226840019226074\n",
      "\tLoss: 0.11086535453796387\n",
      "\tLoss: 0.10289421677589417\n",
      "\tLoss: 0.14255283772945404\n",
      "\tLoss: 0.14205482602119446\n",
      "\tLoss: 0.09083212912082672\n",
      "\tLoss: 0.1158185601234436\n",
      "\tLoss: 0.09296424686908722\n",
      "\tLoss: 0.10104899108409882\n",
      "\tLoss: 0.21550077199935913\n",
      "\tLoss: 0.1427079141139984\n",
      "\tLoss: 0.15845924615859985\n",
      "\tLoss: 0.15894204378128052\n",
      "\tLoss: 0.15305593609809875\n",
      "\tLoss: 0.10570940375328064\n",
      "\tLoss: 0.154392272233963\n",
      "\tLoss: 0.15496781468391418\n",
      "\tLoss: 0.0945214331150055\n",
      "\tLoss: 0.095365509390831\n",
      "\tLoss: 0.07032112032175064\n",
      "\tLoss: 0.11152089387178421\n",
      "\tLoss: 0.1299143135547638\n",
      "\tLoss: 0.10752968490123749\n",
      "\tLoss: 0.10900069773197174\n",
      "\tLoss: 0.12901920080184937\n",
      "\tLoss: 0.14243757724761963\n",
      "\tLoss: 0.08138816058635712\n",
      "\tLoss: 0.18070173263549805\n",
      "\tLoss: 0.059223514050245285\n",
      "\tLoss: 0.1391398310661316\n",
      "\tLoss: 0.12895157933235168\n",
      "\tLoss: 0.10976549237966537\n",
      "\tLoss: 0.10247812420129776\n",
      "\tLoss: 0.08644062280654907\n",
      "\tLoss: 0.11619364470243454\n",
      "\tLoss: 0.09745868295431137\n",
      "\tLoss: 0.10244491696357727\n",
      "\tLoss: 0.11593589931726456\n",
      "\tLoss: 0.11470550298690796\n",
      "\tLoss: 0.13181330263614655\n",
      "\tLoss: 0.13703370094299316\n",
      "\tLoss: 0.14512550830841064\n",
      "\tLoss: 0.12320904433727264\n",
      "\tLoss: 0.22021743655204773\n",
      "\tLoss: 0.18655788898468018\n",
      "\tLoss: 0.1723259687423706\n",
      "\tLoss: 0.1308598816394806\n",
      "\tLoss: 0.10304602980613708\n",
      "\tLoss: 0.14460691809654236\n",
      "\tLoss: 0.16273127496242523\n",
      "\tLoss: 0.10830824077129364\n",
      "\tLoss: 0.099990114569664\n",
      "\tLoss: 0.09852387011051178\n",
      "\tLoss: 0.14331015944480896\n",
      "\tLoss: 0.12484920024871826\n",
      "\tLoss: 0.13607372343540192\n",
      "\tLoss: 0.1389419287443161\n",
      "\tLoss: 0.12056982517242432\n",
      "\tLoss: 0.1370377540588379\n",
      "\tLoss: 0.16215765476226807\n",
      "\tLoss: 0.1445886492729187\n",
      "\tLoss: 0.12653180956840515\n",
      "\tLoss: 0.10029183328151703\n",
      "\tLoss: 0.11658971756696701\n",
      "\tLoss: 0.19202621281147003\n",
      "\tLoss: 0.08312129229307175\n",
      "\tLoss: 0.1230773851275444\n",
      "\tLoss: 0.13238462805747986\n",
      "\tLoss: 0.12022680789232254\n",
      "\tLoss: 0.07204168289899826\n",
      "\tLoss: 0.09383626282215118\n",
      "\tLoss: 0.18614459037780762\n",
      "\tLoss: 0.12007252126932144\n",
      "\tLoss: 0.13559623062610626\n",
      "\tLoss: 0.1281081587076187\n",
      "\tLoss: 0.10561972856521606\n",
      "\tLoss: 0.09327682107686996\n",
      "\tLoss: 0.06248374283313751\n",
      "\tLoss: 0.12257649004459381\n",
      "\tLoss: 0.10967269539833069\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.12198489159345627\n",
      "\tLoss: 0.13548453152179718\n",
      "\tLoss: 0.13772940635681152\n",
      "\tLoss: 0.13735255599021912\n",
      "\tLoss: 0.14272113144397736\n",
      "\tLoss: 0.10217196494340897\n",
      "\tLoss: 0.17416225373744965\n",
      "\tLoss: 0.1350114643573761\n",
      "\tLoss: 0.11541463434696198\n",
      "\tLoss: 0.09751372039318085\n",
      "\tLoss: 0.17090290784835815\n",
      "\tLoss: 0.08184836804866791\n",
      "\tLoss: 0.08576641976833344\n",
      "\tLoss: 0.16529753804206848\n",
      "\tLoss: 0.07888822257518768\n",
      "\tLoss: 0.10266776382923126\n",
      "\tLoss: 0.07506124675273895\n",
      "\tLoss: 0.0821041613817215\n",
      "\tLoss: 0.05895691365003586\n",
      "\tLoss: 0.11029230803251266\n",
      "\tLoss: 0.11122962832450867\n",
      "\tLoss: 0.08437462151050568\n",
      "\tLoss: 0.15435519814491272\n",
      "\tLoss: 0.1016036793589592\n",
      "\tLoss: 0.1365979164838791\n",
      "\tLoss: 0.12248310446739197\n",
      "\tLoss: 0.19079875946044922\n",
      "\tLoss: 0.12768332660198212\n",
      "\tLoss: 0.18041348457336426\n",
      "\tLoss: 0.14863795042037964\n",
      "\tLoss: 0.13802194595336914\n",
      "\tLoss: 0.12204074114561081\n",
      "\tLoss: 0.09299078583717346\n",
      "\tLoss: 0.13700342178344727\n",
      "\tLoss: 0.10606318712234497\n",
      "\tLoss: 0.08891045302152634\n",
      "\tLoss: 0.15098845958709717\n",
      "\tLoss: 0.12181654572486877\n",
      "\tLoss: 0.16694828867912292\n",
      "\tLoss: 0.0844619944691658\n",
      "\tLoss: 0.16744926571846008\n",
      "\tLoss: 0.08427135646343231\n",
      "\tLoss: 0.11477819830179214\n",
      "\tLoss: 0.10223016142845154\n",
      "\tLoss: 0.09787669777870178\n",
      "\tLoss: 0.12510353326797485\n",
      "\tLoss: 0.11195956915616989\n",
      "\tLoss: 0.18391786515712738\n",
      "\tLoss: 0.11994969844818115\n",
      "\tLoss: 0.12497258186340332\n",
      "\tLoss: 0.12817715108394623\n",
      "\tLoss: 0.1105668768286705\n",
      "\tLoss: 0.1296059489250183\n",
      "\tLoss: 0.169437438249588\n",
      "\tLoss: 0.1213727742433548\n",
      "\tLoss: 0.12389451265335083\n",
      "\tLoss: 0.17952942848205566\n",
      "\tLoss: 0.1267137974500656\n",
      "\tLoss: 0.10369178652763367\n",
      "\tLoss: 0.10918314754962921\n",
      "\tLoss: 0.20550689101219177\n",
      "\tLoss: 0.10228873789310455\n",
      "\tLoss: 0.13658146560192108\n",
      "\tLoss: 0.10813061892986298\n",
      "\tLoss: 0.09486066550016403\n",
      "\tLoss: 0.09630302339792252\n",
      "\tLoss: 0.13023389875888824\n",
      "\tLoss: 0.09674450755119324\n",
      "\tLoss: 0.14514632523059845\n",
      "\tLoss: 0.08346673846244812\n",
      "\tLoss: 0.10195840895175934\n",
      "\tLoss: 0.10230833292007446\n",
      "\tLoss: 0.0696968138217926\n",
      "\tLoss: 0.09342868626117706\n",
      "\tLoss: 0.09777805209159851\n",
      "\tLoss: 0.08306825160980225\n",
      "\tLoss: 0.12382560968399048\n",
      "\tLoss: 0.11346650123596191\n",
      "\tLoss: 0.1291779726743698\n",
      "\tLoss: 0.16803577542304993\n",
      "\tLoss: 0.151927188038826\n",
      "\tLoss: 0.1421847939491272\n",
      "\tLoss: 0.17804522812366486\n",
      "\tLoss: 0.09060586243867874\n",
      "\tLoss: 0.10540789365768433\n",
      "\tLoss: 0.12424477189779282\n",
      "\tLoss: 0.11056138575077057\n",
      "\tLoss: 0.10492949932813644\n",
      "\tLoss: 0.16652263700962067\n",
      "\tLoss: 0.13403737545013428\n",
      "\tLoss: 0.1305495947599411\n",
      "\tLoss: 0.11768504977226257\n",
      "\tLoss: 0.1522112786769867\n",
      "\tLoss: 0.09126873314380646\n",
      "\tLoss: 0.12672024965286255\n",
      "\tLoss: 0.07448069006204605\n",
      "\tLoss: 0.13106276094913483\n",
      "\tLoss: 0.10560210794210434\n",
      "\tLoss: 0.09185972809791565\n",
      "\tLoss: 0.12350860238075256\n",
      "\tLoss: 0.10508838295936584\n",
      "\tLoss: 0.168783038854599\n",
      "[time] Epoch 26: 424.3958346871659s = 7.073263911452766m\n",
      "\n",
      "Epoch 27...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.0983063355088234\n",
      "\tLoss: 0.09470615535974503\n",
      "\tLoss: 0.0966004878282547\n",
      "\tLoss: 0.10639187693595886\n",
      "\tLoss: 0.13634978234767914\n",
      "\tLoss: 0.09062838554382324\n",
      "\tLoss: 0.10670025646686554\n",
      "\tLoss: 0.09076212346553802\n",
      "\tLoss: 0.12347319722175598\n",
      "\tLoss: 0.10311947017908096\n",
      "\tLoss: 0.06313595175743103\n",
      "\tLoss: 0.15205085277557373\n",
      "\tLoss: 0.0951712355017662\n",
      "\tLoss: 0.13841986656188965\n",
      "\tLoss: 0.08625916391611099\n",
      "\tLoss: 0.11236000806093216\n",
      "\tLoss: 0.13488703966140747\n",
      "\tLoss: 0.11257380992174149\n",
      "\tLoss: 0.14845331013202667\n",
      "\tLoss: 0.09139528125524521\n",
      "\tLoss: 0.15180720388889313\n",
      "\tLoss: 0.12601357698440552\n",
      "\tLoss: 0.11372564733028412\n",
      "\tLoss: 0.14070262014865875\n",
      "\tLoss: 0.16590410470962524\n",
      "\tLoss: 0.12836314737796783\n",
      "\tLoss: 0.10351147502660751\n",
      "\tLoss: 0.1255340576171875\n",
      "\tLoss: 0.08190670609474182\n",
      "\tLoss: 0.13743656873703003\n",
      "\tLoss: 0.0905691534280777\n",
      "\tLoss: 0.12311219424009323\n",
      "\tLoss: 0.07956190407276154\n",
      "\tLoss: 0.1006002277135849\n",
      "\tLoss: 0.08353297412395477\n",
      "\tLoss: 0.13353592157363892\n",
      "\tLoss: 0.08979174494743347\n",
      "\tLoss: 0.17105823755264282\n",
      "\tLoss: 0.10362605005502701\n",
      "\tLoss: 0.14311854541301727\n",
      "\tLoss: 0.10265739262104034\n",
      "\tLoss: 0.10113590210676193\n",
      "\tLoss: 0.0765024870634079\n",
      "\tLoss: 0.12986914813518524\n",
      "\tLoss: 0.13605907559394836\n",
      "\tLoss: 0.11624754965305328\n",
      "\tLoss: 0.16350559890270233\n",
      "\tLoss: 0.1312883198261261\n",
      "\tLoss: 0.08943627029657364\n",
      "\tLoss: 0.1045345813035965\n",
      "\tLoss: 0.1285005807876587\n",
      "\tLoss: 0.19849294424057007\n",
      "\tLoss: 0.1559247076511383\n",
      "\tLoss: 0.1303122639656067\n",
      "\tLoss: 0.16101092100143433\n",
      "\tLoss: 0.131862074136734\n",
      "\tLoss: 0.10129329562187195\n",
      "\tLoss: 0.11118806898593903\n",
      "\tLoss: 0.11696584522724152\n",
      "\tLoss: 0.10076873004436493\n",
      "\tLoss: 0.1433483064174652\n",
      "\tLoss: 0.12045761942863464\n",
      "\tLoss: 0.16118234395980835\n",
      "\tLoss: 0.139039546251297\n",
      "\tLoss: 0.09252694249153137\n",
      "\tLoss: 0.09013068675994873\n",
      "\tLoss: 0.10098737478256226\n",
      "\tLoss: 0.0917166918516159\n",
      "\tLoss: 0.14246484637260437\n",
      "\tLoss: 0.18570098280906677\n",
      "\tLoss: 0.09280998259782791\n",
      "\tLoss: 0.10915003716945648\n",
      "\tLoss: 0.06876552104949951\n",
      "\tLoss: 0.17898011207580566\n",
      "\tLoss: 0.12268754094839096\n",
      "\tLoss: 0.08681127429008484\n",
      "\tLoss: 0.09942939877510071\n",
      "\tLoss: 0.22412772476673126\n",
      "\tLoss: 0.2029460072517395\n",
      "\tLoss: 0.09930447489023209\n",
      "\tLoss: 0.18415795266628265\n",
      "\tLoss: 0.11217053234577179\n",
      "\tLoss: 0.11787357181310654\n",
      "\tLoss: 0.11909964680671692\n",
      "\tLoss: 0.13013598322868347\n",
      "\tLoss: 0.1753171980381012\n",
      "\tLoss: 0.1403791904449463\n",
      "\tLoss: 0.18306949734687805\n",
      "\tLoss: 0.09420187026262283\n",
      "\tLoss: 0.09605073183774948\n",
      "\tLoss: 0.0644824206829071\n",
      "\tLoss: 0.07783010601997375\n",
      "\tLoss: 0.10894647240638733\n",
      "\tLoss: 0.10255943238735199\n",
      "\tLoss: 0.19088616967201233\n",
      "\tLoss: 0.1334649622440338\n",
      "\tLoss: 0.09428688138723373\n",
      "\tLoss: 0.09925971925258636\n",
      "\tLoss: 0.1652226448059082\n",
      "\tLoss: 0.13814537227153778\n",
      "\tLoss: 0.09790249913930893\n",
      "\tLoss: 0.12497596442699432\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.08637429773807526\n",
      "\tLoss: 0.10426293313503265\n",
      "\tLoss: 0.09844934940338135\n",
      "\tLoss: 0.11206396669149399\n",
      "\tLoss: 0.16259872913360596\n",
      "\tLoss: 0.1349840611219406\n",
      "\tLoss: 0.12741656601428986\n",
      "\tLoss: 0.14824536442756653\n",
      "\tLoss: 0.12001148611307144\n",
      "\tLoss: 0.133850559592247\n",
      "\tLoss: 0.1362987756729126\n",
      "\tLoss: 0.11060801148414612\n",
      "\tLoss: 0.08539019525051117\n",
      "\tLoss: 0.07123294472694397\n",
      "\tLoss: 0.12813879549503326\n",
      "\tLoss: 0.06990975886583328\n",
      "\tLoss: 0.11003527045249939\n",
      "\tLoss: 0.11122576892375946\n",
      "\tLoss: 0.1272428035736084\n",
      "\tLoss: 0.1469474881887436\n",
      "\tLoss: 0.18627485632896423\n",
      "\tLoss: 0.12594595551490784\n",
      "\tLoss: 0.1145801767706871\n",
      "\tLoss: 0.11039932817220688\n",
      "\tLoss: 0.10300252586603165\n",
      "\tLoss: 0.11093731969594955\n",
      "\tLoss: 0.10107123851776123\n",
      "\tLoss: 0.18274623155593872\n",
      "\tLoss: 0.105955570936203\n",
      "\tLoss: 0.11104243993759155\n",
      "\tLoss: 0.15262076258659363\n",
      "\tLoss: 0.154901385307312\n",
      "\tLoss: 0.10246190428733826\n",
      "\tLoss: 0.12421773374080658\n",
      "\tLoss: 0.10087408870458603\n",
      "\tLoss: 0.11846563220024109\n",
      "\tLoss: 0.0831480324268341\n",
      "\tLoss: 0.13384883105754852\n",
      "\tLoss: 0.11402575671672821\n",
      "\tLoss: 0.10766255855560303\n",
      "\tLoss: 0.09080229699611664\n",
      "\tLoss: 0.11796747148036957\n",
      "\tLoss: 0.0894356369972229\n",
      "\tLoss: 0.12608975172042847\n",
      "\tLoss: 0.14387059211730957\n",
      "\tLoss: 0.09719404578208923\n",
      "\tLoss: 0.10443611443042755\n",
      "\tLoss: 0.1720467507839203\n",
      "\tLoss: 0.1116919219493866\n",
      "\tLoss: 0.09943220019340515\n",
      "\tLoss: 0.13312000036239624\n",
      "\tLoss: 0.11697916686534882\n",
      "\tLoss: 0.18331363797187805\n",
      "\tLoss: 0.09844587743282318\n",
      "\tLoss: 0.13752993941307068\n",
      "\tLoss: 0.14789243042469025\n",
      "\tLoss: 0.11595585942268372\n",
      "\tLoss: 0.08957768231630325\n",
      "\tLoss: 0.07525178790092468\n",
      "\tLoss: 0.10565701127052307\n",
      "\tLoss: 0.15189367532730103\n",
      "\tLoss: 0.14943264424800873\n",
      "\tLoss: 0.10901240259408951\n",
      "\tLoss: 0.08385469764471054\n",
      "\tLoss: 0.11642920970916748\n",
      "\tLoss: 0.17396225035190582\n",
      "\tLoss: 0.1483774185180664\n",
      "\tLoss: 0.1319155991077423\n",
      "\tLoss: 0.10635177046060562\n",
      "\tLoss: 0.10141356289386749\n",
      "\tLoss: 0.11570754647254944\n",
      "\tLoss: 0.1707301139831543\n",
      "\tLoss: 0.08058217167854309\n",
      "\tLoss: 0.11400479823350906\n",
      "\tLoss: 0.12341475486755371\n",
      "\tLoss: 0.05497897416353226\n",
      "\tLoss: 0.1169707179069519\n",
      "\tLoss: 0.17401593923568726\n",
      "\tLoss: 0.0948517918586731\n",
      "\tLoss: 0.12419658899307251\n",
      "\tLoss: 0.07835501432418823\n",
      "\tLoss: 0.12112703919410706\n",
      "\tLoss: 0.13366369903087616\n",
      "\tLoss: 0.1325768530368805\n",
      "\tLoss: 0.08113755285739899\n",
      "\tLoss: 0.09064963459968567\n",
      "\tLoss: 0.12805381417274475\n",
      "\tLoss: 0.17937293648719788\n",
      "\tLoss: 0.10100704431533813\n",
      "\tLoss: 0.1266925036907196\n",
      "\tLoss: 0.13668249547481537\n",
      "\tLoss: 0.1107301265001297\n",
      "\tLoss: 0.12704139947891235\n",
      "\tLoss: 0.09590253233909607\n",
      "\tLoss: 0.10953449457883835\n",
      "\tLoss: 0.09401334822177887\n",
      "\tLoss: 0.10326597094535828\n",
      "\tLoss: 0.14991611242294312\n",
      "\tLoss: 0.19095636904239655\n",
      "\tLoss: 0.1226639524102211\n",
      "\tLoss: 0.18894238770008087\n",
      "\tLoss: 0.11763685941696167\n",
      "[time] Epoch 27: 432.703309064731s = 7.2117218177455165m\n",
      "\n",
      "Epoch 28...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.12058031558990479\n",
      "\tLoss: 0.1234130710363388\n",
      "\tLoss: 0.12629160284996033\n",
      "\tLoss: 0.1252056509256363\n",
      "\tLoss: 0.07993059605360031\n",
      "\tLoss: 0.1278964877128601\n",
      "\tLoss: 0.10839569568634033\n",
      "\tLoss: 0.17147819697856903\n",
      "\tLoss: 0.08729919046163559\n",
      "\tLoss: 0.11430541425943375\n",
      "\tLoss: 0.12994946539402008\n",
      "\tLoss: 0.1176207885146141\n",
      "\tLoss: 0.13630324602127075\n",
      "\tLoss: 0.18557152152061462\n",
      "\tLoss: 0.12705376744270325\n",
      "\tLoss: 0.10418665409088135\n",
      "\tLoss: 0.12578189373016357\n",
      "\tLoss: 0.11272062361240387\n",
      "\tLoss: 0.0908445492386818\n",
      "\tLoss: 0.08370224386453629\n",
      "\tLoss: 0.07854419946670532\n",
      "\tLoss: 0.1577059030532837\n",
      "\tLoss: 0.11428846418857574\n",
      "\tLoss: 0.1378944367170334\n",
      "\tLoss: 0.09116403758525848\n",
      "\tLoss: 0.14569130539894104\n",
      "\tLoss: 0.11902754008769989\n",
      "\tLoss: 0.08254459500312805\n",
      "\tLoss: 0.1203535869717598\n",
      "\tLoss: 0.12286537885665894\n",
      "\tLoss: 0.09349237382411957\n",
      "\tLoss: 0.1719471514225006\n",
      "\tLoss: 0.09917326271533966\n",
      "\tLoss: 0.14355015754699707\n",
      "\tLoss: 0.11056385934352875\n",
      "\tLoss: 0.14353378117084503\n",
      "\tLoss: 0.0942239910364151\n",
      "\tLoss: 0.11512647569179535\n",
      "\tLoss: 0.17158877849578857\n",
      "\tLoss: 0.10529518127441406\n",
      "\tLoss: 0.1410001814365387\n",
      "\tLoss: 0.13051386177539825\n",
      "\tLoss: 0.08578909933567047\n",
      "\tLoss: 0.1389971673488617\n",
      "\tLoss: 0.10817792266607285\n",
      "\tLoss: 0.11305923014879227\n",
      "\tLoss: 0.1566612869501114\n",
      "\tLoss: 0.14140963554382324\n",
      "\tLoss: 0.15324699878692627\n",
      "\tLoss: 0.11304517090320587\n",
      "\tLoss: 0.11903279274702072\n",
      "\tLoss: 0.12947019934654236\n",
      "\tLoss: 0.08394519239664078\n",
      "\tLoss: 0.1275651603937149\n",
      "\tLoss: 0.14151617884635925\n",
      "\tLoss: 0.13664978742599487\n",
      "\tLoss: 0.2012842297554016\n",
      "\tLoss: 0.17974670231342316\n",
      "\tLoss: 0.12346763908863068\n",
      "\tLoss: 0.09799401462078094\n",
      "\tLoss: 0.08921831846237183\n",
      "\tLoss: 0.1392536163330078\n",
      "\tLoss: 0.13160651922225952\n",
      "\tLoss: 0.1291029453277588\n",
      "\tLoss: 0.10699701309204102\n",
      "\tLoss: 0.09842803329229355\n",
      "\tLoss: 0.1573714315891266\n",
      "\tLoss: 0.12026512622833252\n",
      "\tLoss: 0.12503181397914886\n",
      "\tLoss: 0.09471972286701202\n",
      "\tLoss: 0.15416058897972107\n",
      "\tLoss: 0.10421888530254364\n",
      "\tLoss: 0.1481347531080246\n",
      "\tLoss: 0.07656971365213394\n",
      "\tLoss: 0.167782723903656\n",
      "\tLoss: 0.0923052728176117\n",
      "\tLoss: 0.1041933074593544\n",
      "\tLoss: 0.1295810043811798\n",
      "\tLoss: 0.1544581949710846\n",
      "\tLoss: 0.11942876875400543\n",
      "\tLoss: 0.08349528163671494\n",
      "\tLoss: 0.14394301176071167\n",
      "\tLoss: 0.11945512145757675\n",
      "\tLoss: 0.14049804210662842\n",
      "\tLoss: 0.1164211630821228\n",
      "\tLoss: 0.16895373165607452\n",
      "\tLoss: 0.12377274036407471\n",
      "\tLoss: 0.13994163274765015\n",
      "\tLoss: 0.1088152676820755\n",
      "\tLoss: 0.11967343837022781\n",
      "\tLoss: 0.16145455837249756\n",
      "\tLoss: 0.16131171584129333\n",
      "\tLoss: 0.07579104602336884\n",
      "\tLoss: 0.160378098487854\n",
      "\tLoss: 0.11232000589370728\n",
      "\tLoss: 0.10040546953678131\n",
      "\tLoss: 0.13314288854599\n",
      "\tLoss: 0.1754215955734253\n",
      "\tLoss: 0.14193764328956604\n",
      "\tLoss: 0.15393348038196564\n",
      "\tLoss: 0.16192585229873657\n",
      "\tLoss: 0.1612427681684494\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.17128197848796844\n",
      "\tLoss: 0.142566978931427\n",
      "\tLoss: 0.16298174858093262\n",
      "\tLoss: 0.1002930998802185\n",
      "\tLoss: 0.1273866593837738\n",
      "\tLoss: 0.14572635293006897\n",
      "\tLoss: 0.1423833668231964\n",
      "\tLoss: 0.07118228077888489\n",
      "\tLoss: 0.11111167073249817\n",
      "\tLoss: 0.12512946128845215\n",
      "\tLoss: 0.09624619781970978\n",
      "\tLoss: 0.1358908712863922\n",
      "\tLoss: 0.15061229467391968\n",
      "\tLoss: 0.10335520654916763\n",
      "\tLoss: 0.05148821324110031\n",
      "\tLoss: 0.11690478026866913\n",
      "\tLoss: 0.08407449722290039\n",
      "\tLoss: 0.1317112296819687\n",
      "\tLoss: 0.144515261054039\n",
      "\tLoss: 0.09218671917915344\n",
      "\tLoss: 0.09237040579319\n",
      "\tLoss: 0.13843974471092224\n",
      "\tLoss: 0.11893071234226227\n",
      "\tLoss: 0.13998132944107056\n",
      "\tLoss: 0.19379480183124542\n",
      "\tLoss: 0.17207831144332886\n",
      "\tLoss: 0.10488127917051315\n",
      "\tLoss: 0.15349851548671722\n",
      "\tLoss: 0.1146072968840599\n",
      "\tLoss: 0.08826836198568344\n",
      "\tLoss: 0.1150260716676712\n",
      "\tLoss: 0.1206999346613884\n",
      "\tLoss: 0.1416248381137848\n",
      "\tLoss: 0.11551156640052795\n",
      "\tLoss: 0.09242425858974457\n",
      "\tLoss: 0.07787970453500748\n",
      "\tLoss: 0.08352307975292206\n",
      "\tLoss: 0.1360204517841339\n",
      "\tLoss: 0.11354277282953262\n",
      "\tLoss: 0.15008535981178284\n",
      "\tLoss: 0.12360601127147675\n",
      "\tLoss: 0.1291423738002777\n",
      "\tLoss: 0.11606483906507492\n",
      "\tLoss: 0.10556457936763763\n",
      "\tLoss: 0.06678467243909836\n",
      "\tLoss: 0.0660068541765213\n",
      "\tLoss: 0.12845100462436676\n",
      "\tLoss: 0.13902968168258667\n",
      "\tLoss: 0.09891689568758011\n",
      "\tLoss: 0.061865098774433136\n",
      "\tLoss: 0.12042029201984406\n",
      "\tLoss: 0.12696421146392822\n",
      "\tLoss: 0.061307020485401154\n",
      "\tLoss: 0.11371930688619614\n",
      "\tLoss: 0.11645478755235672\n",
      "\tLoss: 0.08136209100484848\n",
      "\tLoss: 0.13194739818572998\n",
      "\tLoss: 0.0882149487733841\n",
      "\tLoss: 0.1236313059926033\n",
      "\tLoss: 0.13842587172985077\n",
      "\tLoss: 0.15077248215675354\n",
      "\tLoss: 0.11575428396463394\n",
      "\tLoss: 0.1405901312828064\n",
      "\tLoss: 0.10473989695310593\n",
      "\tLoss: 0.09545981884002686\n",
      "\tLoss: 0.14015403389930725\n",
      "\tLoss: 0.12314590811729431\n",
      "\tLoss: 0.16421611607074738\n",
      "\tLoss: 0.07092650234699249\n",
      "\tLoss: 0.09695392102003098\n",
      "\tLoss: 0.12998104095458984\n",
      "\tLoss: 0.0857241153717041\n",
      "\tLoss: 0.07306645065546036\n",
      "\tLoss: 0.16136674582958221\n",
      "\tLoss: 0.1157739907503128\n",
      "\tLoss: 0.12895084917545319\n",
      "\tLoss: 0.08504415303468704\n",
      "\tLoss: 0.1602165400981903\n",
      "\tLoss: 0.17876558005809784\n",
      "\tLoss: 0.1623515486717224\n",
      "\tLoss: 0.06369537115097046\n",
      "\tLoss: 0.12688931822776794\n",
      "\tLoss: 0.1309611201286316\n",
      "\tLoss: 0.13275814056396484\n",
      "\tLoss: 0.1696130931377411\n",
      "\tLoss: 0.11016707122325897\n",
      "\tLoss: 0.09069682657718658\n",
      "\tLoss: 0.14990100264549255\n",
      "\tLoss: 0.05164637416601181\n",
      "\tLoss: 0.18084698915481567\n",
      "\tLoss: 0.08077787607908249\n",
      "\tLoss: 0.11992229521274567\n",
      "\tLoss: 0.10308429598808289\n",
      "\tLoss: 0.13490110635757446\n",
      "\tLoss: 0.13092081248760223\n",
      "\tLoss: 0.12993942201137543\n",
      "\tLoss: 0.12153942883014679\n",
      "\tLoss: 0.09524545818567276\n",
      "\tLoss: 0.11852774024009705\n",
      "\tLoss: 0.14038561284542084\n",
      "\tLoss: 0.14192262291908264\n",
      "\tLoss: 0.10491465032100677\n",
      "[time] Epoch 28: 422.29428943200037s = 7.038238157200007m\n",
      "\n",
      "Epoch 29...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.0745256245136261\n",
      "\tLoss: 0.12863226234912872\n",
      "\tLoss: 0.1039232462644577\n",
      "\tLoss: 0.11949922144412994\n",
      "\tLoss: 0.09904565662145615\n",
      "\tLoss: 0.14085513353347778\n",
      "\tLoss: 0.15067557990550995\n",
      "\tLoss: 0.15660423040390015\n",
      "\tLoss: 0.09089931100606918\n",
      "\tLoss: 0.10581712424755096\n",
      "\tLoss: 0.15693235397338867\n",
      "\tLoss: 0.13353067636489868\n",
      "\tLoss: 0.06703463196754456\n",
      "\tLoss: 0.08764132857322693\n",
      "\tLoss: 0.13661602139472961\n",
      "\tLoss: 0.07156135141849518\n",
      "\tLoss: 0.12853962182998657\n",
      "\tLoss: 0.12821483612060547\n",
      "\tLoss: 0.10591042786836624\n",
      "\tLoss: 0.16266313195228577\n",
      "\tLoss: 0.14924368262290955\n",
      "\tLoss: 0.135511115193367\n",
      "\tLoss: 0.13358275592327118\n",
      "\tLoss: 0.11116328090429306\n",
      "\tLoss: 0.09126501530408859\n",
      "\tLoss: 0.09673503786325455\n",
      "\tLoss: 0.09825390577316284\n",
      "\tLoss: 0.07867524027824402\n",
      "\tLoss: 0.13418050110340118\n",
      "\tLoss: 0.06209133565425873\n",
      "\tLoss: 0.17213723063468933\n",
      "\tLoss: 0.15715031325817108\n",
      "\tLoss: 0.09677177667617798\n",
      "\tLoss: 0.09350058436393738\n",
      "\tLoss: 0.13163283467292786\n",
      "\tLoss: 0.11107885092496872\n",
      "\tLoss: 0.16467659175395966\n",
      "\tLoss: 0.12086174637079239\n",
      "\tLoss: 0.0920221358537674\n",
      "\tLoss: 0.18553633987903595\n",
      "\tLoss: 0.06860296428203583\n",
      "\tLoss: 0.12389060854911804\n",
      "\tLoss: 0.21518780291080475\n",
      "\tLoss: 0.1357860267162323\n",
      "\tLoss: 0.0779515877366066\n",
      "\tLoss: 0.08747448027133942\n",
      "\tLoss: 0.07785466313362122\n",
      "\tLoss: 0.14413544535636902\n",
      "\tLoss: 0.09423384070396423\n",
      "\tLoss: 0.09089191257953644\n",
      "\tLoss: 0.09497974067926407\n",
      "\tLoss: 0.12708114087581635\n",
      "\tLoss: 0.13213017582893372\n",
      "\tLoss: 0.09905411303043365\n",
      "\tLoss: 0.15201929211616516\n",
      "\tLoss: 0.13129013776779175\n",
      "\tLoss: 0.10699692368507385\n",
      "\tLoss: 0.1181502640247345\n",
      "\tLoss: 0.08602768927812576\n",
      "\tLoss: 0.17444495856761932\n",
      "\tLoss: 0.12237659096717834\n",
      "\tLoss: 0.11947256326675415\n",
      "\tLoss: 0.09063561260700226\n",
      "\tLoss: 0.16170725226402283\n",
      "\tLoss: 0.17605984210968018\n",
      "\tLoss: 0.12371441721916199\n",
      "\tLoss: 0.09137721359729767\n",
      "\tLoss: 0.11489257216453552\n",
      "\tLoss: 0.12737953662872314\n",
      "\tLoss: 0.10339576750993729\n",
      "\tLoss: 0.11568822711706161\n",
      "\tLoss: 0.06178130954504013\n",
      "\tLoss: 0.12738656997680664\n",
      "\tLoss: 0.11907122284173965\n",
      "\tLoss: 0.1608770489692688\n",
      "\tLoss: 0.11457118391990662\n",
      "\tLoss: 0.16596698760986328\n",
      "\tLoss: 0.09336040914058685\n",
      "\tLoss: 0.1105109304189682\n",
      "\tLoss: 0.09769269824028015\n",
      "\tLoss: 0.12502774596214294\n",
      "\tLoss: 0.12293069064617157\n",
      "\tLoss: 0.18446241319179535\n",
      "\tLoss: 0.12619687616825104\n",
      "\tLoss: 0.09917038679122925\n",
      "\tLoss: 0.09296736866235733\n",
      "\tLoss: 0.07297168672084808\n",
      "\tLoss: 0.10154817998409271\n",
      "\tLoss: 0.12450301647186279\n",
      "\tLoss: 0.11052165180444717\n",
      "\tLoss: 0.15718722343444824\n",
      "\tLoss: 0.14024394750595093\n",
      "\tLoss: 0.05985618010163307\n",
      "\tLoss: 0.11942919343709946\n",
      "\tLoss: 0.07878540456295013\n",
      "\tLoss: 0.1086479127407074\n",
      "\tLoss: 0.11066313832998276\n",
      "\tLoss: 0.1267724186182022\n",
      "\tLoss: 0.11871128529310226\n",
      "\tLoss: 0.0777464509010315\n",
      "\tLoss: 0.1425783932209015\n",
      "\tLoss: 0.08278930932283401\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.110854372382164\n",
      "\tLoss: 0.10457716882228851\n",
      "\tLoss: 0.12712141871452332\n",
      "\tLoss: 0.12979114055633545\n",
      "\tLoss: 0.17040038108825684\n",
      "\tLoss: 0.11619776487350464\n",
      "\tLoss: 0.11252789944410324\n",
      "\tLoss: 0.07611442357301712\n",
      "\tLoss: 0.1026240885257721\n",
      "\tLoss: 0.11477510631084442\n",
      "\tLoss: 0.13454708456993103\n",
      "\tLoss: 0.12387984991073608\n",
      "\tLoss: 0.10814151912927628\n",
      "\tLoss: 0.09022076427936554\n",
      "\tLoss: 0.1261977255344391\n",
      "\tLoss: 0.12154270708560944\n",
      "\tLoss: 0.10297192633152008\n",
      "\tLoss: 0.11592423915863037\n",
      "\tLoss: 0.18695496022701263\n",
      "\tLoss: 0.09673891961574554\n",
      "\tLoss: 0.10939944535493851\n",
      "\tLoss: 0.19574350118637085\n",
      "\tLoss: 0.11925376951694489\n",
      "\tLoss: 0.09038810431957245\n",
      "\tLoss: 0.17105266451835632\n",
      "\tLoss: 0.18112316727638245\n",
      "\tLoss: 0.06798180937767029\n",
      "\tLoss: 0.11360502988100052\n",
      "\tLoss: 0.2248121052980423\n",
      "\tLoss: 0.10235796868801117\n",
      "\tLoss: 0.20163601636886597\n",
      "\tLoss: 0.09643655270338058\n",
      "\tLoss: 0.08749938756227493\n",
      "\tLoss: 0.12840746343135834\n",
      "\tLoss: 0.11933927237987518\n",
      "\tLoss: 0.1278020143508911\n",
      "\tLoss: 0.13485440611839294\n",
      "\tLoss: 0.1800849735736847\n",
      "\tLoss: 0.12235292792320251\n",
      "\tLoss: 0.1085892841219902\n",
      "\tLoss: 0.09498792141675949\n",
      "\tLoss: 0.11054883897304535\n",
      "\tLoss: 0.05505303665995598\n",
      "\tLoss: 0.09374809265136719\n",
      "\tLoss: 0.129575714468956\n",
      "\tLoss: 0.06324079632759094\n",
      "\tLoss: 0.11763464659452438\n",
      "\tLoss: 0.1305859237909317\n",
      "\tLoss: 0.09324190020561218\n",
      "\tLoss: 0.15082038938999176\n",
      "\tLoss: 0.08886177837848663\n",
      "\tLoss: 0.12224236130714417\n",
      "\tLoss: 0.10982967168092728\n",
      "\tLoss: 0.14757853746414185\n",
      "\tLoss: 0.11450155079364777\n",
      "\tLoss: 0.14380009472370148\n",
      "\tLoss: 0.13043293356895447\n",
      "\tLoss: 0.17515000700950623\n",
      "\tLoss: 0.10426336526870728\n",
      "\tLoss: 0.10654046386480331\n",
      "\tLoss: 0.1440572738647461\n",
      "\tLoss: 0.06693221628665924\n",
      "\tLoss: 0.1332496851682663\n",
      "\tLoss: 0.08756272494792938\n",
      "\tLoss: 0.13211557269096375\n",
      "\tLoss: 0.11032779514789581\n",
      "\tLoss: 0.07763461023569107\n",
      "\tLoss: 0.08947557210922241\n",
      "\tLoss: 0.10456956177949905\n",
      "\tLoss: 0.1206483244895935\n",
      "\tLoss: 0.11184073984622955\n",
      "\tLoss: 0.09653779864311218\n",
      "\tLoss: 0.10503488779067993\n",
      "\tLoss: 0.1706225425004959\n",
      "\tLoss: 0.09818494319915771\n",
      "\tLoss: 0.09265240281820297\n",
      "\tLoss: 0.1398811936378479\n",
      "\tLoss: 0.14421312510967255\n",
      "\tLoss: 0.1484755277633667\n",
      "\tLoss: 0.14263057708740234\n",
      "\tLoss: 0.11592353880405426\n",
      "\tLoss: 0.10273586958646774\n",
      "\tLoss: 0.19217926263809204\n",
      "\tLoss: 0.11261139810085297\n",
      "\tLoss: 0.16313213109970093\n",
      "\tLoss: 0.11043855547904968\n",
      "\tLoss: 0.09380148351192474\n",
      "\tLoss: 0.11975706368684769\n",
      "\tLoss: 0.12013311684131622\n",
      "\tLoss: 0.08999284356832504\n",
      "\tLoss: 0.09297250956296921\n",
      "\tLoss: 0.098321832716465\n",
      "\tLoss: 0.12643909454345703\n",
      "\tLoss: 0.16153420507907867\n",
      "\tLoss: 0.08436530828475952\n",
      "\tLoss: 0.11844196915626526\n",
      "\tLoss: 0.08864489197731018\n",
      "\tLoss: 0.12207245081663132\n",
      "\tLoss: 0.1493493765592575\n",
      "\tLoss: 0.09437371790409088\n",
      "\tLoss: 0.12611660361289978\n",
      "\tLoss: 0.13803642988204956\n",
      "[time] Epoch 29: 420.02884174510837s = 7.000480695751806m\n",
      "\n",
      "Epoch 30...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.12418723106384277\n",
      "\tLoss: 0.1691778302192688\n",
      "\tLoss: 0.17612914741039276\n",
      "\tLoss: 0.12538881599903107\n",
      "\tLoss: 0.11773224174976349\n",
      "\tLoss: 0.09720410406589508\n",
      "\tLoss: 0.12083212286233902\n",
      "\tLoss: 0.13538479804992676\n",
      "\tLoss: 0.08582859486341476\n",
      "\tLoss: 0.15922695398330688\n",
      "\tLoss: 0.16766393184661865\n",
      "\tLoss: 0.12775589525699615\n",
      "\tLoss: 0.11721327900886536\n",
      "\tLoss: 0.09591065347194672\n",
      "\tLoss: 0.10951731353998184\n",
      "\tLoss: 0.11292886734008789\n",
      "\tLoss: 0.11827918142080307\n",
      "\tLoss: 0.18064019083976746\n",
      "\tLoss: 0.1080770194530487\n",
      "\tLoss: 0.08091239631175995\n",
      "\tLoss: 0.12741003930568695\n",
      "\tLoss: 0.11821147799491882\n",
      "\tLoss: 0.1348026841878891\n",
      "\tLoss: 0.11229908466339111\n",
      "\tLoss: 0.17300134897232056\n",
      "\tLoss: 0.11751201748847961\n",
      "\tLoss: 0.0876636654138565\n",
      "\tLoss: 0.12742234766483307\n",
      "\tLoss: 0.13684722781181335\n",
      "\tLoss: 0.11684600263834\n",
      "\tLoss: 0.0793318897485733\n",
      "\tLoss: 0.14807745814323425\n",
      "\tLoss: 0.13298490643501282\n",
      "\tLoss: 0.08571895211935043\n",
      "\tLoss: 0.10594836622476578\n",
      "\tLoss: 0.10045195370912552\n",
      "\tLoss: 0.13274045288562775\n",
      "\tLoss: 0.057251010090112686\n",
      "\tLoss: 0.14404231309890747\n",
      "\tLoss: 0.11376241594552994\n",
      "\tLoss: 0.1995755434036255\n",
      "\tLoss: 0.13670408725738525\n",
      "\tLoss: 0.15015560388565063\n",
      "\tLoss: 0.08251534402370453\n",
      "\tLoss: 0.13810186088085175\n",
      "\tLoss: 0.11443200707435608\n",
      "\tLoss: 0.1808125227689743\n",
      "\tLoss: 0.14004573225975037\n",
      "\tLoss: 0.14658483862876892\n",
      "\tLoss: 0.11893395334482193\n",
      "\tLoss: 0.10078627616167068\n",
      "\tLoss: 0.12271573394536972\n",
      "\tLoss: 0.05115953087806702\n",
      "\tLoss: 0.11723819375038147\n",
      "\tLoss: 0.0888889953494072\n",
      "\tLoss: 0.14237895607948303\n",
      "\tLoss: 0.07091882079839706\n",
      "\tLoss: 0.11536721885204315\n",
      "\tLoss: 0.08715091645717621\n",
      "\tLoss: 0.14677922427654266\n",
      "\tLoss: 0.15914985537528992\n",
      "\tLoss: 0.08188938349485397\n",
      "\tLoss: 0.06717732548713684\n",
      "\tLoss: 0.12241265177726746\n",
      "\tLoss: 0.13931679725646973\n",
      "\tLoss: 0.12391240149736404\n",
      "\tLoss: 0.11230507493019104\n",
      "\tLoss: 0.12575313448905945\n",
      "\tLoss: 0.14621159434318542\n",
      "\tLoss: 0.09387214481830597\n",
      "\tLoss: 0.06576637923717499\n",
      "\tLoss: 0.08565494418144226\n",
      "\tLoss: 0.15299998223781586\n",
      "\tLoss: 0.054313503205776215\n",
      "\tLoss: 0.17088094353675842\n",
      "\tLoss: 0.1668982207775116\n",
      "\tLoss: 0.1103028729557991\n",
      "\tLoss: 0.18285754323005676\n",
      "\tLoss: 0.10677506029605865\n",
      "\tLoss: 0.10400798916816711\n",
      "\tLoss: 0.12229294329881668\n",
      "\tLoss: 0.09069131314754486\n",
      "\tLoss: 0.19042819738388062\n",
      "\tLoss: 0.14778970181941986\n",
      "\tLoss: 0.11572536081075668\n",
      "\tLoss: 0.09577362984418869\n",
      "\tLoss: 0.11295045167207718\n",
      "\tLoss: 0.14912480115890503\n",
      "\tLoss: 0.1684741973876953\n",
      "\tLoss: 0.17532292008399963\n",
      "\tLoss: 0.09457291662693024\n",
      "\tLoss: 0.07811287045478821\n",
      "\tLoss: 0.11319363862276077\n",
      "\tLoss: 0.0974731370806694\n",
      "\tLoss: 0.1361771523952484\n",
      "\tLoss: 0.21320390701293945\n",
      "\tLoss: 0.10545790195465088\n",
      "\tLoss: 0.11912733316421509\n",
      "\tLoss: 0.0710686445236206\n",
      "\tLoss: 0.12016285955905914\n",
      "\tLoss: 0.08603497594594955\n",
      "\tLoss: 0.10193938761949539\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.13114741444587708\n",
      "\tLoss: 0.10330596566200256\n",
      "\tLoss: 0.23535996675491333\n",
      "\tLoss: 0.1176350861787796\n",
      "\tLoss: 0.17774373292922974\n",
      "\tLoss: 0.06655431538820267\n",
      "\tLoss: 0.18154376745224\n",
      "\tLoss: 0.11863014847040176\n",
      "\tLoss: 0.1196354478597641\n",
      "\tLoss: 0.1376899778842926\n",
      "\tLoss: 0.08021817356348038\n",
      "\tLoss: 0.12917859852313995\n",
      "\tLoss: 0.10374894738197327\n",
      "\tLoss: 0.07819200307130814\n",
      "\tLoss: 0.15693055093288422\n",
      "\tLoss: 0.08775164186954498\n",
      "\tLoss: 0.13694941997528076\n",
      "\tLoss: 0.0818823054432869\n",
      "\tLoss: 0.10844242572784424\n",
      "\tLoss: 0.14942719042301178\n",
      "\tLoss: 0.11649169027805328\n",
      "\tLoss: 0.1092376708984375\n",
      "\tLoss: 0.16791927814483643\n",
      "\tLoss: 0.13668088614940643\n",
      "\tLoss: 0.10682880133390427\n",
      "\tLoss: 0.14932981133460999\n",
      "\tLoss: 0.0544654056429863\n",
      "\tLoss: 0.12724152207374573\n",
      "\tLoss: 0.13767403364181519\n",
      "\tLoss: 0.0869964137673378\n",
      "\tLoss: 0.11934660375118256\n",
      "\tLoss: 0.12717553973197937\n",
      "\tLoss: 0.16282790899276733\n",
      "\tLoss: 0.09590651094913483\n",
      "\tLoss: 0.1427668333053589\n",
      "\tLoss: 0.13839054107666016\n",
      "\tLoss: 0.12789233028888702\n",
      "\tLoss: 0.13831213116645813\n",
      "\tLoss: 0.14989057183265686\n",
      "\tLoss: 0.13844236731529236\n",
      "\tLoss: 0.11319990456104279\n",
      "\tLoss: 0.13223576545715332\n",
      "\tLoss: 0.13155686855316162\n",
      "\tLoss: 0.0887679010629654\n",
      "\tLoss: 0.16653533279895782\n",
      "\tLoss: 0.08714979887008667\n",
      "\tLoss: 0.1468380242586136\n",
      "\tLoss: 0.08905372023582458\n",
      "\tLoss: 0.13935334980487823\n",
      "\tLoss: 0.0928981751203537\n",
      "\tLoss: 0.08665773272514343\n",
      "\tLoss: 0.14725881814956665\n",
      "\tLoss: 0.09509100019931793\n",
      "\tLoss: 0.12552818655967712\n",
      "\tLoss: 0.09284651279449463\n",
      "\tLoss: 0.20101389288902283\n",
      "\tLoss: 0.1216612309217453\n",
      "\tLoss: 0.13781994581222534\n",
      "\tLoss: 0.11299038678407669\n",
      "\tLoss: 0.1067902147769928\n",
      "\tLoss: 0.15657170116901398\n",
      "\tLoss: 0.10246990621089935\n",
      "\tLoss: 0.12221640348434448\n",
      "\tLoss: 0.09174898266792297\n",
      "\tLoss: 0.12989744544029236\n",
      "\tLoss: 0.1302185356616974\n",
      "\tLoss: 0.10575128346681595\n",
      "\tLoss: 0.13122427463531494\n",
      "\tLoss: 0.16619950532913208\n",
      "\tLoss: 0.11523009091615677\n",
      "\tLoss: 0.11579698324203491\n",
      "\tLoss: 0.11207890510559082\n",
      "\tLoss: 0.0775318443775177\n",
      "\tLoss: 0.1323210895061493\n",
      "\tLoss: 0.1196279302239418\n",
      "\tLoss: 0.10621550679206848\n",
      "\tLoss: 0.09662552177906036\n",
      "\tLoss: 0.14319442212581635\n",
      "\tLoss: 0.09981827437877655\n",
      "\tLoss: 0.10421805083751678\n",
      "\tLoss: 0.0899931788444519\n",
      "\tLoss: 0.1252066195011139\n",
      "\tLoss: 0.1792825311422348\n",
      "\tLoss: 0.12458591163158417\n",
      "\tLoss: 0.18201786279678345\n",
      "\tLoss: 0.14296084642410278\n",
      "\tLoss: 0.14502018690109253\n",
      "\tLoss: 0.07672728598117828\n",
      "\tLoss: 0.19999991357326508\n",
      "\tLoss: 0.1305898129940033\n",
      "\tLoss: 0.17406126856803894\n",
      "\tLoss: 0.09596286714076996\n",
      "\tLoss: 0.10350064188241959\n",
      "\tLoss: 0.14036180078983307\n",
      "\tLoss: 0.10226212441921234\n",
      "\tLoss: 0.1528586447238922\n",
      "\tLoss: 0.09801502525806427\n",
      "\tLoss: 0.06878182291984558\n",
      "\tLoss: 0.18615053594112396\n",
      "\tLoss: 0.11829456686973572\n",
      "\tLoss: 0.16961438953876495\n",
      "\tLoss: 0.09717923402786255\n",
      "[time] Epoch 30: 421.7099020779133s = 7.028498367965222m\n",
      "\n",
      "Epoch 31...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.164756178855896\n",
      "\tLoss: 0.10853785276412964\n",
      "\tLoss: 0.09117577970027924\n",
      "\tLoss: 0.14882196485996246\n",
      "\tLoss: 0.09155351668596268\n",
      "\tLoss: 0.09859448671340942\n",
      "\tLoss: 0.040735647082328796\n",
      "\tLoss: 0.11827898025512695\n",
      "\tLoss: 0.12250161916017532\n",
      "\tLoss: 0.15818101167678833\n",
      "\tLoss: 0.10237289220094681\n",
      "\tLoss: 0.1670273095369339\n",
      "\tLoss: 0.1397799551486969\n",
      "\tLoss: 0.10379697382450104\n",
      "\tLoss: 0.10674262791872025\n",
      "\tLoss: 0.07965794205665588\n",
      "\tLoss: 0.08555477857589722\n",
      "\tLoss: 0.10363800823688507\n",
      "\tLoss: 0.087033711373806\n",
      "\tLoss: 0.09066066145896912\n",
      "\tLoss: 0.04521777108311653\n",
      "\tLoss: 0.10731035470962524\n",
      "\tLoss: 0.11175820976495743\n",
      "\tLoss: 0.13382944464683533\n",
      "\tLoss: 0.11080644279718399\n",
      "\tLoss: 0.12210996448993683\n",
      "\tLoss: 0.15012779831886292\n",
      "\tLoss: 0.0949649065732956\n",
      "\tLoss: 0.15621644258499146\n",
      "\tLoss: 0.09648522734642029\n",
      "\tLoss: 0.07715652883052826\n",
      "\tLoss: 0.14754337072372437\n",
      "\tLoss: 0.1183747947216034\n",
      "\tLoss: 0.14123520255088806\n",
      "\tLoss: 0.12316617369651794\n",
      "\tLoss: 0.13354024291038513\n",
      "\tLoss: 0.12749075889587402\n",
      "\tLoss: 0.11760368943214417\n",
      "\tLoss: 0.13609836995601654\n",
      "\tLoss: 0.10770197212696075\n",
      "\tLoss: 0.16646964848041534\n",
      "\tLoss: 0.20243479311466217\n",
      "\tLoss: 0.1046469584107399\n",
      "\tLoss: 0.09823735058307648\n",
      "\tLoss: 0.19134990870952606\n",
      "\tLoss: 0.12308342754840851\n",
      "\tLoss: 0.07253672927618027\n",
      "\tLoss: 0.09616820514202118\n",
      "\tLoss: 0.10040481388568878\n",
      "\tLoss: 0.08200761675834656\n",
      "\tLoss: 0.11208243668079376\n",
      "\tLoss: 0.1547490805387497\n",
      "\tLoss: 0.09218454360961914\n",
      "\tLoss: 0.06469911336898804\n",
      "\tLoss: 0.14103010296821594\n",
      "\tLoss: 0.09771174192428589\n",
      "\tLoss: 0.09151659905910492\n",
      "\tLoss: 0.1361575722694397\n",
      "\tLoss: 0.1313089281320572\n",
      "\tLoss: 0.12178993225097656\n",
      "\tLoss: 0.1669425666332245\n",
      "\tLoss: 0.12389879673719406\n",
      "\tLoss: 0.14992564916610718\n",
      "\tLoss: 0.10543479025363922\n",
      "\tLoss: 0.17858178913593292\n",
      "\tLoss: 0.15369220077991486\n",
      "\tLoss: 0.12574829161167145\n",
      "\tLoss: 0.11013436317443848\n",
      "\tLoss: 0.12258633226156235\n",
      "\tLoss: 0.07410788536071777\n",
      "\tLoss: 0.061523694545030594\n",
      "\tLoss: 0.11658334732055664\n",
      "\tLoss: 0.08577732741832733\n",
      "\tLoss: 0.1442306637763977\n",
      "\tLoss: 0.09083070605993271\n",
      "\tLoss: 0.09578484296798706\n",
      "\tLoss: 0.11743156611919403\n",
      "\tLoss: 0.10619784891605377\n",
      "\tLoss: 0.128896564245224\n",
      "\tLoss: 0.11238646507263184\n",
      "\tLoss: 0.1626555323600769\n",
      "\tLoss: 0.1823277324438095\n",
      "\tLoss: 0.10296791791915894\n",
      "\tLoss: 0.12210103124380112\n",
      "\tLoss: 0.1334628313779831\n",
      "\tLoss: 0.11263667047023773\n",
      "\tLoss: 0.07005979865789413\n",
      "\tLoss: 0.1118597611784935\n",
      "\tLoss: 0.1376660019159317\n",
      "\tLoss: 0.15170572698116302\n",
      "\tLoss: 0.08915088325738907\n",
      "\tLoss: 0.14206960797309875\n",
      "\tLoss: 0.0936499834060669\n",
      "\tLoss: 0.09828506410121918\n",
      "\tLoss: 0.163671612739563\n",
      "\tLoss: 0.15571445226669312\n",
      "\tLoss: 0.08643472194671631\n",
      "\tLoss: 0.1467064768075943\n",
      "\tLoss: 0.09177905321121216\n",
      "\tLoss: 0.11305411159992218\n",
      "\tLoss: 0.09757127612829208\n",
      "\tLoss: 0.1303023099899292\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.08634928613901138\n",
      "\tLoss: 0.1384822130203247\n",
      "\tLoss: 0.12201766669750214\n",
      "\tLoss: 0.09942683577537537\n",
      "\tLoss: 0.14618167281150818\n",
      "\tLoss: 0.11879187077283859\n",
      "\tLoss: 0.14510689675807953\n",
      "\tLoss: 0.13130803406238556\n",
      "\tLoss: 0.17195257544517517\n",
      "\tLoss: 0.10522910207509995\n",
      "\tLoss: 0.1472320556640625\n",
      "\tLoss: 0.11938747763633728\n",
      "\tLoss: 0.1362842172384262\n",
      "\tLoss: 0.0896514505147934\n",
      "\tLoss: 0.11820054799318314\n",
      "\tLoss: 0.06957818567752838\n",
      "\tLoss: 0.1437608003616333\n",
      "\tLoss: 0.0741967037320137\n",
      "\tLoss: 0.1725059151649475\n",
      "\tLoss: 0.1550925076007843\n",
      "\tLoss: 0.08518511056900024\n",
      "\tLoss: 0.12995648384094238\n",
      "\tLoss: 0.09657153487205505\n",
      "\tLoss: 0.09705749154090881\n",
      "\tLoss: 0.14340436458587646\n",
      "\tLoss: 0.0865030586719513\n",
      "\tLoss: 0.10648683458566666\n",
      "\tLoss: 0.09293840825557709\n",
      "\tLoss: 0.06723547726869583\n",
      "\tLoss: 0.10535387694835663\n",
      "\tLoss: 0.12117670476436615\n",
      "\tLoss: 0.13120143115520477\n",
      "\tLoss: 0.096847765147686\n",
      "\tLoss: 0.0553131140768528\n",
      "\tLoss: 0.15864509344100952\n",
      "\tLoss: 0.1334085464477539\n",
      "\tLoss: 0.0968603789806366\n",
      "\tLoss: 0.14171548187732697\n",
      "\tLoss: 0.09685665369033813\n",
      "\tLoss: 0.12403284013271332\n",
      "\tLoss: 0.1282660961151123\n",
      "\tLoss: 0.12911680340766907\n",
      "\tLoss: 0.11748679727315903\n",
      "\tLoss: 0.08703465759754181\n",
      "\tLoss: 0.1422872543334961\n",
      "\tLoss: 0.10722172260284424\n",
      "\tLoss: 0.06791090965270996\n",
      "\tLoss: 0.11618295311927795\n",
      "\tLoss: 0.1015831008553505\n",
      "\tLoss: 0.06943066418170929\n",
      "\tLoss: 0.14656855165958405\n",
      "\tLoss: 0.08325041830539703\n",
      "\tLoss: 0.12998075783252716\n",
      "\tLoss: 0.08208783715963364\n",
      "\tLoss: 0.11613547801971436\n",
      "\tLoss: 0.12058621644973755\n",
      "\tLoss: 0.11162843555212021\n",
      "\tLoss: 0.13249889016151428\n",
      "\tLoss: 0.09796838462352753\n",
      "\tLoss: 0.10624085366725922\n",
      "\tLoss: 0.11772754043340683\n",
      "\tLoss: 0.07613688707351685\n",
      "\tLoss: 0.1678130030632019\n",
      "\tLoss: 0.09836923331022263\n",
      "\tLoss: 0.11569733917713165\n",
      "\tLoss: 0.11260891705751419\n",
      "\tLoss: 0.15245983004570007\n",
      "\tLoss: 0.1401129961013794\n",
      "\tLoss: 0.11292585730552673\n",
      "\tLoss: 0.11398749053478241\n",
      "\tLoss: 0.11287179589271545\n",
      "\tLoss: 0.0846257358789444\n",
      "\tLoss: 0.141769677400589\n",
      "\tLoss: 0.14827696979045868\n",
      "\tLoss: 0.12161754071712494\n",
      "\tLoss: 0.06738844513893127\n",
      "\tLoss: 0.1339983344078064\n",
      "\tLoss: 0.1443040668964386\n",
      "\tLoss: 0.12879636883735657\n",
      "\tLoss: 0.17373481392860413\n",
      "\tLoss: 0.14591765403747559\n",
      "\tLoss: 0.0768173485994339\n",
      "\tLoss: 0.11019726097583771\n",
      "\tLoss: 0.11294366419315338\n",
      "\tLoss: 0.1383051872253418\n",
      "\tLoss: 0.08611953258514404\n",
      "\tLoss: 0.10630796104669571\n",
      "\tLoss: 0.09067239612340927\n",
      "\tLoss: 0.07778748869895935\n",
      "\tLoss: 0.13822214305400848\n",
      "\tLoss: 0.1541074514389038\n",
      "\tLoss: 0.11176241189241409\n",
      "\tLoss: 0.1310293823480606\n",
      "\tLoss: 0.11030951887369156\n",
      "\tLoss: 0.18676665425300598\n",
      "\tLoss: 0.1490551084280014\n",
      "\tLoss: 0.13069956004619598\n",
      "\tLoss: 0.10616359114646912\n",
      "\tLoss: 0.10517913848161697\n",
      "\tLoss: 0.11190973967313766\n",
      "\tLoss: 0.1284305900335312\n",
      "\tLoss: 0.09040825068950653\n",
      "[time] Epoch 31: 426.7556945751421s = 7.1125949095857015m\n",
      "\n",
      "Epoch 32...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.13594508171081543\n",
      "\tLoss: 0.10933467745780945\n",
      "\tLoss: 0.09665762633085251\n",
      "\tLoss: 0.1057189479470253\n",
      "\tLoss: 0.1675383448600769\n",
      "\tLoss: 0.1672784388065338\n",
      "\tLoss: 0.12825852632522583\n",
      "\tLoss: 0.13638131320476532\n",
      "\tLoss: 0.14475944638252258\n",
      "\tLoss: 0.09686143696308136\n",
      "\tLoss: 0.1369485855102539\n",
      "\tLoss: 0.16950976848602295\n",
      "\tLoss: 0.1376848965883255\n",
      "\tLoss: 0.10705311596393585\n",
      "\tLoss: 0.13291700184345245\n",
      "\tLoss: 0.13679029047489166\n",
      "\tLoss: 0.16380256414413452\n",
      "\tLoss: 0.0836646556854248\n",
      "\tLoss: 0.11052503436803818\n",
      "\tLoss: 0.11041396856307983\n",
      "\tLoss: 0.12630733847618103\n",
      "\tLoss: 0.11532418429851532\n",
      "\tLoss: 0.07692337781190872\n",
      "\tLoss: 0.1852019727230072\n",
      "\tLoss: 0.14553026854991913\n",
      "\tLoss: 0.14404910802841187\n",
      "\tLoss: 0.10598795115947723\n",
      "\tLoss: 0.14489927887916565\n",
      "\tLoss: 0.10065574944019318\n",
      "\tLoss: 0.14510929584503174\n",
      "\tLoss: 0.08892159909009933\n",
      "\tLoss: 0.09892210364341736\n",
      "\tLoss: 0.10995150357484818\n",
      "\tLoss: 0.15344488620758057\n",
      "\tLoss: 0.06919901072978973\n",
      "\tLoss: 0.12621556222438812\n",
      "\tLoss: 0.17482703924179077\n",
      "\tLoss: 0.1482256054878235\n",
      "\tLoss: 0.12118938565254211\n",
      "\tLoss: 0.0921783447265625\n",
      "\tLoss: 0.10816887766122818\n",
      "\tLoss: 0.15440259873867035\n",
      "\tLoss: 0.14478042721748352\n",
      "\tLoss: 0.16966612637043\n",
      "\tLoss: 0.1586577296257019\n",
      "\tLoss: 0.11503766477108002\n",
      "\tLoss: 0.1242992952466011\n",
      "\tLoss: 0.12774309515953064\n",
      "\tLoss: 0.13426488637924194\n",
      "\tLoss: 0.20180079340934753\n",
      "\tLoss: 0.1482655256986618\n",
      "\tLoss: 0.10071470588445663\n",
      "\tLoss: 0.12263334542512894\n",
      "\tLoss: 0.11642678827047348\n",
      "\tLoss: 0.1167493462562561\n",
      "\tLoss: 0.12058527022600174\n",
      "\tLoss: 0.14651912450790405\n",
      "\tLoss: 0.14713670313358307\n",
      "\tLoss: 0.057429902255535126\n",
      "\tLoss: 0.09645459055900574\n",
      "\tLoss: 0.1287306249141693\n",
      "\tLoss: 0.1254488229751587\n",
      "\tLoss: 0.07792063802480698\n",
      "\tLoss: 0.12284265458583832\n",
      "\tLoss: 0.12969043850898743\n",
      "\tLoss: 0.1278158724308014\n",
      "\tLoss: 0.10016033053398132\n",
      "\tLoss: 0.13027414679527283\n",
      "\tLoss: 0.09191663563251495\n",
      "\tLoss: 0.10831543803215027\n",
      "\tLoss: 0.12897862493991852\n",
      "\tLoss: 0.10351857542991638\n",
      "\tLoss: 0.08858496695756912\n",
      "\tLoss: 0.16503602266311646\n",
      "\tLoss: 0.12053851783275604\n",
      "\tLoss: 0.1166185587644577\n",
      "\tLoss: 0.16412442922592163\n",
      "\tLoss: 0.07161976397037506\n",
      "\tLoss: 0.15448728203773499\n",
      "\tLoss: 0.09950529038906097\n",
      "\tLoss: 0.130157932639122\n",
      "\tLoss: 0.1609693318605423\n",
      "\tLoss: 0.10479635000228882\n",
      "\tLoss: 0.13792455196380615\n",
      "\tLoss: 0.09372951090335846\n",
      "\tLoss: 0.1494954228401184\n",
      "\tLoss: 0.08278310298919678\n",
      "\tLoss: 0.13540279865264893\n",
      "\tLoss: 0.11389235407114029\n",
      "\tLoss: 0.15062236785888672\n",
      "\tLoss: 0.10624714195728302\n",
      "\tLoss: 0.13638052344322205\n",
      "\tLoss: 0.11671055853366852\n",
      "\tLoss: 0.042708754539489746\n",
      "\tLoss: 0.079424649477005\n",
      "\tLoss: 0.11522962152957916\n",
      "\tLoss: 0.1062421202659607\n",
      "\tLoss: 0.10395082086324692\n",
      "\tLoss: 0.08338501304388046\n",
      "\tLoss: 0.10225869715213776\n",
      "\tLoss: 0.0954599529504776\n",
      "\tLoss: 0.10758265852928162\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.05337978154420853\n",
      "\tLoss: 0.13229839503765106\n",
      "\tLoss: 0.13791511952877045\n",
      "\tLoss: 0.19624759256839752\n",
      "\tLoss: 0.16344863176345825\n",
      "\tLoss: 0.11293631792068481\n",
      "\tLoss: 0.08247759938240051\n",
      "\tLoss: 0.13351082801818848\n",
      "\tLoss: 0.13139626383781433\n",
      "\tLoss: 0.11953140050172806\n",
      "\tLoss: 0.06170375645160675\n",
      "\tLoss: 0.12345921248197556\n",
      "\tLoss: 0.12551116943359375\n",
      "\tLoss: 0.08813847601413727\n",
      "\tLoss: 0.1389138400554657\n",
      "\tLoss: 0.10504373162984848\n",
      "\tLoss: 0.14633971452713013\n",
      "\tLoss: 0.12064248323440552\n",
      "\tLoss: 0.08579953014850616\n",
      "\tLoss: 0.07683242112398148\n",
      "\tLoss: 0.06603383272886276\n",
      "\tLoss: 0.1383887529373169\n",
      "\tLoss: 0.11973468959331512\n",
      "\tLoss: 0.16565221548080444\n",
      "\tLoss: 0.14269784092903137\n",
      "\tLoss: 0.17237943410873413\n",
      "\tLoss: 0.10267537832260132\n",
      "\tLoss: 0.1258840262889862\n",
      "\tLoss: 0.08904224634170532\n",
      "\tLoss: 0.07522185891866684\n",
      "\tLoss: 0.09487417340278625\n",
      "\tLoss: 0.16365298628807068\n",
      "\tLoss: 0.08211889117956161\n"
     ]
    }
   ],
   "source": [
    "# The loss function is the quantity that will be\n",
    "# minimized during training.\n",
    "# TO-DO: Escoger 'loss' adecuado.\n",
    "#loss_func = losses.ContrastiveLoss().to(device)\n",
    "loss_func = nn.CosineEmbeddingLoss().to(device)\n",
    "\n",
    "# The optimizer determines how the network will be\n",
    "# updated based on the loss function.\n",
    "# TO-DO: Escoger 'optimizer' adecuado.\n",
    "#optimizer = torch.optim.SGD(red.parameters(), lr=lr, momentum=momentum) # Usado por MNIST Colab.\n",
    "optimizer = torch.optim.Adam(red.parameters(), lr = lr) # Usado por AlexNet (TMLoss y MSELoss).\n",
    "\n",
    "# Para obtener estadísticas del entrenamiento.\n",
    "loss_history = []\n",
    "epoch_timing = []\n",
    "\n",
    "def train(epoch):\n",
    "    \"\"\"\n",
    "    Entrenamiento de la RN.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicio, en segundos, del epoch.\n",
    "    epoch_start = timer()\n",
    "    \n",
    "    # Imprimimos el número de epoch.\n",
    "    print(f\"Epoch {epoch}...\")\n",
    "    \n",
    "    # Recorremos cada DataLoader.\n",
    "    for dl_idx, dl in enumerate(birds_dl_train):\n",
    "        \n",
    "        # DEBUG.\n",
    "        print(f\"\\tTrabajando con el {dl_idx}-ésimo DataLoader:\")\n",
    "        \n",
    "        for batch_idx, data in enumerate(dl):\n",
    "\n",
    "            # DEBUG.\n",
    "            relative_batch_str = f\"{batch_idx+1}/{len(dl)}\"\n",
    "            #print(f\"\\tProcesando lote {relative_batch_str}...\")\n",
    "        \n",
    "            # Completando lotes que no tienen tamaño batch_size.\n",
    "            incremento = 0\n",
    "            tam_original = len(data[2])\n",
    "            while len(data[2]) < batch_size:\n",
    "                x,y,l = birds_ds.__getitem__()\n",
    "                x = torch.tensor(x)[None, :]\n",
    "                y = torch.tensor(y)[None, :]\n",
    "                l = torch.unsqueeze(torch.tensor(l), 0)\n",
    "                data[0] = torch.cat((data[0], x), 0)\n",
    "                data[1] = torch.cat((data[1], y), 0)\n",
    "                data[2] = torch.cat((data[2], l), 0)\n",
    "                incremento += 1\n",
    "            if incremento != 0:\n",
    "                print(f\"\\tLote {relative_batch_str} de tamaño {tam_original} incrementado en {incremento}.\")\n",
    "            assert len(data[0]) == len(data[1])\n",
    "            assert len(data[0]) == len(data[2])\n",
    "\n",
    "            # 'data' es una lista que representa un lote:\n",
    "            # data[0] contiene los primeros cachos de audio.\n",
    "            # data[1] contiene los segundos cachos de audio.\n",
    "            # data[2] contiene las etiquetas.\n",
    "            for i,d in enumerate(data):\n",
    "                data[i] = d.to(device)\n",
    "\n",
    "            # Convertimos las etiquetas a tipo flotante.\n",
    "            # Necesario para la función de pérdida BCE.\n",
    "            #data[2] = data[2].to(torch.float32)\n",
    "            # TO-DO: ¿Ya no se usará BCE?\n",
    "\n",
    "            # Vaciamos los gradientes para este lote.\n",
    "            optimizer.zero_grad()\n",
    "            # In PyTorch, for every mini-batch during the training phase,\n",
    "            # we typically want to explicitly set the gradients to zero\n",
    "            # before starting to do backpropragation (i.e., updating the\n",
    "            # Weights and biases) because PyTorch accumulates the gradients\n",
    "            # on subsequent backward passes.\n",
    "\n",
    "            # Metemos los datos a la red neuronal.\n",
    "            output_x, output_y = red(data[0], data[1])\n",
    "\n",
    "            # TO-DO: Describir.\n",
    "\n",
    "            start = timer()\n",
    "            loss = loss_func(output_x, output_y, data[2])\n",
    "            end = timer()\n",
    "            #print(f\"\\t[time] Loss function: {end-start}s\") # DEBUG\n",
    "\n",
    "            start = timer()\n",
    "            loss.backward() #dloss/dx for every variable\n",
    "            end = timer()\n",
    "            #print(f\"\\t[time] Loss backward: {end-start}s\") # DEBUG\n",
    "\n",
    "            # TensorBoard.\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            start = timer()\n",
    "            optimizer.step() #to do a one-step update on our parameter.\n",
    "            end = timer()\n",
    "            #print(f\"\\t[time] Optimizer step: {end-start}s\") # DEBUG\n",
    "\n",
    "            # Guardamos estadísticas del entrenamiento.\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "\n",
    "            # DEBUG:\n",
    "            print(f\"\\tLoss: {loss}\")\n",
    "            #print(f\"\\tEtiquetas: {data[2]}\")\n",
    "            #print(f\"\\tOutput: {output}\")\n",
    "            #print()\n",
    "\n",
    "            #break # DEBUG: Permite el entrenamiento de sólo un lote.\n",
    "    \n",
    "    # Fin, en segundos, del epoch.\n",
    "    epoch_end = timer()\n",
    "    \n",
    "    # Tiempo total, en segundos, del epoch.\n",
    "    epoch_time_seconds = epoch_end-epoch_start\n",
    "    epoch_timing.append(epoch_time_seconds)\n",
    "    \n",
    "    # DEBUG:\n",
    "    print(f\"[time] Epoch {epoch}: {epoch_time_seconds}s = {epoch_time_seconds/(60)}m\")\n",
    "    print()\n",
    "    \n",
    "# red.train() le indica al modelo que está siendo entrenado.\n",
    "# Esto ayuda con capas como Dropout y BatchNorm, que están\n",
    "# diseñadas para comporsarse distinto durante entrenamiento\n",
    "# y evaluación.\n",
    "red.train()\n",
    "    \n",
    "# Ejecutamos el entrenamiento definido, \"epochs\" veces.\n",
    "train_start = timer()\n",
    "for epoch in range(1, epochs+1): # Rango [a, b)\n",
    "    train(epoch)\n",
    "    #break # DEBUG: Permite la ejecución de sólo un epoch.\n",
    "train_end = timer()\n",
    "\n",
    "# Call flush() method to make sure that all pending events have been written to disk.\n",
    "# If you do not need the summary writer anymore, call close() method.\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas del entrenamiendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20, 10))\n",
    "plt.plot(range(0,len(loss_history)), loss_history, 'r')\n",
    "plt.title('loss_history')\n",
    "plt.xlabel('Lote')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time] Total training time: 50761.739s = 14.100h\n",
      "[time] Time for each epoch:\n",
      "\t1     432.437s\n",
      "\t2     426.448s\n",
      "\t3     427.402s\n",
      "\t4     433.972s\n",
      "\t5     439.679s\n",
      "\t6     432.551s\n",
      "\t7     428.577s\n",
      "\t8     439.163s\n",
      "\t9     437.648s\n",
      "\t10    435.556s\n",
      "\t11    436.158s\n",
      "\t12    438.646s\n",
      "\t13    435.747s\n",
      "\t14    432.400s\n",
      "\t15    433.652s\n",
      "\t16    429.072s\n",
      "\t17    435.681s\n",
      "\t18    426.718s\n",
      "\t19    431.433s\n",
      "\t20    432.906s\n",
      "\t21    432.846s\n",
      "\t22    432.045s\n",
      "\t23    426.982s\n",
      "\t24    428.186s\n",
      "\t25    432.585s\n",
      "\t26    424.396s\n",
      "\t27    432.703s\n",
      "\t28    422.294s\n",
      "\t29    420.029s\n",
      "\t30    421.710s\n",
      "\t31    426.756s\n",
      "\t32    417.613s\n",
      "\t33    429.131s\n",
      "\t34    428.974s\n",
      "\t35    421.848s\n",
      "\t36    421.261s\n",
      "\t37    424.676s\n",
      "\t38    426.378s\n",
      "\t39    434.298s\n",
      "\t40    430.603s\n",
      "\t41    431.038s\n",
      "\t42    428.533s\n",
      "\t43    434.596s\n",
      "\t44    427.022s\n",
      "\t45    420.442s\n",
      "\t46    425.426s\n",
      "\t47    429.720s\n",
      "\t48    424.325s\n",
      "\t49    425.763s\n",
      "\t50    429.379s\n",
      "\t51    438.492s\n",
      "\t52    434.501s\n",
      "\t53    435.587s\n",
      "\t54    431.052s\n",
      "\t55    433.240s\n",
      "\t56    433.052s\n",
      "\t57    427.926s\n",
      "\t58    431.037s\n",
      "\t59    422.772s\n",
      "\t60    425.433s\n",
      "\t61    429.621s\n",
      "\t62    424.629s\n",
      "\t63    420.898s\n",
      "\t64    420.585s\n",
      "\t65    426.332s\n",
      "\t66    428.501s\n",
      "\t67    421.722s\n",
      "\t68    423.646s\n",
      "\t69    423.742s\n",
      "\t70    428.224s\n",
      "\t71    421.466s\n",
      "\t72    423.625s\n",
      "\t73    423.589s\n",
      "\t74    416.421s\n",
      "\t75    418.326s\n",
      "\t76    420.291s\n",
      "\t77    417.889s\n",
      "\t78    417.664s\n",
      "\t79    414.741s\n",
      "\t80    415.852s\n",
      "\t81    413.032s\n",
      "\t82    416.348s\n",
      "\t83    415.339s\n",
      "\t84    418.571s\n",
      "\t85    410.945s\n",
      "\t86    412.334s\n",
      "\t87    414.437s\n",
      "\t88    417.264s\n",
      "\t89    416.911s\n",
      "\t90    407.324s\n",
      "\t91    412.811s\n",
      "\t92    411.013s\n",
      "\t93    415.760s\n",
      "\t94    414.963s\n",
      "\t95    412.247s\n",
      "\t96    412.816s\n",
      "\t97    423.542s\n",
      "\t98    413.238s\n",
      "\t99    410.705s\n",
      "\t100   412.521s\n",
      "\t101   412.648s\n",
      "\t102   411.078s\n",
      "\t103   409.935s\n",
      "\t104   411.806s\n",
      "\t105   413.504s\n",
      "\t106   413.591s\n",
      "\t107   409.108s\n",
      "\t108   406.894s\n",
      "\t109   413.389s\n",
      "\t110   409.525s\n",
      "\t111   415.163s\n",
      "\t112   416.381s\n",
      "\t113   415.297s\n",
      "\t114   412.888s\n",
      "\t115   418.175s\n",
      "\t116   406.810s\n",
      "\t117   415.488s\n",
      "\t118   413.121s\n",
      "\t119   416.083s\n",
      "\t120   410.446s\n"
     ]
    }
   ],
   "source": [
    "train_time_seconds = train_end-train_start\n",
    "print(\"[time] Total training time: {:.3f}s = {:.3f}h\".format(train_time_seconds, train_time_seconds/(60*60)))\n",
    "print(f\"[time] Time for each epoch:\")\n",
    "for i,x in enumerate(epoch_timing):\n",
    "    print (\"\\t{:<5} {:.3f}s\".format(i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testeo de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1...\n",
      "\tProcesando lote 1/103...\n",
      "\t\tCon umbral -0.5: 12/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 2/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 3/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 4/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 5/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 6/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 7/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 28/32\n",
      "\tProcesando lote 8/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 9/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 22/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 10/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 11/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 12/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 13/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 14/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 15/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 16/103...\n",
      "\t\tCon umbral -0.5: 12/32\n",
      "\t\tCon umbral    0: 20/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 17/103...\n",
      "\t\tCon umbral -0.5: 11/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 18/103...\n",
      "\t\tCon umbral -0.5: 21/32\n",
      "\t\tCon umbral    0: 30/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 19/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 22/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 28/32\n",
      "\tProcesando lote 20/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 29/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 21/103...\n",
      "\t\tCon umbral -0.5: 11/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 22/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 23/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 24/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 25/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 26/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 27/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 28/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 29/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 30/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 31/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 27/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 32/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 33/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 28/32\n",
      "\tProcesando lote 34/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 35/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 36/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 37/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 38/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 39/103...\n",
      "\t\tCon umbral -0.5: 11/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 40/103...\n",
      "\t\tCon umbral -0.5: 22/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 41/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 42/103...\n",
      "\t\tCon umbral -0.5: 10/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 43/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 44/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 45/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 46/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 47/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 48/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 29/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 49/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 27/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 50/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 51/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 52/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 53/103...\n",
      "\t\tCon umbral -0.5: 10/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 54/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 55/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 56/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 57/103...\n",
      "\t\tCon umbral -0.5: 21/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 58/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 59/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 60/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 61/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 62/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 63/103...\n",
      "\t\tCon umbral -0.5: 12/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 64/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 65/103...\n",
      "\t\tCon umbral -0.5: 12/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 66/103...\n",
      "\t\tCon umbral -0.5: 23/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 67/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 68/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 69/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 70/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 71/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 72/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 28/32\n",
      "\tProcesando lote 73/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 74/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 75/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 22/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 76/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 77/103...\n",
      "\t\tCon umbral -0.5: 21/32\n",
      "\t\tCon umbral    0: 29/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 78/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 79/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 80/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 81/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 82/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 83/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 28/32\n",
      "\tProcesando lote 84/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 85/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 86/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 87/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 88/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 89/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 90/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 22/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 91/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 92/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 93/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 94/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 95/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 26/32\n",
      "\tProcesando lote 96/103...\n",
      "\t\tCon umbral -0.5: 13/32\n",
      "\t\tCon umbral    0: 22/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 97/103...\n",
      "\t\tCon umbral -0.5: 21/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 26/32\n",
      "\tProcesando lote 98/103...\n",
      "\t\tCon umbral -0.5: 13/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 99/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 100/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 30/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 101/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 102/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 103/103...\n",
      "\tLote de tamaño 13 incrementado en 19.\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n"
     ]
    }
   ],
   "source": [
    "# Cargado de la RN.\n",
    "fecha = \"2023-01-28-17-37\"\n",
    "#red = RN().to(device)\n",
    "#red.load_state_dict(torch.load(DIR_models+fecha+\".pt\"))\n",
    "red.eval()\n",
    "    \n",
    "def test(epoch):\n",
    "    \"\"\"\n",
    "    Testeo de la RN.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicio, en segundos, del epoch.\n",
    "    epoch_start = timer()\n",
    "    \n",
    "    # Imprimimos el número de epoch.\n",
    "    print(f\"Epoch {epoch}...\")\n",
    "    \n",
    "    for batch_idx, data in enumerate(birds_dl_test):\n",
    "\n",
    "        # DEBUG.\n",
    "        print(f\"\\tProcesando lote {batch_idx+1}/{len(birds_dl_test)}...\")\n",
    "        \n",
    "        # Completando lotes que no tienen tamaño batch_size.\n",
    "        incremento = 0\n",
    "        tam_original = len(data[2])\n",
    "        while len(data[2]) < batch_size:\n",
    "            x,y,l = birds_ds.__getitem__()\n",
    "            x = torch.tensor(x)[None, :]\n",
    "            y = torch.tensor(y)[None, :]\n",
    "            l = torch.unsqueeze(torch.tensor(l), 0)\n",
    "            data[0] = torch.cat((data[0], x), 0)\n",
    "            data[1] = torch.cat((data[1], y), 0)\n",
    "            data[2] = torch.cat((data[2], l), 0)\n",
    "            incremento += 1\n",
    "        if incremento != 0:\n",
    "            print(f\"\\tLote de tamaño {tam_original} incrementado en {incremento}.\")\n",
    "        assert len(data[0]) == len(data[1])\n",
    "        assert len(data[0]) == len(data[2])\n",
    "        \n",
    "        # 'data' es una lista que representa un lote:\n",
    "        # data[0] contiene los primeros cachos de audio.\n",
    "        # data[1] contiene los segundos cachos de audio.\n",
    "        # data[2] contiene las etiquetas.\n",
    "        for i,d in enumerate(data):\n",
    "            data[i] = d.to(device)\n",
    "        \n",
    "        # Metemos los datos a la red neuronal.\n",
    "        output_x, output_y = red(data[0], data[1])\n",
    "        \n",
    "        # Realizamos la diferencia con Similitud Coseno.\n",
    "        cos = nn.CosineSimilarity()\n",
    "        diff = cos(output_x, output_y)\n",
    "        \n",
    "        #print(f\"Etiquetas: {data[2]}\")\n",
    "        #print(f\"CosineSimilarity: {diff}\")\n",
    "\n",
    "        # Estadísticas del testeo del lote.\n",
    "        correctNeg50 = 0\n",
    "        correctZero = 0\n",
    "        correctPos50 = 0\n",
    "        correctUmbral = 0\n",
    "        umbral = 0.75 # Umbral entre -1 y 1.\n",
    "        \n",
    "        for i,l in enumerate(data[2]):\n",
    "            #total += 1\n",
    "            \n",
    "            if (diff[i] >= -0.5 and l == 1) or (diff[i] < -0.5 and l == -1):\n",
    "                correctNeg50 += 1\n",
    "            \n",
    "            if (diff[i] >= 0 and l == 1) or (diff[i] < 0 and l == -1):\n",
    "                correctZero += 1\n",
    "            \n",
    "            if (diff[i] >= 0.5 and l == 1) or (diff[i] < 0.5 and l == -1):\n",
    "                correctPos50 += 1\n",
    "            \n",
    "            if (diff[i] >= umbral and l == 1) or (diff[i] < umbral and l == -1):\n",
    "                correctUmbral += 1\n",
    "        \n",
    "        print(f\"\\t\\tCon umbral -0.5: {correctNeg50}/{len(data[2])}\")\n",
    "        print(f\"\\t\\tCon umbral    0: {correctZero}/{len(data[2])}\")\n",
    "        print(f\"\\t\\tCon umbral  0.5: {correctPos50}/{len(data[2])}\")\n",
    "        print(f\"\\t\\tCon umbral {umbral}: {correctUmbral}/{len(data[2])}\")\n",
    "        \n",
    "        # DEBUG: Permite el testeo de un sólo lote\n",
    "        #break\n",
    "        \n",
    "        # TO-DO: Terminar definición del testeo.\n",
    "\n",
    "# Ejecutamos el testeo definido, \"epochs\" veces.\n",
    "test_start = timer()\n",
    "for epoch in range(1, epochs+1): # Rango [a, b)\n",
    "    test(epoch)\n",
    "    break # DEBUG: Permite la ejecución de sólo un epoch.\n",
    "test_end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas del testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time] Total testing time: 207.584s = 3.460m = 0.058h\n"
     ]
    }
   ],
   "source": [
    "test_time_seconds = test_end-test_start\n",
    "print(\"[time] Total testing time: {:.3f}s = {:.3f}m = {:.3f}h\".format(test_time_seconds, test_time_seconds/60, test_time_seconds/(60*60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardado de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(red.state_dict(), DIR_models+dt_string+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG\n",
    "\n",
    "Esta celda y las siguientes son para testear. Han de ser eliminadas cuando se limpie el código de este *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado de objetos.\n",
    "with open(DIR_objects+dt_string+\"_loss-history\", \"wb\") as file:\n",
    "    pickle.dump(loss_history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargado de objetos.\n",
    "fecha = \"\"\n",
    "\n",
    "#with open(DIR_objects+fecha+\"_loss-history\", \"rb\") as file:\n",
    "    #pickle.dump(loss_history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = birds_df.iloc[5][file_col_name]\n",
    "#librosa_process(birds_path+f, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.uniform(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9\n",
      "1 2 3 4 5 6 7 8 9\n"
     ]
    }
   ],
   "source": [
    "print(*range(0,10))\n",
    "print(*range(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06-02-2023-16-05-55\n"
     ]
    }
   ],
   "source": [
    "dt_string = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "print(dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargado de modelos.\n",
    "fecha = \"\"\n",
    "\n",
    "# Definimos un modelo que alojará a la Red Neuronal.\n",
    "#modelo = RN().to(device)\n",
    "\n",
    "# Actualizamos el modelo.\n",
    "#modelo.load_state_dict(torch.load(DIR_models+fecha+\".pt\"))\n",
    "\n",
    "# modelo.eval() le indica al modelo que ha de evaluar.\n",
    "#modelo.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3277\n",
      "3277\n",
      "total_audios/batch_size = 102.40625\n"
     ]
    }
   ],
   "source": [
    "print(len(birds_df))\n",
    "print(len(birds_ds))\n",
    "print(f\"total_audios/batch_size = {len(birds_df)/batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha = \"2023-02-01-05-24\"\n",
    "#red = RN().to(device)\n",
    "#red.load_state_dict(torch.load(DIR_models+fecha+\".pt\"))\n",
    "#red.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: -1\n",
      "diff: tensor([-0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657,\n",
      "        -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657,\n",
      "        -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657,\n",
      "        -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657, -0.0657],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x,y,l = birds_ds.__getitem__()\n",
    "\n",
    "x = np.expand_dims(x, 0)\n",
    "x = np.repeat(x, batch_size, axis=0)\n",
    "x = torch.tensor(x).to(device)\n",
    "\n",
    "y = np.expand_dims(y, 0)\n",
    "y = np.repeat(y, batch_size, axis=0)\n",
    "y = torch.tensor(y).to(device)\n",
    "\n",
    "#l = np.repeat(l, batch_size)\n",
    "\n",
    "output_x, output_y = red(x, y)\n",
    "cos = nn.CosineSimilarity()\n",
    "diff = cos(output_x, output_y)\n",
    "\n",
    "print(f\"label: {l}\")\n",
    "print(f\"diff: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En México:\n",
      "2023-02-06 10:05:55.271085-06:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "print(\"En México:\")\n",
    "print(datetime.now(timezone(timedelta(hours=-6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
