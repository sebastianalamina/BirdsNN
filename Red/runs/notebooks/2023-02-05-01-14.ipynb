{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Este *notebook* incluye:\n",
    "- Pequeños ejemplos de uso de *pandas*.\n",
    "- Un *DataSet* (de *PyTorch*) que almacena información de los archivos de audio con los cantos de las aves. Este *DataSet*, al solicitársele el i-ésimo *item*, devuelve un cacho del i-ésimo audio, un cacho de un j-ésimo audio, y un 0 o 1 si `i != j` o `i == j` respectivamente.\n",
    "- Un *DataLoader* (de *PyTorch*) que envuelve al *DataSet* previamente descrito.\n",
    "- Una Red Neuronal (de *PyTorch*) que toma los espectrogramas de dos audios de longitud 1s cada uno, y devuelve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones\n",
    "\n",
    "Importación de las bibliotecas a utilizar, y una pequeña descripción de cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas is an open source data analysis and manipulation tool.\n",
    "import pandas as pd\n",
    "\n",
    "# NumPy is for scientific computing with Python\n",
    "import numpy as np\n",
    "\n",
    "# \n",
    "import tensorflow as tf\n",
    "\n",
    "# PyTorch is an open source machine learning framework.\n",
    "import torch\n",
    "\n",
    "# PyTorch provides the torch.nn module to help us\n",
    "# in creating and training of the neural network.\n",
    "import torch.nn as nn\n",
    "\n",
    "# PyTorch has two primitives to work with data:\n",
    "# torch.utils.data.Dataset stores the samples and their corresponding labels.\n",
    "# torch.utils.data.DataLoader wraps an iterable around the Dataset.\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# \"The easiest way to use deep metric learning in your application\".\n",
    "# Written in PyTorch.\n",
    "# https://github.com/KevinMusgrave/pytorch-metric-learning\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "# librosa is for music and audio analysis; it provides\n",
    "# the building blocks necessary to create music\n",
    "# information retrieval systems.\n",
    "import librosa\n",
    "\n",
    "# Displays a spectrogram/chromagram/cqt/etc.\n",
    "from librosa.display import specshow\n",
    "\n",
    "# matplotlib.pyplot is a collection of functions that make\n",
    "# matplotlib work like MATLAB. Each pyplot function makes\n",
    "# some change to a figure: e.g., creates a figure, creates\n",
    "# a plotting area in a figure, plots some lines in a plotting\n",
    "# area, decorates the plot with labels, etc.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorBoard is a visualization toolkit for machine learning\n",
    "# experimentation. TensorBoard allows tracking and visualizing\n",
    "# metrics such as loss and accuracy, visualizing the model graph,\n",
    "# viewing histograms, displaying images and much more.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Para tomar el tiempo que toman ciertos procesos de la siguiente manera:\n",
    "# start = timer()\n",
    "# (algún proceso)\n",
    "# end = timer()\n",
    "# El tiempo en segundos es end-start.\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# CUDA-accelerated PyTorch implementation of the\n",
    "# T-Stochastic Neighbor Embedding algorithm.\n",
    "#from tsne_torch import TorchTSNE as TSNE\n",
    "\n",
    "# Manejo de guardado y cargado de objetos mediante archivos.\n",
    "import pickle\n",
    "\n",
    "# Manejo de pseudo-aleatoriedad.\n",
    "import random\n",
    "\n",
    "# Manejo de funciones matemáticas.\n",
    "import math\n",
    "\n",
    "# Manejo de fecha y tiempo.\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables generales\n",
    "\n",
    "Variables generales/globales que se utilizarán a lo largo del *notebook*. Conviene tener este apartado para consultarlas y modificarlas fácilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizando cuda:0 para el procesamiento de datos.\n"
     ]
    }
   ],
   "source": [
    "# Uso del GPU, si está disponible.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilizando {device} para el procesamiento de datos.\")\n",
    "\n",
    "# Cadena con la ubicación del archivo CSV que contiene\n",
    "# el DataFrame con datos de los audios de aves.\n",
    "birds_csv = \"/media/birds/BirdsDataFrame.csv\"\n",
    "\n",
    "# Cadena con la ubicación de los archivos WAV y XML\n",
    "# correspondientes a los audios de aves a procesar.\n",
    "birds_path = \"/media/birds/data/\"\n",
    "\n",
    "# Otras ubicaciones útiles.\n",
    "DIR_runs = \"./runs/\"\n",
    "DIR_objects = DIR_runs+\"python_objects/\"\n",
    "DIR_tensorboard = DIR_runs+\"tensorboard/\"\n",
    "DIR_notebooks = DIR_runs+\"notebooks/\"\n",
    "DIR_models = DIR_runs+\"models/\"\n",
    "\n",
    "# Nombre de la columna, dentro del DataFrame,\n",
    "# que contiene el nombre de los archivos de audio.\n",
    "file_col_name = \"FileName\"\n",
    "\n",
    "# DataFrame (de 'pandas') del archivo CSV dado.\n",
    "birds_df = pd.read_csv(birds_csv)\n",
    "\n",
    "# Los audios de aves se cortarán en cachos cuya longitud\n",
    "# varíe entre len_min segundos y len_max segundos.\n",
    "len_min = 1\n",
    "len_max = 1\n",
    "\n",
    "# Ancho y alto de cada espectrograma.\n",
    "# TO-DO: ¿Es posible calcular esto mediante una fórmula? Resulta del size()/shape de aplicar \"stft\" al audio \"y\".\n",
    "ancho,alto = 1025,87\n",
    "\n",
    "# Número de canales que tendrá cada audio.\n",
    "# Hasta ahora, si un audio tiene 1 canal, y aquí se\n",
    "# especifican 2, se copia el primer canal en un\n",
    "# segundo canal. Si un audio tiene más de 2 canales,\n",
    "# la operación no está definida.\n",
    "audio_channels = 2\n",
    "\n",
    "# Frecuencia de muestreo a la cual TODOS los audios se\n",
    "# muestrearán. Esto es necesario para que los vectores\n",
    "# que representan a los audios tengan los mismos tamaños.\n",
    "sr = 44100\n",
    "\n",
    "# Probabilidad de que dos audios de aves (o\n",
    "# cachos de audios) compartan cierta propiedad.\n",
    "p_prop = 0.5\n",
    "\n",
    "# Variables asociadas a la Red Neuronal.\n",
    "batch_size = 32 # Número de muestras que se tomarán por lote/epoch.\n",
    "epochs = 60 # Veces que se recorrerá un DataSet entero.\n",
    "lr = 6e-05 # Learning Rate.\n",
    "#momentum = 0.5 # The SGD momentum (default: 0.5) is the moving average of our gradients (helps to keep direction).\n",
    "\n",
    "# Para TensorBoard, creamos un SummaryWriter.\n",
    "# Éste escribiría al directorio ./runs/ por defecto.\n",
    "dt_string = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "writer = SummaryWriter(log_dir=DIR_tensorboard+dt_string+\"_adbekunkus\")\n",
    "\n",
    "# Función a utilizar para procesar los audios de aves.\n",
    "def librosa_process(path, cut, cut_len=None):\n",
    "    \"\"\"\n",
    "    Función que carga un audio con Librosa y devuelve el vector\n",
    "    unidimensional que representa al audio, y su frecuencia de muestreo.\n",
    "    :param str path: Ruta donde se ubica el audio.\n",
    "    :param bool cut: ¿Se cortará (y devolverá) sólo un cacho aleatorio del audio?\n",
    "    :param float cut_len: Longitud del cacho de audio (si cut==True).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Longitud del audio completo en segundos.\n",
    "    audio_len = librosa.get_duration(filename=path)\n",
    "    \n",
    "    # Si se desea el audio completo, 'librosa' lo\n",
    "    # cargará desde el inicio hasta el final.\n",
    "    start = 0\n",
    "    duracion = audio_len\n",
    "        \n",
    "    # Si se desea sólo un cacho del audio...\n",
    "    if cut:\n",
    "        \n",
    "        # Determinamos la longitud del cacho\n",
    "        # aleatorio de audio en segundos.\n",
    "        duracion = cut_len if cut_len != None else random.uniform(len_min, len_max) # Rango [a,b].\n",
    "        \n",
    "        # Aseguramos que el audio completo es más\n",
    "        # grande que el tamaño del cacho que queremos.\n",
    "        assert audio_len > duracion\n",
    "        \n",
    "        # Definimos en dónde empezará\n",
    "        # (aleatoriamente) el cacho de audio.\n",
    "        start = random.uniform(0, audio_len-duracion) # Rango [a,b].\n",
    "    \n",
    "    # Obtenemos el audio-vector y su (nueva) frecuencia de muestreo.\n",
    "    y, sampling_rate = librosa.load(path, sr=sr, offset=start, duration=duracion, mono=False)\n",
    "    \n",
    "    # Algunos audios fueron grabados en dos canales (stereo), y otros en\n",
    "    # uno (mono). Convertimos los que fueron grabados en un canal en\n",
    "    # audios de dos canales (al duplicar el único canal que tienen).\n",
    "    if y.ndim == 1:\n",
    "        y = np.repeat(y[np.newaxis, :], 2, axis=0)\n",
    "    \n",
    "    # Función no definida para audios que tienen más de dos canales.\n",
    "    # Igual se lanza un error si los vectores no tienen la longitud adecuada (sr).\n",
    "    assert(y.shape == (2, sr))\n",
    "    \n",
    "    # Short-time Fourier transform (STFT).\n",
    "    # The STFT represents a signal in the time-frequency domain by computing\n",
    "    # discrete Fourier transforms (DFT) over short overlapping windows.\n",
    "    stft = librosa.stft(y)\n",
    "    \n",
    "    # This function (stft) returns a complex-valued matrix D such that\n",
    "    # np.abs(D[..., f, t]) is the magnitude of frequency bin f at frame t.\n",
    "    magnitude = np.abs(stft)\n",
    "    \n",
    "    # Converts an amplitude spectrogram to dB-scaled spectrogram.\n",
    "    spectogram = librosa.amplitude_to_db(magnitude)\n",
    "    \n",
    "    # Devolvemos el espectrograma y su frecuencia de muestreo.\n",
    "    return spectogram, sampling_rate\n",
    "\n",
    "# Comprobaciones sobre las variables aquí definidas.\n",
    "assert len_min <= len_max # Lógicamente, min<=max.\n",
    "assert p_prop >= 0 and p_prop <= 1 # Las probabilidades se encuentran en este rango."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _pandas_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el archivo `birds_csv` cuenta con *N* columnas `columna0,columna1,...,columnaN-1`, imprimimos a continuación el nombre de cada columna, enumerándolas desde cero.\n",
    "\n",
    "**NOTA**: La primera columna no tiene nombre, por lo que *pandas*, al convertir el archivo CSV en un *DataFrame* mediante la función `read_csv()`, le asigna el nombre `Unnamed: 0`. Esta columna sirve para indexar a las entradas dentro del archivo CSV (no confundir con la columna 'index' cuyo propósito es indexar a los archivos de audio de otra manera)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Por cada columna del DataFrame, imprimimos dicha columna.\n",
    "#for i,col in enumerate(birds_df.columns):\n",
    "#    print(f\"{i}:{col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplificamos con la primera entrada del archivo al imprimir qué valor tiene asociado a cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"iloc\" permite indexar por posiciones mediante el uso de enteros.\n",
    "# Por cada columna y valor en la primera línea, imprimimos ambos.\n",
    "#for col, val in birds_df.iloc[0].iteritems():\n",
    "#    print(f\"{col}:{val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algunas columnas tal que todas las entradas del archivo comparten un mismo valor dentro de esa columna. A continuación imprimimos los nombres de las columnas que cumplen ésto, así como el valor que todas las entradas comparten en dicha columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Por cada columna del DataFrame...\n",
    "#for col in birds_df.columns:\n",
    "    \n",
    "    # Si todas las entradas tienen el mismo valor en dicha\n",
    "    # columna, imprimimos la columna y el valor correspondiente.\n",
    "    #if (birds_df[col] == birds_df[col][0]).all():\n",
    "        #print(f\"{col}:{birds_df[col][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del *DataSet*\n",
    "\n",
    "Creamos el *DataSet* de *PyTorch* que guarda y maneja los datos de los archivos de audio (que contienen los cantos de las aves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBirdDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset de audios de aves.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, process_func, audio_path, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        The __init__ function is run once when instantiating the Dataset object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 'df' es el DataFrame a almacenar.\n",
    "        self.df = df\n",
    "        \n",
    "        # 'process_func' toma la ruta de un audio a procesar, y lo procesa.\n",
    "        self.process_func = process_func\n",
    "        \n",
    "        # 'audio_path' es la ruta donde se ubican los archivos de audio.\n",
    "        self.audio_path = audio_path\n",
    "        \n",
    "        # 'transform' and 'target_transform' modify the samples and labels respectively.\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The __len__ function returns the number of samples in our dataset.\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx=None):\n",
    "        \"\"\"\n",
    "        The __getitem__ function loads and returns a sample from the dataset at the given index 'idx'.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Si no se especifica un índice, se toma una muestra aleatoria.\n",
    "        if idx == None:\n",
    "            idx = random.randrange(0, birds_ds.__len__()) # Rango [a,b).\n",
    "        \n",
    "        # Obtenemos la idx-ésima línea del DataFrame almacenado.\n",
    "        # Y el nombre del archivo de audio a procesar.\n",
    "        item = self.df.iloc[idx]\n",
    "        filename = item[file_col_name]\n",
    "        \n",
    "        # Procesamos el primer cacho de audio.\n",
    "        x,_ = self.process_func(self.audio_path+filename, True)\n",
    "        \n",
    "        # Si se desea que ambos cachos de audio compartan la propiedad,\n",
    "        # sólo dejamos la etiqueta como \"1\", y volvemos a procesar\n",
    "        # el mismo archivo de audio de manera aleatoria (más adelante).\n",
    "        if (random.random() < p_prop):\n",
    "            target = 1\n",
    "            \n",
    "        # Si, por otro lado, se desea que los cachos no compartan la\n",
    "        # propiedad, dejamos la etiqueta como \"-1\", y buscamos otro\n",
    "        # archivo de audio para procesar.\n",
    "        else:\n",
    "            target = -1\n",
    "            \n",
    "            # Guardamos la especie del ave del primer cacho de audio.\n",
    "            primera_especie = item[\"Species\"]\n",
    "            \n",
    "            # Quitamos el primer archivo de audio (que ya fue procesado) del\n",
    "            # DataFrame (temporalmente), obtenemos algún renglón aleatorio de\n",
    "            # este nuevo DataFrame (sample() devuelve un DataFrame, por lo que\n",
    "            # es necesario tomar el primer renglón con iloc[0]), y obtenemos\n",
    "            # el nombre del nuevo archivo de audio a procesar.\n",
    "            item = self.df.drop(idx).sample().iloc[0]\n",
    "            filename = item[file_col_name]\n",
    "                  \n",
    "            # Guardamos la especie del ave del segundo cacho de audio.\n",
    "            segunda_especie = item[\"Species\"]\n",
    "            \n",
    "            # TO-DO: Si son la misma especie, ¿sigo buscando otro segundo cacho\n",
    "            # de audio, o cambio el \"target\" a 1? Por ahora sólo lo cambio a 1.\n",
    "            #print(f\"El primer cacho de audio pertenece a un ave {primera_especie}, y el segundo pertenece a un ave {segunda_especie}.\")\n",
    "            if primera_especie == segunda_especie:\n",
    "                target = 1\n",
    "\n",
    "        # Procesamos el segundo cacho de audio.\n",
    "        y,_ = self.process_func(self.audio_path+filename, True)\n",
    "            \n",
    "        # NOTA:\n",
    "        # Aún no se define el uso para 'transform' y 'target_transform'.\n",
    "        # Una propuesta es que 'transform' sustituya a 'process_func'.\n",
    "        \n",
    "        # Devolvemos el primer cacho de audio, el segundo cacho de audio,\n",
    "        # y la etiqueta que indica si ambos comparten (1) o no (0) la propiedad.\n",
    "        return x, y, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el *DataSet* al pasarle:\n",
    "- El *DataFrame* creado previamente con *pandas*.\n",
    "- La función a utilizar para procesar los audios.\n",
    "- La ruta del directorio en el cual se encuentran los archivos de audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_ds = CustomBirdDataset(birds_df, librosa_process, birds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo del *DataSet*\n",
    "\n",
    "Y obtenemos una muestra aleatoria del *DataSet* mediante su función `__getitem__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#birds_ds.__getitem__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del *DataLoader*\n",
    "\n",
    "Creamos dos *DataLoader* de *PyTorch* que envuelven el *DataSet* previamente definido. Uno está definido para el entrenamiento, mientras que otro está definido para el testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_dl_train = [\n",
    "    DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    DataLoader(birds_ds, batch_size=batch_size, shuffle=False, drop_last=True),\n",
    "    #DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    #DataLoader(birds_ds, batch_size=batch_size, shuffle=False, drop_last=True),\n",
    "    #DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "]\n",
    "\n",
    "birds_dl_test = DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo del **DataLoader**\n",
    "\n",
    "El *DataLoader* contiene listas (que regresa la función `__getitem__()` correspondiente al *DataSet*). Estas listas contienen los lotes de tamaño `batch_size` y, para abarcar todos los datos, contiene aproximadamente `tamaño_de_todos_los_datos/batch_size` listas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tamaño del DataSet (de PyTorch) = 3277 = 3277 = Tamaño del DataFrame (de pandas)\n"
     ]
    }
   ],
   "source": [
    "print(f\"- Tamaño del DataSet (de PyTorch) = {len(birds_ds)} = {len(birds_df)} = Tamaño del DataFrame (de pandas)\")\n",
    "#print(f\"- Tamaño del DataLoader (de PyTorch): {len(birds_dl)}\")\n",
    "#iterador = iter(birds_dl)\n",
    "#primer_lote = next(iterador)\n",
    "#print(f\"- Tamaño de la primera lista del DataLoader: {len(primer_lote)}\")\n",
    "#print(f\"- Tamaño de los elementos de la primera lista: {len(primer_lote[0])} {len(primer_lote[1])} {len(primer_lote[2])}\")\n",
    "#print(f\"- Tamaño del DataLoader por el tamaño de cada lote: {len(birds_dl)*batch_size} ≈ {len(birds_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos datos sobre el primer lote para ejemplificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#primeros_cachos, segundos_cachos, labels = primer_lote\n",
    "#print(f\"- Los primeros cachos de audio del primer lote tienen tamaño: {primeros_cachos.size()}\")\n",
    "#print(f\"- Los segundos cachos de audio del primer lote tienen tamaño: {segundos_cachos.size()}\")\n",
    "#print(f\"- Las etiquetas del primer lote tienen tamaño: {labels.size()}\")\n",
    "#print(f\"- Etiquetas del primer lote: {labels}\")\n",
    "#print(f\"- Primeros cachos del primer lote: {primeros_cachos}\")\n",
    "#print(f\"- Segundos cachos del primer lote: {segundos_cachos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module is the base class for all neural network modules.\n",
    "# Our models should also subclass this class.\n",
    "# Modules can also contain other Modules, allowing to nest them in a tree structure.\n",
    "class RN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red Neuronal.\n",
    "    \"\"\"\n",
    "    \n",
    "    #This defines the structure of the NN.\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicialización de la Red Neuronal.\n",
    "        Aquí se define su estructura.\n",
    "        \"\"\"\n",
    "        \n",
    "        #\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inicio de las capas convolucionales.\n",
    "        conv_layers = []\n",
    "        \n",
    "        # Primera capa convolucional.\n",
    "        self.conv1 = nn.Conv2d(in_channels=audio_channels, out_channels=batch_size, kernel_size=(10,10), stride=(2,1))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        conv_layers += [self.conv1, self.relu1, self.mp1]\n",
    "        \n",
    "        # Segunda capa convolucional.\n",
    "        self.conv2 = nn.Conv2d(in_channels=batch_size, out_channels=(batch_size//2), kernel_size=(7,7), stride=(2,1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        conv_layers += [self.conv2, self.relu2, self.mp2]\n",
    "        \n",
    "        # Tercera capa convolucional.\n",
    "        self.conv3 = nn.Conv2d(in_channels=(batch_size//2), out_channels=(batch_size//4), kernel_size=(4,4), stride=(2,1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.mp3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        conv_layers += [self.conv3, self.relu3, self.mp3]\n",
    "        \n",
    "        # Fin de las capas convoluciones.\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        # Inicio de las capas lineales (fully-connected).\n",
    "        fc_layers = []\n",
    "        \n",
    "        # Primera capa lineal.\n",
    "        # TO-DO: Determinar entrada.\n",
    "        # TO-DO: Determinar salida.\n",
    "        self.fc1 = nn.Linear(int(22.5*batch_size),512)\n",
    "        fc_layers += [self.fc1]\n",
    "        \n",
    "        # Fin de las capas lineales.\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "        \n",
    "        # Segunda capa lineal.\n",
    "        # TO-DO: Determinar entrada.\n",
    "        # TO-DO: Determinar salida. ¿Es 1 valor para cada entrada?\n",
    "        self.fc2 = nn.Linear(4063232, 1)\n",
    "        # Ésta no se agrega a las demás,\n",
    "        # pues no se aplica individualmente a\n",
    "        # cada entrada; primero es necesario\n",
    "        # realizar la operación de distancia\n",
    "        # sobre éstas para después aplicar\n",
    "        # esta capa lineal.\n",
    "    \n",
    "    def invididual_process(self, z):\n",
    "        \n",
    "        start = timer()\n",
    "        z = self.conv(z)\n",
    "        end = timer()\n",
    "        #print(f\"\\t\\t[time] Capas convolucionales: time={end-start}s out_size={z.size()}\") # DEBUG\n",
    "        \n",
    "        # Para que 'z' tenga sólo una dimensión...\n",
    "        #print(f\"\\t\\tz antes de z.view: {z.size()}\") # DEBUG\n",
    "        z = z.view(batch_size, -1)\n",
    "        #print(f\"\\t\\tz después de z.view: {z.size()}\") # DEBUG\n",
    "        \n",
    "        start = timer()\n",
    "        z = self.fc(z)\n",
    "        end = timer()\n",
    "        #print(f\"\\t\\t[time] Capas lineales: time={end-start}s out_size={z.size()}\") # DEBUG\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        # TO-DO: Analizar las múltiples dimensiones en 2 o 3 dimensiones.\n",
    "        #TSNE(n_components=2, verbose=True).fit_transform(x)\n",
    "        \n",
    "        #print(f\"\\tProcesando x: {x.size()}\") # DEBUG\n",
    "        x = self.invididual_process(x)\n",
    "        #print(f\"\\tProcesando y: {y.size()}\") # DEBUG\n",
    "        y = self.invididual_process(y)\n",
    "        \n",
    "        # Para que 'x' y 'y' tengan sólo una dimensión...\n",
    "        x = x.view(batch_size, -1)\n",
    "        #print(f\"\\t\\tDespués de aplanar x: {x.size()}\") # DEBUG\n",
    "        y = y.view(batch_size, -1)\n",
    "        #print(f\"\\t\\tDespués de aplanar y: {y.size()}\") # DEBUG\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Definición del modelo.\n",
    "red = RN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.4580209255218506\n",
      "\tLoss: 0.3146936297416687\n",
      "\tLoss: 0.5464334487915039\n",
      "\tLoss: 0.3986399471759796\n",
      "\tLoss: 0.4652072787284851\n",
      "\tLoss: 0.414777547121048\n",
      "\tLoss: 0.5908101797103882\n",
      "\tLoss: 0.4624566435813904\n",
      "\tLoss: 0.4777158498764038\n",
      "\tLoss: 0.5701103210449219\n",
      "\tLoss: 0.424498975276947\n",
      "\tLoss: 0.44678306579589844\n",
      "\tLoss: 0.3688960671424866\n",
      "\tLoss: 0.3892694413661957\n",
      "\tLoss: 0.4395667314529419\n",
      "\tLoss: 0.3976837396621704\n",
      "\tLoss: 0.32370680570602417\n",
      "\tLoss: 0.37446385622024536\n",
      "\tLoss: 0.4418583810329437\n",
      "\tLoss: 0.3987443149089813\n",
      "\tLoss: 0.35805943608283997\n",
      "\tLoss: 0.2647418975830078\n",
      "\tLoss: 0.22427567839622498\n",
      "\tLoss: 0.3807940185070038\n",
      "\tLoss: 0.3387654423713684\n",
      "\tLoss: 0.32224249839782715\n",
      "\tLoss: 0.2741738557815552\n",
      "\tLoss: 0.3249624967575073\n",
      "\tLoss: 0.2922906279563904\n",
      "\tLoss: 0.38125020265579224\n",
      "\tLoss: 0.24842339754104614\n",
      "\tLoss: 0.2806037366390228\n",
      "\tLoss: 0.21992605924606323\n",
      "\tLoss: 0.27060168981552124\n",
      "\tLoss: 0.3027876019477844\n",
      "\tLoss: 0.27364522218704224\n",
      "\tLoss: 0.2657982110977173\n",
      "\tLoss: 0.2667073607444763\n",
      "\tLoss: 0.1910059005022049\n",
      "\tLoss: 0.32436004281044006\n",
      "\tLoss: 0.35971254110336304\n",
      "\tLoss: 0.2982015013694763\n",
      "\tLoss: 0.24141395092010498\n",
      "\tLoss: 0.32471755146980286\n",
      "\tLoss: 0.2692447304725647\n",
      "\tLoss: 0.1930801272392273\n",
      "\tLoss: 0.27612239122390747\n",
      "\tLoss: 0.23811915516853333\n",
      "\tLoss: 0.2873061001300812\n",
      "\tLoss: 0.25731852650642395\n",
      "\tLoss: 0.27584660053253174\n",
      "\tLoss: 0.2499348670244217\n",
      "\tLoss: 0.2717171311378479\n",
      "\tLoss: 0.25354689359664917\n",
      "\tLoss: 0.3566439747810364\n",
      "\tLoss: 0.24775968492031097\n",
      "\tLoss: 0.377562552690506\n",
      "\tLoss: 0.2298445701599121\n",
      "\tLoss: 0.2567823529243469\n",
      "\tLoss: 0.32272961735725403\n",
      "\tLoss: 0.2725573480129242\n",
      "\tLoss: 0.2547706961631775\n",
      "\tLoss: 0.28780561685562134\n",
      "\tLoss: 0.2411249279975891\n",
      "\tLoss: 0.27541255950927734\n",
      "\tLoss: 0.15168558061122894\n",
      "\tLoss: 0.22826355695724487\n",
      "\tLoss: 0.31087836623191833\n",
      "\tLoss: 0.20530471205711365\n",
      "\tLoss: 0.1501714587211609\n",
      "\tLoss: 0.37400007247924805\n",
      "\tLoss: 0.2749450206756592\n",
      "\tLoss: 0.2416127771139145\n",
      "\tLoss: 0.2542579174041748\n",
      "\tLoss: 0.1750566065311432\n",
      "\tLoss: 0.24285148084163666\n",
      "\tLoss: 0.17569246888160706\n",
      "\tLoss: 0.2316102534532547\n",
      "\tLoss: 0.26885247230529785\n",
      "\tLoss: 0.20773856341838837\n",
      "\tLoss: 0.261971652507782\n",
      "\tLoss: 0.2954472601413727\n",
      "\tLoss: 0.3021535277366638\n",
      "\tLoss: 0.2269739806652069\n",
      "\tLoss: 0.22502408921718597\n",
      "\tLoss: 0.21241864562034607\n",
      "\tLoss: 0.18814682960510254\n",
      "\tLoss: 0.18833336234092712\n",
      "\tLoss: 0.20104680955410004\n",
      "\tLoss: 0.2241741418838501\n",
      "\tLoss: 0.2437480390071869\n",
      "\tLoss: 0.20341435074806213\n",
      "\tLoss: 0.19210495054721832\n",
      "\tLoss: 0.3233662545681\n",
      "\tLoss: 0.26372992992401123\n",
      "\tLoss: 0.20795616507530212\n",
      "\tLoss: 0.22102250158786774\n",
      "\tLoss: 0.2125333994626999\n",
      "\tLoss: 0.15312305092811584\n",
      "\tLoss: 0.31479713320732117\n",
      "\tLoss: 0.16806933283805847\n",
      "\tLoss: 0.2454575151205063\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.2102186679840088\n",
      "\tLoss: 0.22524069249629974\n",
      "\tLoss: 0.18124757707118988\n",
      "\tLoss: 0.22451572120189667\n",
      "\tLoss: 0.21912890672683716\n",
      "\tLoss: 0.3099042475223541\n",
      "\tLoss: 0.24876821041107178\n",
      "\tLoss: 0.2529314160346985\n",
      "\tLoss: 0.19716468453407288\n",
      "\tLoss: 0.13883891701698303\n",
      "\tLoss: 0.22697749733924866\n",
      "\tLoss: 0.24688205122947693\n",
      "\tLoss: 0.2072531133890152\n",
      "\tLoss: 0.2498432844877243\n",
      "\tLoss: 0.14935266971588135\n",
      "\tLoss: 0.17932376265525818\n",
      "\tLoss: 0.1553996354341507\n",
      "\tLoss: 0.16531768441200256\n",
      "\tLoss: 0.13873521983623505\n",
      "\tLoss: 0.16449284553527832\n",
      "\tLoss: 0.16602252423763275\n",
      "\tLoss: 0.19888919591903687\n",
      "\tLoss: 0.23662063479423523\n",
      "\tLoss: 0.31130698323249817\n",
      "\tLoss: 0.1275402158498764\n",
      "\tLoss: 0.1660633534193039\n",
      "\tLoss: 0.26164865493774414\n",
      "\tLoss: 0.18439406156539917\n",
      "\tLoss: 0.20454823970794678\n",
      "\tLoss: 0.18089637160301208\n",
      "\tLoss: 0.2835829257965088\n",
      "\tLoss: 0.15675489604473114\n",
      "\tLoss: 0.1610039472579956\n",
      "\tLoss: 0.15941189229488373\n",
      "\tLoss: 0.18077223002910614\n",
      "\tLoss: 0.21714195609092712\n",
      "\tLoss: 0.19417870044708252\n",
      "\tLoss: 0.1730627566576004\n",
      "\tLoss: 0.18313930928707123\n",
      "\tLoss: 0.15836338698863983\n",
      "\tLoss: 0.2228112816810608\n",
      "\tLoss: 0.18169847130775452\n",
      "\tLoss: 0.22067904472351074\n",
      "\tLoss: 0.23878267407417297\n",
      "\tLoss: 0.17385762929916382\n",
      "\tLoss: 0.18874503672122955\n",
      "\tLoss: 0.26277488470077515\n",
      "\tLoss: 0.2151118516921997\n",
      "\tLoss: 0.19908839464187622\n",
      "\tLoss: 0.22688327729701996\n",
      "\tLoss: 0.22084157168865204\n",
      "\tLoss: 0.194729283452034\n",
      "\tLoss: 0.23864150047302246\n",
      "\tLoss: 0.2445719689130783\n",
      "\tLoss: 0.235863596200943\n",
      "\tLoss: 0.1700626015663147\n",
      "\tLoss: 0.24808262288570404\n",
      "\tLoss: 0.1575576812028885\n",
      "\tLoss: 0.1734790951013565\n",
      "\tLoss: 0.14376232028007507\n",
      "\tLoss: 0.1930762678384781\n",
      "\tLoss: 0.20342105627059937\n",
      "\tLoss: 0.21589967608451843\n",
      "\tLoss: 0.12261834740638733\n",
      "\tLoss: 0.15264509618282318\n",
      "\tLoss: 0.12797904014587402\n",
      "\tLoss: 0.17316994071006775\n",
      "\tLoss: 0.2080279141664505\n",
      "\tLoss: 0.19802381098270416\n",
      "\tLoss: 0.1260152906179428\n",
      "\tLoss: 0.18409481644630432\n",
      "\tLoss: 0.26888638734817505\n",
      "\tLoss: 0.18091268837451935\n",
      "\tLoss: 0.21956954896450043\n",
      "\tLoss: 0.2572924792766571\n",
      "\tLoss: 0.18940645456314087\n",
      "\tLoss: 0.22639602422714233\n",
      "\tLoss: 0.14821051061153412\n",
      "\tLoss: 0.14964532852172852\n",
      "\tLoss: 0.1743726283311844\n",
      "\tLoss: 0.18561390042304993\n",
      "\tLoss: 0.1366589069366455\n",
      "\tLoss: 0.22710204124450684\n",
      "\tLoss: 0.24680563807487488\n",
      "\tLoss: 0.1989428699016571\n",
      "\tLoss: 0.1549137532711029\n",
      "\tLoss: 0.2130189687013626\n",
      "\tLoss: 0.23442310094833374\n",
      "\tLoss: 0.28255316615104675\n",
      "\tLoss: 0.18723033368587494\n",
      "\tLoss: 0.17790456116199493\n",
      "\tLoss: 0.16932161152362823\n",
      "\tLoss: 0.19012539088726044\n",
      "\tLoss: 0.19859620928764343\n",
      "\tLoss: 0.21363797783851624\n",
      "\tLoss: 0.2549419105052948\n",
      "\tLoss: 0.12920978665351868\n",
      "\tLoss: 0.18178221583366394\n",
      "\tLoss: 0.20334282517433167\n",
      "\tLoss: 0.17756284773349762\n",
      "\tLoss: 0.16652053594589233\n",
      "\tLoss: 0.2367457151412964\n",
      "[time] Epoch 1: 483.76561552798375s = 8.06276025879973m\n",
      "\n",
      "Epoch 2...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.18519717454910278\n",
      "\tLoss: 0.21550098061561584\n",
      "\tLoss: 0.258562296628952\n",
      "\tLoss: 0.20726615190505981\n",
      "\tLoss: 0.17982065677642822\n",
      "\tLoss: 0.19122613966464996\n",
      "\tLoss: 0.18892930448055267\n",
      "\tLoss: 0.1963619589805603\n",
      "\tLoss: 0.15907809138298035\n",
      "\tLoss: 0.24846181273460388\n",
      "\tLoss: 0.22036561369895935\n",
      "\tLoss: 0.1799256056547165\n",
      "\tLoss: 0.1693536788225174\n",
      "\tLoss: 0.234140545129776\n",
      "\tLoss: 0.12819933891296387\n",
      "\tLoss: 0.12792837619781494\n",
      "\tLoss: 0.25525033473968506\n",
      "\tLoss: 0.1728464663028717\n",
      "\tLoss: 0.15566007792949677\n",
      "\tLoss: 0.2203938513994217\n",
      "\tLoss: 0.19916702806949615\n",
      "\tLoss: 0.29677024483680725\n",
      "\tLoss: 0.17972177267074585\n",
      "\tLoss: 0.23257513344287872\n",
      "\tLoss: 0.18606458604335785\n",
      "\tLoss: 0.16441503167152405\n",
      "\tLoss: 0.1935487985610962\n",
      "\tLoss: 0.19039833545684814\n",
      "\tLoss: 0.2659747004508972\n",
      "\tLoss: 0.15907208621501923\n",
      "\tLoss: 0.12073579430580139\n",
      "\tLoss: 0.17736107110977173\n",
      "\tLoss: 0.17962303757667542\n",
      "\tLoss: 0.16179142892360687\n",
      "\tLoss: 0.20866383612155914\n",
      "\tLoss: 0.1860693395137787\n",
      "\tLoss: 0.20484086871147156\n",
      "\tLoss: 0.18703649938106537\n",
      "\tLoss: 0.17758077383041382\n",
      "\tLoss: 0.16129867732524872\n",
      "\tLoss: 0.27789223194122314\n",
      "\tLoss: 0.1992102563381195\n",
      "\tLoss: 0.23349356651306152\n",
      "\tLoss: 0.14431914687156677\n",
      "\tLoss: 0.12597814202308655\n",
      "\tLoss: 0.17976197600364685\n",
      "\tLoss: 0.210099995136261\n",
      "\tLoss: 0.15974292159080505\n",
      "\tLoss: 0.19865237176418304\n",
      "\tLoss: 0.20117256045341492\n",
      "\tLoss: 0.1621028631925583\n",
      "\tLoss: 0.10992544889450073\n",
      "\tLoss: 0.22017399966716766\n",
      "\tLoss: 0.22233542799949646\n",
      "\tLoss: 0.1841718554496765\n",
      "\tLoss: 0.1802535057067871\n",
      "\tLoss: 0.1557871401309967\n",
      "\tLoss: 0.1900874376296997\n",
      "\tLoss: 0.15337632596492767\n",
      "\tLoss: 0.19943904876708984\n",
      "\tLoss: 0.18210788071155548\n",
      "\tLoss: 0.20068266987800598\n",
      "\tLoss: 0.14949831366539001\n",
      "\tLoss: 0.15325774252414703\n",
      "\tLoss: 0.19393956661224365\n",
      "\tLoss: 0.2422293871641159\n",
      "\tLoss: 0.18146315217018127\n",
      "\tLoss: 0.14493988454341888\n",
      "\tLoss: 0.14962351322174072\n",
      "\tLoss: 0.19387616217136383\n",
      "\tLoss: 0.20579731464385986\n",
      "\tLoss: 0.2006395161151886\n",
      "\tLoss: 0.13057804107666016\n",
      "\tLoss: 0.2209446132183075\n",
      "\tLoss: 0.17742598056793213\n",
      "\tLoss: 0.11450672894716263\n",
      "\tLoss: 0.19140717387199402\n",
      "\tLoss: 0.2216581106185913\n",
      "\tLoss: 0.22745555639266968\n",
      "\tLoss: 0.21434850990772247\n",
      "\tLoss: 0.1951294094324112\n",
      "\tLoss: 0.19119006395339966\n",
      "\tLoss: 0.11856737732887268\n",
      "\tLoss: 0.16783909499645233\n",
      "\tLoss: 0.23587504029273987\n",
      "\tLoss: 0.2144641876220703\n",
      "\tLoss: 0.14781035482883453\n",
      "\tLoss: 0.19939690828323364\n",
      "\tLoss: 0.20405682921409607\n",
      "\tLoss: 0.1429659128189087\n",
      "\tLoss: 0.19170933961868286\n",
      "\tLoss: 0.1647365838289261\n",
      "\tLoss: 0.18514303863048553\n",
      "\tLoss: 0.11861623823642731\n",
      "\tLoss: 0.12363956868648529\n",
      "\tLoss: 0.17382733523845673\n",
      "\tLoss: 0.1544751226902008\n",
      "\tLoss: 0.2506892681121826\n",
      "\tLoss: 0.1649758219718933\n",
      "\tLoss: 0.21853257715702057\n",
      "\tLoss: 0.1688685417175293\n",
      "\tLoss: 0.13530652225017548\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.15841370820999146\n",
      "\tLoss: 0.1721097230911255\n",
      "\tLoss: 0.20535483956336975\n",
      "\tLoss: 0.13095787167549133\n",
      "\tLoss: 0.1968073695898056\n",
      "\tLoss: 0.1470043659210205\n",
      "\tLoss: 0.22386208176612854\n",
      "\tLoss: 0.15454205870628357\n",
      "\tLoss: 0.14085322618484497\n",
      "\tLoss: 0.13636910915374756\n",
      "\tLoss: 0.13684847950935364\n",
      "\tLoss: 0.16667808592319489\n",
      "\tLoss: 0.10794417560100555\n",
      "\tLoss: 0.09317168593406677\n",
      "\tLoss: 0.12856964766979218\n",
      "\tLoss: 0.18580491840839386\n",
      "\tLoss: 0.18118047714233398\n",
      "\tLoss: 0.15853223204612732\n",
      "\tLoss: 0.22916202247142792\n",
      "\tLoss: 0.14261214435100555\n",
      "\tLoss: 0.1378343403339386\n",
      "\tLoss: 0.1569167971611023\n",
      "\tLoss: 0.089522585272789\n",
      "\tLoss: 0.1635260283946991\n",
      "\tLoss: 0.13011524081230164\n",
      "\tLoss: 0.18353034555912018\n",
      "\tLoss: 0.18659424781799316\n",
      "\tLoss: 0.17653536796569824\n",
      "\tLoss: 0.2194593846797943\n",
      "\tLoss: 0.17142590880393982\n",
      "\tLoss: 0.21775469183921814\n",
      "\tLoss: 0.1674860119819641\n",
      "\tLoss: 0.24124404788017273\n",
      "\tLoss: 0.17051273584365845\n",
      "\tLoss: 0.1742154061794281\n",
      "\tLoss: 0.19363200664520264\n",
      "\tLoss: 0.19525718688964844\n",
      "\tLoss: 0.19882002472877502\n",
      "\tLoss: 0.13044601678848267\n",
      "\tLoss: 0.15705141425132751\n",
      "\tLoss: 0.15127617120742798\n",
      "\tLoss: 0.19918537139892578\n",
      "\tLoss: 0.15599006414413452\n",
      "\tLoss: 0.21223151683807373\n",
      "\tLoss: 0.23305721580982208\n",
      "\tLoss: 0.150501549243927\n",
      "\tLoss: 0.13020524382591248\n",
      "\tLoss: 0.2017887383699417\n",
      "\tLoss: 0.18234093487262726\n",
      "\tLoss: 0.21395136415958405\n",
      "\tLoss: 0.21060776710510254\n",
      "\tLoss: 0.10966582596302032\n",
      "\tLoss: 0.19507315754890442\n",
      "\tLoss: 0.14014753699302673\n",
      "\tLoss: 0.21242718398571014\n",
      "\tLoss: 0.14003074169158936\n",
      "\tLoss: 0.16632087528705597\n",
      "\tLoss: 0.13309010863304138\n",
      "\tLoss: 0.1978292614221573\n",
      "\tLoss: 0.19879262149333954\n",
      "\tLoss: 0.14652350544929504\n",
      "\tLoss: 0.16308476030826569\n",
      "\tLoss: 0.15833774209022522\n",
      "\tLoss: 0.18132877349853516\n",
      "\tLoss: 0.12982183694839478\n",
      "\tLoss: 0.15515244007110596\n",
      "\tLoss: 0.17875182628631592\n",
      "\tLoss: 0.1449039876461029\n",
      "\tLoss: 0.15482807159423828\n",
      "\tLoss: 0.20479516685009003\n",
      "\tLoss: 0.16712318360805511\n",
      "\tLoss: 0.160121351480484\n",
      "\tLoss: 0.1978222131729126\n",
      "\tLoss: 0.1129608005285263\n",
      "\tLoss: 0.18480713665485382\n",
      "\tLoss: 0.12407354265451431\n",
      "\tLoss: 0.20509205758571625\n",
      "\tLoss: 0.18333503603935242\n",
      "\tLoss: 0.23094944655895233\n",
      "\tLoss: 0.22514715790748596\n",
      "\tLoss: 0.14409787952899933\n",
      "\tLoss: 0.17469698190689087\n",
      "\tLoss: 0.21297767758369446\n",
      "\tLoss: 0.13860978186130524\n",
      "\tLoss: 0.19478073716163635\n",
      "\tLoss: 0.13397817313671112\n",
      "\tLoss: 0.2229400873184204\n",
      "\tLoss: 0.24788200855255127\n",
      "\tLoss: 0.13931190967559814\n",
      "\tLoss: 0.13299967348575592\n",
      "\tLoss: 0.1887044459581375\n",
      "\tLoss: 0.1258103847503662\n",
      "\tLoss: 0.1325107365846634\n",
      "\tLoss: 0.21025098860263824\n",
      "\tLoss: 0.1988517940044403\n",
      "\tLoss: 0.1472691297531128\n",
      "\tLoss: 0.1809632033109665\n",
      "\tLoss: 0.1874903440475464\n",
      "\tLoss: 0.1650007963180542\n",
      "\tLoss: 0.10449941456317902\n",
      "\tLoss: 0.1613781452178955\n",
      "\tLoss: 0.27307361364364624\n",
      "[time] Epoch 2: 465.5706479838118s = 7.759510799730196m\n",
      "\n",
      "Epoch 3...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.18010351061820984\n",
      "\tLoss: 0.1485074758529663\n",
      "\tLoss: 0.2553344666957855\n",
      "\tLoss: 0.17616519331932068\n",
      "\tLoss: 0.19755831360816956\n",
      "\tLoss: 0.13428948819637299\n",
      "\tLoss: 0.19782611727714539\n",
      "\tLoss: 0.16537407040596008\n",
      "\tLoss: 0.09296225756406784\n",
      "\tLoss: 0.15622833371162415\n",
      "\tLoss: 0.12410381436347961\n",
      "\tLoss: 0.19064286351203918\n",
      "\tLoss: 0.21168750524520874\n",
      "\tLoss: 0.1616029590368271\n",
      "\tLoss: 0.13217738270759583\n",
      "\tLoss: 0.12135026603937149\n",
      "\tLoss: 0.1610966920852661\n",
      "\tLoss: 0.1852124035358429\n",
      "\tLoss: 0.2328224778175354\n",
      "\tLoss: 0.13738420605659485\n",
      "\tLoss: 0.15760722756385803\n",
      "\tLoss: 0.16330033540725708\n",
      "\tLoss: 0.1565321981906891\n",
      "\tLoss: 0.12786534428596497\n",
      "\tLoss: 0.18761582672595978\n",
      "\tLoss: 0.11972223967313766\n",
      "\tLoss: 0.158847376704216\n",
      "\tLoss: 0.17526623606681824\n",
      "\tLoss: 0.18412253260612488\n",
      "\tLoss: 0.20566371083259583\n",
      "\tLoss: 0.15229889750480652\n",
      "\tLoss: 0.16189734637737274\n",
      "\tLoss: 0.11397586762905121\n",
      "\tLoss: 0.20244334638118744\n",
      "\tLoss: 0.11901967227458954\n",
      "\tLoss: 0.15793322026729584\n",
      "\tLoss: 0.18741413950920105\n",
      "\tLoss: 0.14252808690071106\n",
      "\tLoss: 0.15601813793182373\n",
      "\tLoss: 0.18297860026359558\n",
      "\tLoss: 0.10734528303146362\n",
      "\tLoss: 0.1785830408334732\n",
      "\tLoss: 0.2006130814552307\n",
      "\tLoss: 0.13075780868530273\n",
      "\tLoss: 0.1989232301712036\n",
      "\tLoss: 0.2122574746608734\n",
      "\tLoss: 0.20040050148963928\n",
      "\tLoss: 0.2080388069152832\n",
      "\tLoss: 0.13993722200393677\n",
      "\tLoss: 0.177473783493042\n",
      "\tLoss: 0.17970266938209534\n",
      "\tLoss: 0.15577097237110138\n",
      "\tLoss: 0.1568244993686676\n",
      "\tLoss: 0.1894039362668991\n",
      "\tLoss: 0.21214905381202698\n",
      "\tLoss: 0.1433316469192505\n",
      "\tLoss: 0.15468589961528778\n",
      "\tLoss: 0.22542297840118408\n",
      "\tLoss: 0.24022744596004486\n",
      "\tLoss: 0.17827457189559937\n",
      "\tLoss: 0.2258332073688507\n",
      "\tLoss: 0.10668306052684784\n",
      "\tLoss: 0.20940276980400085\n",
      "\tLoss: 0.11747648566961288\n",
      "\tLoss: 0.22593842446804047\n",
      "\tLoss: 0.15420641005039215\n",
      "\tLoss: 0.21795934438705444\n",
      "\tLoss: 0.13208305835723877\n",
      "\tLoss: 0.17683812975883484\n",
      "\tLoss: 0.19583046436309814\n",
      "\tLoss: 0.1880073845386505\n",
      "\tLoss: 0.16955748200416565\n",
      "\tLoss: 0.08381541073322296\n",
      "\tLoss: 0.15460556745529175\n",
      "\tLoss: 0.17522838711738586\n",
      "\tLoss: 0.15063446760177612\n",
      "\tLoss: 0.1732960343360901\n",
      "\tLoss: 0.2086459994316101\n",
      "\tLoss: 0.1175868883728981\n",
      "\tLoss: 0.21309247612953186\n",
      "\tLoss: 0.14169509708881378\n",
      "\tLoss: 0.12426339834928513\n",
      "\tLoss: 0.13435383141040802\n",
      "\tLoss: 0.14892174303531647\n",
      "\tLoss: 0.1521570384502411\n",
      "\tLoss: 0.20312882959842682\n",
      "\tLoss: 0.14335203170776367\n",
      "\tLoss: 0.1534862369298935\n",
      "\tLoss: 0.17200177907943726\n",
      "\tLoss: 0.16931907832622528\n",
      "\tLoss: 0.13805446028709412\n",
      "\tLoss: 0.1130577027797699\n",
      "\tLoss: 0.17381063103675842\n",
      "\tLoss: 0.21796420216560364\n",
      "\tLoss: 0.17000806331634521\n",
      "\tLoss: 0.13401956856250763\n",
      "\tLoss: 0.15256693959236145\n",
      "\tLoss: 0.16430610418319702\n",
      "\tLoss: 0.16700494289398193\n",
      "\tLoss: 0.13425883650779724\n",
      "\tLoss: 0.15343451499938965\n",
      "\tLoss: 0.13740888237953186\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.1694219410419464\n",
      "\tLoss: 0.1638311743736267\n",
      "\tLoss: 0.17478513717651367\n",
      "\tLoss: 0.1836608350276947\n",
      "\tLoss: 0.12207706272602081\n",
      "\tLoss: 0.176431804895401\n",
      "\tLoss: 0.17400985956192017\n",
      "\tLoss: 0.20843049883842468\n",
      "\tLoss: 0.1979438066482544\n",
      "\tLoss: 0.15462139248847961\n",
      "\tLoss: 0.14510303735733032\n",
      "\tLoss: 0.1588929295539856\n",
      "\tLoss: 0.1312749981880188\n",
      "\tLoss: 0.1362263709306717\n",
      "\tLoss: 0.1628650426864624\n",
      "\tLoss: 0.1424054503440857\n",
      "\tLoss: 0.09520647674798965\n",
      "\tLoss: 0.13379892706871033\n",
      "\tLoss: 0.1828911006450653\n",
      "\tLoss: 0.14346818625926971\n",
      "\tLoss: 0.17194905877113342\n",
      "\tLoss: 0.18012367188930511\n",
      "\tLoss: 0.1983693242073059\n",
      "\tLoss: 0.14983516931533813\n",
      "\tLoss: 0.12567037343978882\n",
      "\tLoss: 0.230649933218956\n",
      "\tLoss: 0.11827068030834198\n",
      "\tLoss: 0.17506086826324463\n",
      "\tLoss: 0.20029044151306152\n",
      "\tLoss: 0.11549555510282516\n",
      "\tLoss: 0.10665497928857803\n",
      "\tLoss: 0.17046129703521729\n",
      "\tLoss: 0.18055294454097748\n",
      "\tLoss: 0.10187356173992157\n",
      "\tLoss: 0.18736356496810913\n",
      "\tLoss: 0.15973041951656342\n",
      "\tLoss: 0.11710979044437408\n",
      "\tLoss: 0.2645575702190399\n",
      "\tLoss: 0.14517785608768463\n",
      "\tLoss: 0.19048169255256653\n",
      "\tLoss: 0.08479408174753189\n",
      "\tLoss: 0.12641730904579163\n",
      "\tLoss: 0.15861564874649048\n",
      "\tLoss: 0.10529331862926483\n",
      "\tLoss: 0.15527750551700592\n",
      "\tLoss: 0.12733864784240723\n",
      "\tLoss: 0.1481914073228836\n",
      "\tLoss: 0.18394653499126434\n",
      "\tLoss: 0.1687278151512146\n",
      "\tLoss: 0.1434827297925949\n",
      "\tLoss: 0.21213659644126892\n",
      "\tLoss: 0.17998133599758148\n",
      "\tLoss: 0.15322518348693848\n",
      "\tLoss: 0.17028376460075378\n",
      "\tLoss: 0.16216614842414856\n",
      "\tLoss: 0.15984930098056793\n",
      "\tLoss: 0.15268224477767944\n",
      "\tLoss: 0.1596878170967102\n",
      "\tLoss: 0.16120243072509766\n",
      "\tLoss: 0.15668116509914398\n",
      "\tLoss: 0.20231863856315613\n",
      "\tLoss: 0.1180310994386673\n",
      "\tLoss: 0.18702183663845062\n",
      "\tLoss: 0.14005757868289948\n",
      "\tLoss: 0.10448858886957169\n",
      "\tLoss: 0.09598483145236969\n",
      "\tLoss: 0.07637131959199905\n",
      "\tLoss: 0.17238807678222656\n",
      "\tLoss: 0.08217184245586395\n",
      "\tLoss: 0.14704859256744385\n",
      "\tLoss: 0.12789283692836761\n",
      "\tLoss: 0.11782227456569672\n",
      "\tLoss: 0.22703960537910461\n",
      "\tLoss: 0.1377873569726944\n",
      "\tLoss: 0.1530478596687317\n",
      "\tLoss: 0.1748170703649521\n",
      "\tLoss: 0.17577429115772247\n",
      "\tLoss: 0.1850486397743225\n",
      "\tLoss: 0.0944746807217598\n",
      "\tLoss: 0.17076103389263153\n",
      "\tLoss: 0.19737806916236877\n",
      "\tLoss: 0.2285277396440506\n",
      "\tLoss: 0.1921825110912323\n",
      "\tLoss: 0.17316749691963196\n",
      "\tLoss: 0.127193883061409\n",
      "\tLoss: 0.18564462661743164\n",
      "\tLoss: 0.17213259637355804\n",
      "\tLoss: 0.09188979119062424\n",
      "\tLoss: 0.1373431533575058\n",
      "\tLoss: 0.24241486191749573\n",
      "\tLoss: 0.11810024827718735\n",
      "\tLoss: 0.12279601395130157\n",
      "\tLoss: 0.10991282761096954\n",
      "\tLoss: 0.15208563208580017\n",
      "\tLoss: 0.1583397537469864\n",
      "\tLoss: 0.18699918687343597\n",
      "\tLoss: 0.17456620931625366\n",
      "\tLoss: 0.11152272671461105\n",
      "\tLoss: 0.2338227480649948\n",
      "\tLoss: 0.18459728360176086\n",
      "\tLoss: 0.2147216647863388\n",
      "\tLoss: 0.1576637625694275\n",
      "[time] Epoch 3: 460.90221513668075s = 7.681703585611346m\n",
      "\n",
      "Epoch 4...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.13000112771987915\n",
      "\tLoss: 0.16013848781585693\n",
      "\tLoss: 0.14744311571121216\n",
      "\tLoss: 0.14268799126148224\n",
      "\tLoss: 0.20282995700836182\n",
      "\tLoss: 0.16977757215499878\n",
      "\tLoss: 0.16213198006153107\n",
      "\tLoss: 0.1646081954240799\n",
      "\tLoss: 0.14742664992809296\n",
      "\tLoss: 0.12196330726146698\n",
      "\tLoss: 0.15396299958229065\n",
      "\tLoss: 0.12070789188146591\n",
      "\tLoss: 0.2185598611831665\n",
      "\tLoss: 0.11773352324962616\n",
      "\tLoss: 0.16451935470104218\n",
      "\tLoss: 0.1555567979812622\n",
      "\tLoss: 0.16325980424880981\n",
      "\tLoss: 0.1185157522559166\n",
      "\tLoss: 0.11373264342546463\n",
      "\tLoss: 0.1819656789302826\n",
      "\tLoss: 0.14860375225543976\n",
      "\tLoss: 0.15686866641044617\n",
      "\tLoss: 0.16526645421981812\n",
      "\tLoss: 0.1992027759552002\n",
      "\tLoss: 0.22030803561210632\n",
      "\tLoss: 0.20463451743125916\n",
      "\tLoss: 0.14146554470062256\n",
      "\tLoss: 0.13843706250190735\n",
      "\tLoss: 0.1965097039937973\n",
      "\tLoss: 0.1469939798116684\n",
      "\tLoss: 0.18486905097961426\n",
      "\tLoss: 0.1417067050933838\n",
      "\tLoss: 0.1922932267189026\n",
      "\tLoss: 0.09623737633228302\n",
      "\tLoss: 0.17649872601032257\n",
      "\tLoss: 0.09237946569919586\n",
      "\tLoss: 0.14254271984100342\n",
      "\tLoss: 0.2660047709941864\n",
      "\tLoss: 0.2282906472682953\n",
      "\tLoss: 0.16201108694076538\n",
      "\tLoss: 0.15767472982406616\n",
      "\tLoss: 0.1505586802959442\n",
      "\tLoss: 0.1106269434094429\n",
      "\tLoss: 0.17882150411605835\n",
      "\tLoss: 0.22785203158855438\n",
      "\tLoss: 0.12130456417798996\n",
      "\tLoss: 0.20651617646217346\n",
      "\tLoss: 0.14598771929740906\n",
      "\tLoss: 0.19724631309509277\n",
      "\tLoss: 0.2546907067298889\n",
      "\tLoss: 0.1194375604391098\n",
      "\tLoss: 0.12517443299293518\n",
      "\tLoss: 0.20042914152145386\n",
      "\tLoss: 0.11857350915670395\n",
      "\tLoss: 0.1245933473110199\n",
      "\tLoss: 0.18333971500396729\n",
      "\tLoss: 0.22975584864616394\n",
      "\tLoss: 0.13111524283885956\n",
      "\tLoss: 0.09356251358985901\n",
      "\tLoss: 0.08516254276037216\n",
      "\tLoss: 0.11150390654802322\n",
      "\tLoss: 0.18789395689964294\n",
      "\tLoss: 0.10632945597171783\n",
      "\tLoss: 0.18016645312309265\n",
      "\tLoss: 0.16290050745010376\n",
      "\tLoss: 0.14122670888900757\n",
      "\tLoss: 0.2010151445865631\n",
      "\tLoss: 0.21220412850379944\n",
      "\tLoss: 0.11837130784988403\n",
      "\tLoss: 0.12821629643440247\n",
      "\tLoss: 0.10588368773460388\n",
      "\tLoss: 0.16380611062049866\n",
      "\tLoss: 0.1263372004032135\n",
      "\tLoss: 0.08139845728874207\n",
      "\tLoss: 0.182870015501976\n",
      "\tLoss: 0.1242326945066452\n",
      "\tLoss: 0.17389720678329468\n",
      "\tLoss: 0.13391539454460144\n",
      "\tLoss: 0.135809063911438\n",
      "\tLoss: 0.16985419392585754\n",
      "\tLoss: 0.13338524103164673\n",
      "\tLoss: 0.1733027994632721\n",
      "\tLoss: 0.15731024742126465\n",
      "\tLoss: 0.12611475586891174\n",
      "\tLoss: 0.11741945147514343\n",
      "\tLoss: 0.16893349587917328\n",
      "\tLoss: 0.12267964333295822\n",
      "\tLoss: 0.13681700825691223\n",
      "\tLoss: 0.13849866390228271\n",
      "\tLoss: 0.10453955829143524\n",
      "\tLoss: 0.16567306220531464\n",
      "\tLoss: 0.16757895052433014\n",
      "\tLoss: 0.11399764567613602\n",
      "\tLoss: 0.1868416666984558\n",
      "\tLoss: 0.21613533794879913\n",
      "\tLoss: 0.14798295497894287\n",
      "\tLoss: 0.18244163691997528\n",
      "\tLoss: 0.20885394513607025\n",
      "\tLoss: 0.13743285834789276\n",
      "\tLoss: 0.1595287322998047\n",
      "\tLoss: 0.16751635074615479\n",
      "\tLoss: 0.15097466111183167\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.12679041922092438\n",
      "\tLoss: 0.13478411734104156\n",
      "\tLoss: 0.18664607405662537\n",
      "\tLoss: 0.2162337452173233\n",
      "\tLoss: 0.13855032622814178\n",
      "\tLoss: 0.10592114925384521\n",
      "\tLoss: 0.15347090363502502\n",
      "\tLoss: 0.11978577077388763\n",
      "\tLoss: 0.09543167054653168\n",
      "\tLoss: 0.18170146644115448\n",
      "\tLoss: 0.17820671200752258\n",
      "\tLoss: 0.14611570537090302\n",
      "\tLoss: 0.14317116141319275\n",
      "\tLoss: 0.10759411007165909\n",
      "\tLoss: 0.14802169799804688\n",
      "\tLoss: 0.13797828555107117\n",
      "\tLoss: 0.1367960423231125\n",
      "\tLoss: 0.11704825609922409\n",
      "\tLoss: 0.147459477186203\n",
      "\tLoss: 0.14247484505176544\n",
      "\tLoss: 0.13893286883831024\n",
      "\tLoss: 0.11309946328401566\n",
      "\tLoss: 0.15564502775669098\n",
      "\tLoss: 0.17378470301628113\n",
      "\tLoss: 0.18654057383537292\n",
      "\tLoss: 0.12887787818908691\n",
      "\tLoss: 0.20086127519607544\n",
      "\tLoss: 0.09962819516658783\n",
      "\tLoss: 0.20789970457553864\n",
      "\tLoss: 0.13853004574775696\n",
      "\tLoss: 0.0801285058259964\n",
      "\tLoss: 0.11622725427150726\n",
      "\tLoss: 0.18651226162910461\n",
      "\tLoss: 0.16378381848335266\n",
      "\tLoss: 0.12733644247055054\n",
      "\tLoss: 0.13350749015808105\n",
      "\tLoss: 0.26280927658081055\n",
      "\tLoss: 0.167706698179245\n",
      "\tLoss: 0.17002734541893005\n",
      "\tLoss: 0.09381382167339325\n",
      "\tLoss: 0.16664311289787292\n",
      "\tLoss: 0.10112281143665314\n",
      "\tLoss: 0.13356485962867737\n",
      "\tLoss: 0.17504748702049255\n",
      "\tLoss: 0.1266377568244934\n",
      "\tLoss: 0.12617895007133484\n",
      "\tLoss: 0.17781148850917816\n",
      "\tLoss: 0.1709858924150467\n",
      "\tLoss: 0.1307705193758011\n",
      "\tLoss: 0.1446392685174942\n",
      "\tLoss: 0.13411149382591248\n",
      "\tLoss: 0.17243774235248566\n",
      "\tLoss: 0.11478278040885925\n",
      "\tLoss: 0.1264047473669052\n",
      "\tLoss: 0.11986830830574036\n",
      "\tLoss: 0.18332020938396454\n",
      "\tLoss: 0.07947668433189392\n",
      "\tLoss: 0.17132902145385742\n",
      "\tLoss: 0.1549285650253296\n",
      "\tLoss: 0.19385576248168945\n",
      "\tLoss: 0.18400131165981293\n",
      "\tLoss: 0.1481899917125702\n",
      "\tLoss: 0.1246589720249176\n",
      "\tLoss: 0.18883070349693298\n",
      "\tLoss: 0.139645516872406\n",
      "\tLoss: 0.1645282655954361\n",
      "\tLoss: 0.13255247473716736\n",
      "\tLoss: 0.1264231652021408\n",
      "\tLoss: 0.14555491507053375\n",
      "\tLoss: 0.14161019027233124\n",
      "\tLoss: 0.18845318257808685\n",
      "\tLoss: 0.15303388237953186\n",
      "\tLoss: 0.13846948742866516\n",
      "\tLoss: 0.10220098495483398\n",
      "\tLoss: 0.13007000088691711\n",
      "\tLoss: 0.12388376891613007\n",
      "\tLoss: 0.19940973818302155\n",
      "\tLoss: 0.14448118209838867\n",
      "\tLoss: 0.1060272753238678\n",
      "\tLoss: 0.12086256593465805\n",
      "\tLoss: 0.18353313207626343\n",
      "\tLoss: 0.15804700553417206\n",
      "\tLoss: 0.12939271330833435\n",
      "\tLoss: 0.16843050718307495\n",
      "\tLoss: 0.1363489329814911\n",
      "\tLoss: 0.18515190482139587\n",
      "\tLoss: 0.13289840519428253\n",
      "\tLoss: 0.14660537242889404\n",
      "\tLoss: 0.17566800117492676\n",
      "\tLoss: 0.15668800473213196\n",
      "\tLoss: 0.13862058520317078\n",
      "\tLoss: 0.18161939084529877\n",
      "\tLoss: 0.10494986176490784\n",
      "\tLoss: 0.12313192337751389\n",
      "\tLoss: 0.138302743434906\n",
      "\tLoss: 0.1517387181520462\n",
      "\tLoss: 0.10875070840120316\n",
      "\tLoss: 0.21759098768234253\n",
      "\tLoss: 0.12021088600158691\n",
      "\tLoss: 0.17749425768852234\n",
      "\tLoss: 0.08759166300296783\n",
      "\tLoss: 0.17481018602848053\n",
      "[time] Epoch 4: 458.1499689081684s = 7.63583281513614m\n",
      "\n",
      "Epoch 5...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.13654646277427673\n",
      "\tLoss: 0.08317460864782333\n",
      "\tLoss: 0.16453716158866882\n",
      "\tLoss: 0.09358301758766174\n",
      "\tLoss: 0.130899116396904\n",
      "\tLoss: 0.1371404528617859\n",
      "\tLoss: 0.19169285893440247\n",
      "\tLoss: 0.13693922758102417\n",
      "\tLoss: 0.1395171582698822\n",
      "\tLoss: 0.1760900914669037\n",
      "\tLoss: 0.09803733229637146\n",
      "\tLoss: 0.1652488112449646\n",
      "\tLoss: 0.10234081745147705\n",
      "\tLoss: 0.12304649502038956\n",
      "\tLoss: 0.15697883069515228\n",
      "\tLoss: 0.1463254988193512\n",
      "\tLoss: 0.12935352325439453\n",
      "\tLoss: 0.13108444213867188\n",
      "\tLoss: 0.10818763077259064\n",
      "\tLoss: 0.155685693025589\n",
      "\tLoss: 0.11696624755859375\n",
      "\tLoss: 0.14870017766952515\n",
      "\tLoss: 0.14481566846370697\n",
      "\tLoss: 0.13639505207538605\n",
      "\tLoss: 0.12374122440814972\n",
      "\tLoss: 0.18516114354133606\n",
      "\tLoss: 0.21088388562202454\n",
      "\tLoss: 0.09100477397441864\n",
      "\tLoss: 0.1427760124206543\n",
      "\tLoss: 0.209196537733078\n",
      "\tLoss: 0.14342857897281647\n",
      "\tLoss: 0.08253872394561768\n",
      "\tLoss: 0.14376339316368103\n",
      "\tLoss: 0.1347493827342987\n",
      "\tLoss: 0.18359780311584473\n",
      "\tLoss: 0.12929964065551758\n",
      "\tLoss: 0.11072147637605667\n",
      "\tLoss: 0.2058107554912567\n",
      "\tLoss: 0.20688393712043762\n",
      "\tLoss: 0.11962532997131348\n",
      "\tLoss: 0.17894767224788666\n",
      "\tLoss: 0.15674608945846558\n",
      "\tLoss: 0.19057723879814148\n",
      "\tLoss: 0.1654735803604126\n",
      "\tLoss: 0.17563983798027039\n",
      "\tLoss: 0.16711238026618958\n",
      "\tLoss: 0.15747740864753723\n",
      "\tLoss: 0.1228381097316742\n",
      "\tLoss: 0.09640081226825714\n",
      "\tLoss: 0.196669340133667\n",
      "\tLoss: 0.09100693464279175\n",
      "\tLoss: 0.09722253680229187\n",
      "\tLoss: 0.13981565833091736\n",
      "\tLoss: 0.15878689289093018\n",
      "\tLoss: 0.17831921577453613\n",
      "\tLoss: 0.10109938681125641\n",
      "\tLoss: 0.12865646183490753\n",
      "\tLoss: 0.07634945213794708\n",
      "\tLoss: 0.14390258491039276\n",
      "\tLoss: 0.08272913098335266\n",
      "\tLoss: 0.14730733633041382\n",
      "\tLoss: 0.1481173038482666\n",
      "\tLoss: 0.14851467311382294\n",
      "\tLoss: 0.18636980652809143\n",
      "\tLoss: 0.1783071756362915\n",
      "\tLoss: 0.14944642782211304\n",
      "\tLoss: 0.11587348580360413\n",
      "\tLoss: 0.10433335602283478\n",
      "\tLoss: 0.2103775590658188\n",
      "\tLoss: 0.10272101312875748\n",
      "\tLoss: 0.21955956518650055\n",
      "\tLoss: 0.141853466629982\n",
      "\tLoss: 0.11390405148267746\n",
      "\tLoss: 0.14340433478355408\n",
      "\tLoss: 0.14386484026908875\n",
      "\tLoss: 0.17074516415596008\n",
      "\tLoss: 0.1593015193939209\n",
      "\tLoss: 0.0991029143333435\n",
      "\tLoss: 0.11337660253047943\n",
      "\tLoss: 0.10885339230298996\n",
      "\tLoss: 0.07505804300308228\n",
      "\tLoss: 0.13248905539512634\n",
      "\tLoss: 0.13420070707798004\n",
      "\tLoss: 0.1798483431339264\n",
      "\tLoss: 0.10383526980876923\n",
      "\tLoss: 0.2005002498626709\n",
      "\tLoss: 0.07495038956403732\n",
      "\tLoss: 0.13865935802459717\n",
      "\tLoss: 0.12297601252794266\n",
      "\tLoss: 0.16402551531791687\n",
      "\tLoss: 0.16765370965003967\n",
      "\tLoss: 0.16366273164749146\n",
      "\tLoss: 0.14648938179016113\n",
      "\tLoss: 0.1748323142528534\n",
      "\tLoss: 0.132937952876091\n",
      "\tLoss: 0.11572238802909851\n",
      "\tLoss: 0.1378529965877533\n",
      "\tLoss: 0.21721100807189941\n",
      "\tLoss: 0.12364129722118378\n",
      "\tLoss: 0.19265811145305634\n",
      "\tLoss: 0.19040386378765106\n",
      "\tLoss: 0.12536630034446716\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.17296253144741058\n",
      "\tLoss: 0.19877345860004425\n",
      "\tLoss: 0.11163622885942459\n",
      "\tLoss: 0.19895844161510468\n",
      "\tLoss: 0.16569356620311737\n",
      "\tLoss: 0.13035652041435242\n",
      "\tLoss: 0.25483423471450806\n",
      "\tLoss: 0.11942370235919952\n",
      "\tLoss: 0.11615786701440811\n",
      "\tLoss: 0.18065443634986877\n",
      "\tLoss: 0.12391425669193268\n",
      "\tLoss: 0.08359430730342865\n",
      "\tLoss: 0.10266546905040741\n",
      "\tLoss: 0.0905844122171402\n",
      "\tLoss: 0.15634256601333618\n",
      "\tLoss: 0.1879933774471283\n",
      "\tLoss: 0.10368862003087997\n",
      "\tLoss: 0.15682700276374817\n",
      "\tLoss: 0.1846083402633667\n",
      "\tLoss: 0.12718436121940613\n",
      "\tLoss: 0.15494656562805176\n",
      "\tLoss: 0.17834308743476868\n",
      "\tLoss: 0.16396048665046692\n",
      "\tLoss: 0.19674980640411377\n",
      "\tLoss: 0.10426962375640869\n",
      "\tLoss: 0.1511061191558838\n",
      "\tLoss: 0.17493268847465515\n",
      "\tLoss: 0.14644116163253784\n",
      "\tLoss: 0.1788157820701599\n",
      "\tLoss: 0.20696589350700378\n",
      "\tLoss: 0.08157218992710114\n",
      "\tLoss: 0.11104638874530792\n",
      "\tLoss: 0.10329455137252808\n",
      "\tLoss: 0.13786238431930542\n",
      "\tLoss: 0.12024202197790146\n",
      "\tLoss: 0.17258349061012268\n",
      "\tLoss: 0.1351262927055359\n",
      "\tLoss: 0.0995793417096138\n",
      "\tLoss: 0.12131496518850327\n",
      "\tLoss: 0.1283915936946869\n",
      "\tLoss: 0.128090500831604\n",
      "\tLoss: 0.1520734429359436\n",
      "\tLoss: 0.12502560019493103\n",
      "\tLoss: 0.15214328467845917\n",
      "\tLoss: 0.12673595547676086\n",
      "\tLoss: 0.1597222089767456\n",
      "\tLoss: 0.13684527575969696\n",
      "\tLoss: 0.17927083373069763\n",
      "\tLoss: 0.0993586927652359\n",
      "\tLoss: 0.15367113053798676\n",
      "\tLoss: 0.15658967196941376\n",
      "\tLoss: 0.12182221561670303\n",
      "\tLoss: 0.09341780841350555\n",
      "\tLoss: 0.19039003551006317\n",
      "\tLoss: 0.14450642466545105\n",
      "\tLoss: 0.15885290503501892\n",
      "\tLoss: 0.16899502277374268\n",
      "\tLoss: 0.1363600790500641\n",
      "\tLoss: 0.17901688814163208\n",
      "\tLoss: 0.11408182978630066\n",
      "\tLoss: 0.18537014722824097\n",
      "\tLoss: 0.13768815994262695\n",
      "\tLoss: 0.1739710569381714\n",
      "\tLoss: 0.14994123578071594\n",
      "\tLoss: 0.09935203194618225\n",
      "\tLoss: 0.12050910294055939\n",
      "\tLoss: 0.12760087847709656\n",
      "\tLoss: 0.12361067533493042\n",
      "\tLoss: 0.18725420534610748\n",
      "\tLoss: 0.18413342535495758\n",
      "\tLoss: 0.1430610865354538\n",
      "\tLoss: 0.2136368304491043\n",
      "\tLoss: 0.1068137139081955\n",
      "\tLoss: 0.09230154752731323\n",
      "\tLoss: 0.11665496230125427\n",
      "\tLoss: 0.10118351131677628\n",
      "\tLoss: 0.16676686704158783\n",
      "\tLoss: 0.1833319365978241\n",
      "\tLoss: 0.18124909698963165\n",
      "\tLoss: 0.09989854693412781\n",
      "\tLoss: 0.09226595610380173\n",
      "\tLoss: 0.12446440756320953\n",
      "\tLoss: 0.13162490725517273\n",
      "\tLoss: 0.12544488906860352\n",
      "\tLoss: 0.1342511773109436\n",
      "\tLoss: 0.16609305143356323\n",
      "\tLoss: 0.13964290916919708\n",
      "\tLoss: 0.1293487548828125\n",
      "\tLoss: 0.1530068814754486\n",
      "\tLoss: 0.21614542603492737\n",
      "\tLoss: 0.14555183053016663\n",
      "\tLoss: 0.1416824609041214\n",
      "\tLoss: 0.1793573945760727\n",
      "\tLoss: 0.11758732050657272\n",
      "\tLoss: 0.11897924542427063\n",
      "\tLoss: 0.06884641200304031\n",
      "\tLoss: 0.1226421445608139\n",
      "\tLoss: 0.14516448974609375\n",
      "\tLoss: 0.16229456663131714\n",
      "\tLoss: 0.16341933608055115\n",
      "\tLoss: 0.25012362003326416\n",
      "\tLoss: 0.16714398562908173\n",
      "[time] Epoch 5: 457.734304448124s = 7.6289050741354005m\n",
      "\n",
      "Epoch 6...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.14605146646499634\n",
      "\tLoss: 0.2182427942752838\n",
      "\tLoss: 0.13263937830924988\n",
      "\tLoss: 0.21062946319580078\n",
      "\tLoss: 0.17398382723331451\n",
      "\tLoss: 0.10091532766819\n",
      "\tLoss: 0.17477457225322723\n",
      "\tLoss: 0.07678103446960449\n",
      "\tLoss: 0.12874971330165863\n",
      "\tLoss: 0.1490597128868103\n",
      "\tLoss: 0.12089865654706955\n",
      "\tLoss: 0.14853310585021973\n",
      "\tLoss: 0.1702013909816742\n",
      "\tLoss: 0.09348161518573761\n",
      "\tLoss: 0.20810699462890625\n",
      "\tLoss: 0.20373839139938354\n",
      "\tLoss: 0.1092032939195633\n",
      "\tLoss: 0.2520299553871155\n",
      "\tLoss: 0.18799728155136108\n",
      "\tLoss: 0.10989128053188324\n",
      "\tLoss: 0.13703972101211548\n",
      "\tLoss: 0.08289770781993866\n",
      "\tLoss: 0.10475568473339081\n",
      "\tLoss: 0.1587989777326584\n",
      "\tLoss: 0.18097874522209167\n",
      "\tLoss: 0.21638327836990356\n",
      "\tLoss: 0.14414149522781372\n",
      "\tLoss: 0.14345724880695343\n",
      "\tLoss: 0.14308121800422668\n",
      "\tLoss: 0.17383317649364471\n",
      "\tLoss: 0.21871984004974365\n",
      "\tLoss: 0.16788242757320404\n",
      "\tLoss: 0.10097411274909973\n",
      "\tLoss: 0.1855657547712326\n",
      "\tLoss: 0.1218625158071518\n",
      "\tLoss: 0.20351094007492065\n",
      "\tLoss: 0.16552084684371948\n",
      "\tLoss: 0.19279974699020386\n",
      "\tLoss: 0.14746198058128357\n",
      "\tLoss: 0.139223113656044\n",
      "\tLoss: 0.1223488599061966\n",
      "\tLoss: 0.11265089362859726\n",
      "\tLoss: 0.12050456553697586\n",
      "\tLoss: 0.11242306977510452\n",
      "\tLoss: 0.18260839581489563\n",
      "\tLoss: 0.09886990487575531\n",
      "\tLoss: 0.07620501518249512\n",
      "\tLoss: 0.1203857958316803\n",
      "\tLoss: 0.17826634645462036\n",
      "\tLoss: 0.14175133407115936\n",
      "\tLoss: 0.16639496386051178\n",
      "\tLoss: 0.17781490087509155\n",
      "\tLoss: 0.12837764620780945\n",
      "\tLoss: 0.1240265816450119\n",
      "\tLoss: 0.10231296718120575\n",
      "\tLoss: 0.13180087506771088\n",
      "\tLoss: 0.13858555257320404\n",
      "\tLoss: 0.17443260550498962\n",
      "\tLoss: 0.1846599578857422\n",
      "\tLoss: 0.12876012921333313\n",
      "\tLoss: 0.14973612129688263\n",
      "\tLoss: 0.13399755954742432\n",
      "\tLoss: 0.12253787368535995\n",
      "\tLoss: 0.13395708799362183\n",
      "\tLoss: 0.16418230533599854\n",
      "\tLoss: 0.13004857301712036\n",
      "\tLoss: 0.16782282292842865\n",
      "\tLoss: 0.19454193115234375\n",
      "\tLoss: 0.122523233294487\n",
      "\tLoss: 0.18057996034622192\n",
      "\tLoss: 0.13681688904762268\n",
      "\tLoss: 0.12044627219438553\n",
      "\tLoss: 0.13958099484443665\n",
      "\tLoss: 0.16720205545425415\n",
      "\tLoss: 0.1121249571442604\n",
      "\tLoss: 0.07465674728155136\n",
      "\tLoss: 0.10747715830802917\n",
      "\tLoss: 0.16725188493728638\n",
      "\tLoss: 0.09027919918298721\n",
      "\tLoss: 0.17828381061553955\n",
      "\tLoss: 0.17499715089797974\n",
      "\tLoss: 0.1591460406780243\n",
      "\tLoss: 0.13031207025051117\n",
      "\tLoss: 0.1279822736978531\n",
      "\tLoss: 0.08113764226436615\n",
      "\tLoss: 0.142250657081604\n",
      "\tLoss: 0.14925502240657806\n",
      "\tLoss: 0.13038642704486847\n",
      "\tLoss: 0.1865566521883011\n",
      "\tLoss: 0.08571381866931915\n",
      "\tLoss: 0.14406709372997284\n",
      "\tLoss: 0.1348390281200409\n",
      "\tLoss: 0.09542122483253479\n",
      "\tLoss: 0.196836918592453\n",
      "\tLoss: 0.10321690142154694\n",
      "\tLoss: 0.12292295694351196\n",
      "\tLoss: 0.0968426913022995\n",
      "\tLoss: 0.12445959448814392\n",
      "\tLoss: 0.16173680126667023\n",
      "\tLoss: 0.17753095924854279\n",
      "\tLoss: 0.0915268287062645\n",
      "\tLoss: 0.1696522831916809\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.09155705571174622\n",
      "\tLoss: 0.13291597366333008\n",
      "\tLoss: 0.22140851616859436\n",
      "\tLoss: 0.17320947349071503\n",
      "\tLoss: 0.17121073603630066\n",
      "\tLoss: 0.1717851608991623\n",
      "\tLoss: 0.1373746395111084\n",
      "\tLoss: 0.08366821706295013\n",
      "\tLoss: 0.1064307913184166\n",
      "\tLoss: 0.1260518729686737\n",
      "\tLoss: 0.16992181539535522\n",
      "\tLoss: 0.14064350724220276\n",
      "\tLoss: 0.12809324264526367\n",
      "\tLoss: 0.08334615081548691\n",
      "\tLoss: 0.12017612159252167\n",
      "\tLoss: 0.12334687262773514\n",
      "\tLoss: 0.11551904678344727\n",
      "\tLoss: 0.11226095259189606\n",
      "\tLoss: 0.10449256747961044\n",
      "\tLoss: 0.1684863269329071\n",
      "\tLoss: 0.15046675503253937\n",
      "\tLoss: 0.1863766461610794\n",
      "\tLoss: 0.10313338041305542\n",
      "\tLoss: 0.08834539353847504\n",
      "\tLoss: 0.1365734338760376\n",
      "\tLoss: 0.15808452665805817\n",
      "\tLoss: 0.13808846473693848\n",
      "\tLoss: 0.12083599716424942\n",
      "\tLoss: 0.11723912507295609\n",
      "\tLoss: 0.08802755922079086\n",
      "\tLoss: 0.1346893310546875\n",
      "\tLoss: 0.1442531943321228\n",
      "\tLoss: 0.14784064888954163\n",
      "\tLoss: 0.16299116611480713\n",
      "\tLoss: 0.16231584548950195\n",
      "\tLoss: 0.11058156937360764\n",
      "\tLoss: 0.19850455224514008\n",
      "\tLoss: 0.18745052814483643\n",
      "\tLoss: 0.10516388714313507\n",
      "\tLoss: 0.22608625888824463\n",
      "\tLoss: 0.10679762810468674\n",
      "\tLoss: 0.12275969982147217\n",
      "\tLoss: 0.15127745270729065\n",
      "\tLoss: 0.10481389611959457\n",
      "\tLoss: 0.1711750030517578\n",
      "\tLoss: 0.13177341222763062\n",
      "\tLoss: 0.08271290361881256\n",
      "\tLoss: 0.15834495425224304\n",
      "\tLoss: 0.17517930269241333\n",
      "\tLoss: 0.1646672785282135\n",
      "\tLoss: 0.12382470816373825\n",
      "\tLoss: 0.14791342616081238\n",
      "\tLoss: 0.14608445763587952\n",
      "\tLoss: 0.20502761006355286\n",
      "\tLoss: 0.1863323450088501\n",
      "\tLoss: 0.11639111489057541\n",
      "\tLoss: 0.26610836386680603\n",
      "\tLoss: 0.17740094661712646\n",
      "\tLoss: 0.16626448929309845\n",
      "\tLoss: 0.16697297990322113\n",
      "\tLoss: 0.13142473995685577\n",
      "\tLoss: 0.17913353443145752\n",
      "\tLoss: 0.17268146574497223\n",
      "\tLoss: 0.14855054020881653\n",
      "\tLoss: 0.20857201516628265\n",
      "\tLoss: 0.16811498999595642\n",
      "\tLoss: 0.16572213172912598\n",
      "\tLoss: 0.21004393696784973\n",
      "\tLoss: 0.14288359880447388\n",
      "\tLoss: 0.20746633410453796\n",
      "\tLoss: 0.19813956320285797\n",
      "\tLoss: 0.21152156591415405\n",
      "\tLoss: 0.1500963270664215\n",
      "\tLoss: 0.16982127726078033\n",
      "\tLoss: 0.12171143293380737\n",
      "\tLoss: 0.08752807974815369\n",
      "\tLoss: 0.1192549467086792\n",
      "\tLoss: 0.1271868497133255\n",
      "\tLoss: 0.20667052268981934\n",
      "\tLoss: 0.09185555577278137\n",
      "\tLoss: 0.16850504279136658\n",
      "\tLoss: 0.16434907913208008\n",
      "\tLoss: 0.19013865292072296\n",
      "\tLoss: 0.12629631161689758\n",
      "\tLoss: 0.175016388297081\n",
      "\tLoss: 0.1789856255054474\n",
      "\tLoss: 0.15410616993904114\n",
      "\tLoss: 0.1982765793800354\n",
      "\tLoss: 0.09950364381074905\n",
      "\tLoss: 0.12551328539848328\n",
      "\tLoss: 0.12160442769527435\n",
      "\tLoss: 0.060245681554079056\n",
      "\tLoss: 0.11294087767601013\n",
      "\tLoss: 0.14273932576179504\n",
      "\tLoss: 0.1315641701221466\n",
      "\tLoss: 0.11627234518527985\n",
      "\tLoss: 0.10416777431964874\n",
      "\tLoss: 0.1325511634349823\n",
      "\tLoss: 0.17785978317260742\n",
      "\tLoss: 0.156133234500885\n",
      "\tLoss: 0.09705453366041183\n",
      "\tLoss: 0.09780319035053253\n",
      "[time] Epoch 6: 448.02743792999536s = 7.467123965499923m\n",
      "\n",
      "Epoch 7...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.07525131106376648\n",
      "\tLoss: 0.10941478610038757\n",
      "\tLoss: 0.09595535695552826\n",
      "\tLoss: 0.16645993292331696\n",
      "\tLoss: 0.13680414855480194\n",
      "\tLoss: 0.13731320202350616\n",
      "\tLoss: 0.1458544135093689\n",
      "\tLoss: 0.15623290836811066\n",
      "\tLoss: 0.1121395081281662\n",
      "\tLoss: 0.12082867324352264\n",
      "\tLoss: 0.12645231187343597\n",
      "\tLoss: 0.11485825479030609\n",
      "\tLoss: 0.16876934468746185\n",
      "\tLoss: 0.1170024424791336\n",
      "\tLoss: 0.2107885777950287\n",
      "\tLoss: 0.1819181591272354\n",
      "\tLoss: 0.1653197556734085\n",
      "\tLoss: 0.10809439420700073\n",
      "\tLoss: 0.10330454260110855\n",
      "\tLoss: 0.1424340009689331\n",
      "\tLoss: 0.14619788527488708\n",
      "\tLoss: 0.11260975897312164\n",
      "\tLoss: 0.17637649178504944\n",
      "\tLoss: 0.16490185260772705\n",
      "\tLoss: 0.22004112601280212\n",
      "\tLoss: 0.14093995094299316\n",
      "\tLoss: 0.1242135614156723\n",
      "\tLoss: 0.10226213932037354\n",
      "\tLoss: 0.14457038044929504\n",
      "\tLoss: 0.10507036745548248\n",
      "\tLoss: 0.16026556491851807\n",
      "\tLoss: 0.1962650865316391\n",
      "\tLoss: 0.19205573201179504\n",
      "\tLoss: 0.139550119638443\n",
      "\tLoss: 0.0987461656332016\n",
      "\tLoss: 0.1342194825410843\n",
      "\tLoss: 0.11139275133609772\n",
      "\tLoss: 0.10894932597875595\n",
      "\tLoss: 0.16312378644943237\n",
      "\tLoss: 0.19019815325737\n",
      "\tLoss: 0.08951576054096222\n",
      "\tLoss: 0.18831761181354523\n",
      "\tLoss: 0.21207992732524872\n",
      "\tLoss: 0.17725691199302673\n",
      "\tLoss: 0.1484551876783371\n",
      "\tLoss: 0.12438340485095978\n",
      "\tLoss: 0.1428816020488739\n",
      "\tLoss: 0.10855844616889954\n",
      "\tLoss: 0.1279761642217636\n",
      "\tLoss: 0.08668816834688187\n",
      "\tLoss: 0.16093766689300537\n",
      "\tLoss: 0.14378102123737335\n",
      "\tLoss: 0.19739845395088196\n",
      "\tLoss: 0.10285274684429169\n",
      "\tLoss: 0.16747716069221497\n",
      "\tLoss: 0.17020300030708313\n",
      "\tLoss: 0.17066998779773712\n",
      "\tLoss: 0.13941338658332825\n",
      "\tLoss: 0.11450642347335815\n",
      "\tLoss: 0.16203314065933228\n",
      "\tLoss: 0.15952874720096588\n",
      "\tLoss: 0.12842199206352234\n",
      "\tLoss: 0.12639379501342773\n",
      "\tLoss: 0.2034982442855835\n",
      "\tLoss: 0.15408778190612793\n",
      "\tLoss: 0.15547257661819458\n",
      "\tLoss: 0.14891831576824188\n",
      "\tLoss: 0.07076612114906311\n",
      "\tLoss: 0.12996378540992737\n",
      "\tLoss: 0.16529154777526855\n",
      "\tLoss: 0.10518230497837067\n",
      "\tLoss: 0.10870472341775894\n",
      "\tLoss: 0.13641798496246338\n",
      "\tLoss: 0.17909952998161316\n",
      "\tLoss: 0.19168195128440857\n",
      "\tLoss: 0.12560269236564636\n",
      "\tLoss: 0.209705650806427\n",
      "\tLoss: 0.12940391898155212\n",
      "\tLoss: 0.1696760058403015\n",
      "\tLoss: 0.1305955946445465\n",
      "\tLoss: 0.11533869802951813\n",
      "\tLoss: 0.1361277997493744\n",
      "\tLoss: 0.167384535074234\n",
      "\tLoss: 0.19496671855449677\n",
      "\tLoss: 0.1930823028087616\n",
      "\tLoss: 0.16332271695137024\n",
      "\tLoss: 0.13544151186943054\n",
      "\tLoss: 0.11841561645269394\n",
      "\tLoss: 0.12432660907506943\n",
      "\tLoss: 0.20579469203948975\n",
      "\tLoss: 0.14662235975265503\n",
      "\tLoss: 0.11199793964624405\n",
      "\tLoss: 0.195201575756073\n",
      "\tLoss: 0.1353810578584671\n",
      "\tLoss: 0.1066204383969307\n",
      "\tLoss: 0.13967359066009521\n",
      "\tLoss: 0.08701255917549133\n",
      "\tLoss: 0.0981060191988945\n",
      "\tLoss: 0.16142277419567108\n",
      "\tLoss: 0.15569756925106049\n",
      "\tLoss: 0.10518446564674377\n",
      "\tLoss: 0.12757153809070587\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.1329745203256607\n",
      "\tLoss: 0.1258215308189392\n",
      "\tLoss: 0.13138173520565033\n",
      "\tLoss: 0.1478513926267624\n",
      "\tLoss: 0.17274446785449982\n",
      "\tLoss: 0.09687528759241104\n",
      "\tLoss: 0.17432892322540283\n",
      "\tLoss: 0.1522066444158554\n",
      "\tLoss: 0.11351682245731354\n",
      "\tLoss: 0.15547344088554382\n",
      "\tLoss: 0.08630552887916565\n",
      "\tLoss: 0.10649684071540833\n",
      "\tLoss: 0.10766860097646713\n",
      "\tLoss: 0.12367979437112808\n",
      "\tLoss: 0.11947779357433319\n",
      "\tLoss: 0.19791296124458313\n",
      "\tLoss: 0.15267552435398102\n",
      "\tLoss: 0.10263194143772125\n",
      "\tLoss: 0.11396264284849167\n",
      "\tLoss: 0.11864366382360458\n",
      "\tLoss: 0.14915695786476135\n",
      "\tLoss: 0.1060175746679306\n",
      "\tLoss: 0.1923772394657135\n",
      "\tLoss: 0.1110176295042038\n",
      "\tLoss: 0.19668108224868774\n",
      "\tLoss: 0.20610769093036652\n",
      "\tLoss: 0.13142654299736023\n",
      "\tLoss: 0.15782305598258972\n",
      "\tLoss: 0.131799578666687\n",
      "\tLoss: 0.14794494211673737\n",
      "\tLoss: 0.16515110433101654\n",
      "\tLoss: 0.10207568109035492\n",
      "\tLoss: 0.11604561656713486\n",
      "\tLoss: 0.1547105312347412\n",
      "\tLoss: 0.09874621033668518\n",
      "\tLoss: 0.09639231860637665\n",
      "\tLoss: 0.11266952753067017\n",
      "\tLoss: 0.11450503766536713\n",
      "\tLoss: 0.15050527453422546\n",
      "\tLoss: 0.12265056371688843\n",
      "\tLoss: 0.10709695518016815\n",
      "\tLoss: 0.12693607807159424\n",
      "\tLoss: 0.10766753554344177\n",
      "\tLoss: 0.14348620176315308\n",
      "\tLoss: 0.12330389767885208\n",
      "\tLoss: 0.10608942806720734\n",
      "\tLoss: 0.14867277443408966\n",
      "\tLoss: 0.10963909327983856\n",
      "\tLoss: 0.041837990283966064\n",
      "\tLoss: 0.15930598974227905\n",
      "\tLoss: 0.19140861928462982\n",
      "\tLoss: 0.1742255985736847\n",
      "\tLoss: 0.15449179708957672\n",
      "\tLoss: 0.1271839588880539\n",
      "\tLoss: 0.17581802606582642\n",
      "\tLoss: 0.1850917637348175\n",
      "\tLoss: 0.17476747930049896\n",
      "\tLoss: 0.12267465144395828\n",
      "\tLoss: 0.11490204930305481\n",
      "\tLoss: 0.16389736533164978\n",
      "\tLoss: 0.138502299785614\n",
      "\tLoss: 0.12513142824172974\n",
      "\tLoss: 0.14099571108818054\n",
      "\tLoss: 0.10946828126907349\n",
      "\tLoss: 0.13000494241714478\n",
      "\tLoss: 0.1005881056189537\n",
      "\tLoss: 0.17437505722045898\n",
      "\tLoss: 0.11570462584495544\n",
      "\tLoss: 0.14702525734901428\n",
      "\tLoss: 0.09633028507232666\n",
      "\tLoss: 0.1815231591463089\n",
      "\tLoss: 0.09436025470495224\n",
      "\tLoss: 0.15823997557163239\n",
      "\tLoss: 0.14334706962108612\n",
      "\tLoss: 0.13695351779460907\n",
      "\tLoss: 0.17189699411392212\n",
      "\tLoss: 0.22820797562599182\n",
      "\tLoss: 0.1367482990026474\n",
      "\tLoss: 0.19775013625621796\n",
      "\tLoss: 0.10791254788637161\n",
      "\tLoss: 0.13495931029319763\n",
      "\tLoss: 0.12972937524318695\n",
      "\tLoss: 0.10288725048303604\n",
      "\tLoss: 0.1494116485118866\n",
      "\tLoss: 0.12600010633468628\n",
      "\tLoss: 0.15431439876556396\n",
      "\tLoss: 0.15770399570465088\n",
      "\tLoss: 0.12387755513191223\n",
      "\tLoss: 0.17864948511123657\n",
      "\tLoss: 0.11252474784851074\n",
      "\tLoss: 0.17814290523529053\n",
      "\tLoss: 0.11364258080720901\n",
      "\tLoss: 0.15544317662715912\n",
      "\tLoss: 0.15952304005622864\n",
      "\tLoss: 0.13486728072166443\n",
      "\tLoss: 0.18691766262054443\n",
      "\tLoss: 0.08496066927909851\n",
      "\tLoss: 0.11169496923685074\n",
      "\tLoss: 0.13172392547130585\n",
      "\tLoss: 0.1463332623243332\n",
      "\tLoss: 0.17690759897232056\n",
      "\tLoss: 0.1551511287689209\n",
      "[time] Epoch 7: 444.6440493129194s = 7.410734155215323m\n",
      "\n",
      "Epoch 8...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.1874532252550125\n",
      "\tLoss: 0.0994584858417511\n",
      "\tLoss: 0.15153919160366058\n",
      "\tLoss: 0.14370885491371155\n",
      "\tLoss: 0.13295584917068481\n",
      "\tLoss: 0.14913564920425415\n",
      "\tLoss: 0.06765903532505035\n",
      "\tLoss: 0.2011091709136963\n",
      "\tLoss: 0.11046846210956573\n",
      "\tLoss: 0.1356954574584961\n",
      "\tLoss: 0.16886664927005768\n",
      "\tLoss: 0.15063251554965973\n",
      "\tLoss: 0.12476013600826263\n",
      "\tLoss: 0.12614265084266663\n",
      "\tLoss: 0.13583485782146454\n",
      "\tLoss: 0.12160077691078186\n",
      "\tLoss: 0.10710544884204865\n",
      "\tLoss: 0.13416002690792084\n",
      "\tLoss: 0.22316208481788635\n",
      "\tLoss: 0.07359540462493896\n",
      "\tLoss: 0.15922924876213074\n",
      "\tLoss: 0.15954530239105225\n",
      "\tLoss: 0.22431281208992004\n",
      "\tLoss: 0.21153217554092407\n",
      "\tLoss: 0.13400068879127502\n",
      "\tLoss: 0.16943717002868652\n",
      "\tLoss: 0.1534515619277954\n",
      "\tLoss: 0.11722588539123535\n",
      "\tLoss: 0.16352660953998566\n",
      "\tLoss: 0.12180661410093307\n",
      "\tLoss: 0.13607598841190338\n",
      "\tLoss: 0.1499631404876709\n",
      "\tLoss: 0.20454230904579163\n",
      "\tLoss: 0.1541995108127594\n",
      "\tLoss: 0.1351708471775055\n",
      "\tLoss: 0.1892605572938919\n",
      "\tLoss: 0.17123591899871826\n",
      "\tLoss: 0.1330968141555786\n",
      "\tLoss: 0.13491514325141907\n",
      "\tLoss: 0.11820047348737717\n",
      "\tLoss: 0.15258941054344177\n",
      "\tLoss: 0.13019150495529175\n",
      "\tLoss: 0.15187384188175201\n",
      "\tLoss: 0.1630527526140213\n",
      "\tLoss: 0.12340397387742996\n",
      "\tLoss: 0.0811336413025856\n",
      "\tLoss: 0.1586645543575287\n",
      "\tLoss: 0.10716607421636581\n",
      "\tLoss: 0.12514865398406982\n",
      "\tLoss: 0.10742712765932083\n",
      "\tLoss: 0.15834587812423706\n",
      "\tLoss: 0.10253308713436127\n",
      "\tLoss: 0.21937887370586395\n",
      "\tLoss: 0.0889928862452507\n",
      "\tLoss: 0.20375172793865204\n",
      "\tLoss: 0.13807621598243713\n",
      "\tLoss: 0.11899363994598389\n",
      "\tLoss: 0.08770708739757538\n",
      "\tLoss: 0.15654507279396057\n",
      "\tLoss: 0.10783650726079941\n",
      "\tLoss: 0.11971686035394669\n",
      "\tLoss: 0.17760667204856873\n",
      "\tLoss: 0.13543486595153809\n",
      "\tLoss: 0.12660762667655945\n",
      "\tLoss: 0.1358199268579483\n",
      "\tLoss: 0.1519041806459427\n",
      "\tLoss: 0.13330459594726562\n",
      "\tLoss: 0.09559954702854156\n",
      "\tLoss: 0.14593540132045746\n",
      "\tLoss: 0.09289747476577759\n",
      "\tLoss: 0.17775151133537292\n",
      "\tLoss: 0.15921534597873688\n",
      "\tLoss: 0.09004774689674377\n",
      "\tLoss: 0.11311104893684387\n",
      "\tLoss: 0.1611851155757904\n",
      "\tLoss: 0.14200367033481598\n",
      "\tLoss: 0.17638981342315674\n",
      "\tLoss: 0.19560647010803223\n",
      "\tLoss: 0.09124858677387238\n",
      "\tLoss: 0.10728351771831512\n",
      "\tLoss: 0.13961508870124817\n",
      "\tLoss: 0.08285591006278992\n",
      "\tLoss: 0.19903118908405304\n",
      "\tLoss: 0.15568360686302185\n",
      "\tLoss: 0.16382244229316711\n",
      "\tLoss: 0.1147736981511116\n",
      "\tLoss: 0.1418599784374237\n",
      "\tLoss: 0.17163832485675812\n",
      "\tLoss: 0.14937865734100342\n",
      "\tLoss: 0.18623268604278564\n",
      "\tLoss: 0.11313335597515106\n",
      "\tLoss: 0.1490885317325592\n",
      "\tLoss: 0.1359235942363739\n",
      "\tLoss: 0.12114852666854858\n",
      "\tLoss: 0.0816647931933403\n",
      "\tLoss: 0.06429184228181839\n",
      "\tLoss: 0.16519342362880707\n",
      "\tLoss: 0.12564510107040405\n",
      "\tLoss: 0.163490429520607\n",
      "\tLoss: 0.14723269641399384\n",
      "\tLoss: 0.11450345814228058\n",
      "\tLoss: 0.14358162879943848\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.10861761122941971\n",
      "\tLoss: 0.17125096917152405\n",
      "\tLoss: 0.16865256428718567\n",
      "\tLoss: 0.18984867632389069\n",
      "\tLoss: 0.11444011330604553\n",
      "\tLoss: 0.13743005692958832\n",
      "\tLoss: 0.1409079134464264\n",
      "\tLoss: 0.12106762826442719\n",
      "\tLoss: 0.1264660656452179\n",
      "\tLoss: 0.08218566328287125\n",
      "\tLoss: 0.1575063169002533\n",
      "\tLoss: 0.11654693633317947\n",
      "\tLoss: 0.14083465933799744\n",
      "\tLoss: 0.12791907787322998\n",
      "\tLoss: 0.09648077934980392\n",
      "\tLoss: 0.12576624751091003\n",
      "\tLoss: 0.15842679142951965\n",
      "\tLoss: 0.16115981340408325\n",
      "\tLoss: 0.11199511587619781\n",
      "\tLoss: 0.10966064035892487\n",
      "\tLoss: 0.13028165698051453\n",
      "\tLoss: 0.13108530640602112\n",
      "\tLoss: 0.1679118275642395\n",
      "\tLoss: 0.11250972002744675\n",
      "\tLoss: 0.1258632093667984\n",
      "\tLoss: 0.18236002326011658\n",
      "\tLoss: 0.09786505997180939\n",
      "\tLoss: 0.18457213044166565\n",
      "\tLoss: 0.1858496069908142\n",
      "\tLoss: 0.1278882622718811\n",
      "\tLoss: 0.18364045023918152\n",
      "\tLoss: 0.136127308011055\n",
      "\tLoss: 0.10110896825790405\n",
      "\tLoss: 0.12333092838525772\n",
      "\tLoss: 0.0943080484867096\n",
      "\tLoss: 0.1193973571062088\n",
      "\tLoss: 0.15782283246517181\n",
      "\tLoss: 0.08279646188020706\n",
      "\tLoss: 0.13609282672405243\n",
      "\tLoss: 0.1206410676240921\n",
      "\tLoss: 0.10259371250867844\n",
      "\tLoss: 0.1058330312371254\n",
      "\tLoss: 0.11698378622531891\n",
      "\tLoss: 0.13629606366157532\n",
      "\tLoss: 0.09964320063591003\n",
      "\tLoss: 0.1827434003353119\n",
      "\tLoss: 0.13155749440193176\n",
      "\tLoss: 0.08876544237136841\n",
      "\tLoss: 0.0979308933019638\n",
      "\tLoss: 0.10994109511375427\n",
      "\tLoss: 0.13489246368408203\n",
      "\tLoss: 0.06887084245681763\n",
      "\tLoss: 0.13636893033981323\n",
      "\tLoss: 0.11655332148075104\n",
      "\tLoss: 0.17774687707424164\n",
      "\tLoss: 0.10859476029872894\n",
      "\tLoss: 0.11353840678930283\n",
      "\tLoss: 0.21672049164772034\n",
      "\tLoss: 0.10829956829547882\n",
      "\tLoss: 0.12985587120056152\n",
      "\tLoss: 0.1727609932422638\n",
      "\tLoss: 0.13675843179225922\n",
      "\tLoss: 0.16119015216827393\n",
      "\tLoss: 0.06716173887252808\n",
      "\tLoss: 0.17459142208099365\n",
      "\tLoss: 0.16440936923027039\n",
      "\tLoss: 0.10153954476118088\n",
      "\tLoss: 0.0915929526090622\n",
      "\tLoss: 0.1411401331424713\n",
      "\tLoss: 0.15410268306732178\n",
      "\tLoss: 0.08873257040977478\n",
      "\tLoss: 0.13543184101581573\n",
      "\tLoss: 0.10876818001270294\n",
      "\tLoss: 0.13615185022354126\n",
      "\tLoss: 0.21166567504405975\n",
      "\tLoss: 0.14252610504627228\n",
      "\tLoss: 0.09935371577739716\n",
      "\tLoss: 0.15296852588653564\n",
      "\tLoss: 0.07808474451303482\n",
      "\tLoss: 0.11156458407640457\n",
      "\tLoss: 0.14198198914527893\n",
      "\tLoss: 0.12249511480331421\n",
      "\tLoss: 0.1690848469734192\n",
      "\tLoss: 0.09560579061508179\n",
      "\tLoss: 0.13616159558296204\n",
      "\tLoss: 0.2209216058254242\n",
      "\tLoss: 0.1817632019519806\n",
      "\tLoss: 0.12564979493618011\n",
      "\tLoss: 0.13001199066638947\n",
      "\tLoss: 0.14548012614250183\n",
      "\tLoss: 0.19041511416435242\n",
      "\tLoss: 0.1285513937473297\n",
      "\tLoss: 0.08162972331047058\n",
      "\tLoss: 0.10665486752986908\n",
      "\tLoss: 0.08348417282104492\n",
      "\tLoss: 0.16568627953529358\n",
      "\tLoss: 0.17544347047805786\n",
      "\tLoss: 0.16811633110046387\n",
      "\tLoss: 0.12279114872217178\n",
      "\tLoss: 0.18389850854873657\n",
      "\tLoss: 0.12063898146152496\n",
      "\tLoss: 0.17981724441051483\n",
      "[time] Epoch 8: 444.82607213407755s = 7.413767868901292m\n",
      "\n",
      "Epoch 9...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.1322324573993683\n",
      "\tLoss: 0.18518824875354767\n",
      "\tLoss: 0.08802258968353271\n",
      "\tLoss: 0.0986892357468605\n",
      "\tLoss: 0.1044103130698204\n",
      "\tLoss: 0.09454703330993652\n",
      "\tLoss: 0.15913128852844238\n",
      "\tLoss: 0.11277145892381668\n",
      "\tLoss: 0.13626137375831604\n",
      "\tLoss: 0.1939067542552948\n",
      "\tLoss: 0.21790626645088196\n",
      "\tLoss: 0.16448645293712616\n",
      "\tLoss: 0.1845247447490692\n",
      "\tLoss: 0.1264326274394989\n",
      "\tLoss: 0.11578372865915298\n",
      "\tLoss: 0.14312249422073364\n",
      "\tLoss: 0.181776762008667\n",
      "\tLoss: 0.17497719824314117\n",
      "\tLoss: 0.16572654247283936\n",
      "\tLoss: 0.14880019426345825\n",
      "\tLoss: 0.10220871865749359\n",
      "\tLoss: 0.13295596837997437\n",
      "\tLoss: 0.1624234914779663\n",
      "\tLoss: 0.1552700400352478\n",
      "\tLoss: 0.11707036942243576\n",
      "\tLoss: 0.10510624200105667\n",
      "\tLoss: 0.09619282186031342\n",
      "\tLoss: 0.07963979989290237\n",
      "\tLoss: 0.1202208399772644\n",
      "\tLoss: 0.13187938928604126\n",
      "\tLoss: 0.08355889469385147\n",
      "\tLoss: 0.13336898386478424\n",
      "\tLoss: 0.20887881517410278\n",
      "\tLoss: 0.1342076063156128\n",
      "\tLoss: 0.14415642619132996\n",
      "\tLoss: 0.1356862485408783\n",
      "\tLoss: 0.08127258718013763\n",
      "\tLoss: 0.11586150527000427\n",
      "\tLoss: 0.12171772867441177\n",
      "\tLoss: 0.09883971512317657\n",
      "\tLoss: 0.10097837448120117\n",
      "\tLoss: 0.0972648411989212\n",
      "\tLoss: 0.1718260943889618\n",
      "\tLoss: 0.16247349977493286\n",
      "\tLoss: 0.19889235496520996\n",
      "\tLoss: 0.11346178501844406\n",
      "\tLoss: 0.16211888194084167\n",
      "\tLoss: 0.15500394999980927\n",
      "\tLoss: 0.15456004440784454\n",
      "\tLoss: 0.09682194143533707\n",
      "\tLoss: 0.16545042395591736\n",
      "\tLoss: 0.1326543092727661\n",
      "\tLoss: 0.15370656549930573\n",
      "\tLoss: 0.17735852301120758\n",
      "\tLoss: 0.16390837728977203\n",
      "\tLoss: 0.1536465883255005\n",
      "\tLoss: 0.21278667449951172\n",
      "\tLoss: 0.12835362553596497\n",
      "\tLoss: 0.11819306761026382\n",
      "\tLoss: 0.08840896189212799\n",
      "\tLoss: 0.12127281725406647\n",
      "\tLoss: 0.10294964164495468\n",
      "\tLoss: 0.19263045489788055\n",
      "\tLoss: 0.15363936126232147\n",
      "\tLoss: 0.12375031411647797\n",
      "\tLoss: 0.13290807604789734\n",
      "\tLoss: 0.13638867437839508\n",
      "\tLoss: 0.12294909358024597\n",
      "\tLoss: 0.09702107310295105\n",
      "\tLoss: 0.10983346402645111\n",
      "\tLoss: 0.08036204427480698\n",
      "\tLoss: 0.15804040431976318\n",
      "\tLoss: 0.17306268215179443\n",
      "\tLoss: 0.07796064019203186\n",
      "\tLoss: 0.18542243540287018\n",
      "\tLoss: 0.10745227336883545\n",
      "\tLoss: 0.10056190937757492\n",
      "\tLoss: 0.19717779755592346\n",
      "\tLoss: 0.15747283399105072\n",
      "\tLoss: 0.21680551767349243\n",
      "\tLoss: 0.10378337651491165\n",
      "\tLoss: 0.17506404221057892\n",
      "\tLoss: 0.14185170829296112\n",
      "\tLoss: 0.15770107507705688\n",
      "\tLoss: 0.09302470833063126\n",
      "\tLoss: 0.11650435626506805\n",
      "\tLoss: 0.13112697005271912\n",
      "\tLoss: 0.13922317326068878\n",
      "\tLoss: 0.1526924967765808\n",
      "\tLoss: 0.19649511575698853\n",
      "\tLoss: 0.07653909921646118\n",
      "\tLoss: 0.13942629098892212\n",
      "\tLoss: 0.08301521837711334\n",
      "\tLoss: 0.15064695477485657\n",
      "\tLoss: 0.1735038310289383\n",
      "\tLoss: 0.08925627171993256\n",
      "\tLoss: 0.13445621728897095\n",
      "\tLoss: 0.09104101359844208\n",
      "\tLoss: 0.2578727900981903\n",
      "\tLoss: 0.13181182742118835\n",
      "\tLoss: 0.12838155031204224\n",
      "\tLoss: 0.07111427932977676\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.1185419112443924\n",
      "\tLoss: 0.2127414345741272\n",
      "\tLoss: 0.15423470735549927\n",
      "\tLoss: 0.1485159546136856\n",
      "\tLoss: 0.15321527421474457\n",
      "\tLoss: 0.15786497294902802\n",
      "\tLoss: 0.12979355454444885\n",
      "\tLoss: 0.08755183219909668\n",
      "\tLoss: 0.11490094661712646\n",
      "\tLoss: 0.09715734422206879\n",
      "\tLoss: 0.16911327838897705\n",
      "\tLoss: 0.12462221831083298\n",
      "\tLoss: 0.08909177035093307\n",
      "\tLoss: 0.14619478583335876\n",
      "\tLoss: 0.10259129106998444\n",
      "\tLoss: 0.13214153051376343\n",
      "\tLoss: 0.1090795174241066\n",
      "\tLoss: 0.08250605314970016\n",
      "\tLoss: 0.13112816214561462\n",
      "\tLoss: 0.08323707431554794\n",
      "\tLoss: 0.09104739874601364\n",
      "\tLoss: 0.17809128761291504\n",
      "\tLoss: 0.10087147355079651\n",
      "\tLoss: 0.11202961206436157\n",
      "\tLoss: 0.15326711535453796\n",
      "\tLoss: 0.18972595036029816\n",
      "\tLoss: 0.12172974646091461\n",
      "\tLoss: 0.1301896572113037\n",
      "\tLoss: 0.1604619324207306\n",
      "\tLoss: 0.12373918294906616\n",
      "\tLoss: 0.16930124163627625\n",
      "\tLoss: 0.09564937651157379\n",
      "\tLoss: 0.1257420778274536\n",
      "\tLoss: 0.12804093956947327\n",
      "\tLoss: 0.11743369698524475\n",
      "\tLoss: 0.17818889021873474\n",
      "\tLoss: 0.10465199500322342\n",
      "\tLoss: 0.1308537721633911\n",
      "\tLoss: 0.10310573875904083\n",
      "\tLoss: 0.09509691596031189\n",
      "\tLoss: 0.1575501263141632\n",
      "\tLoss: 0.11381068825721741\n",
      "\tLoss: 0.16179579496383667\n",
      "\tLoss: 0.1018979549407959\n",
      "\tLoss: 0.12642161548137665\n",
      "\tLoss: 0.06246555969119072\n",
      "\tLoss: 0.1507309079170227\n",
      "\tLoss: 0.15322083234786987\n",
      "\tLoss: 0.10670321434736252\n",
      "\tLoss: 0.14877057075500488\n",
      "\tLoss: 0.18121802806854248\n",
      "\tLoss: 0.17811773717403412\n",
      "\tLoss: 0.1635894477367401\n",
      "\tLoss: 0.22174665331840515\n",
      "\tLoss: 0.1497465968132019\n",
      "\tLoss: 0.14229318499565125\n",
      "\tLoss: 0.1820250153541565\n",
      "\tLoss: 0.11635981500148773\n",
      "\tLoss: 0.109036386013031\n",
      "\tLoss: 0.12767857313156128\n",
      "\tLoss: 0.15435713529586792\n",
      "\tLoss: 0.1428905725479126\n",
      "\tLoss: 0.1531040370464325\n",
      "\tLoss: 0.16188190877437592\n",
      "\tLoss: 0.12556049227714539\n",
      "\tLoss: 0.0798444151878357\n",
      "\tLoss: 0.19033223390579224\n",
      "\tLoss: 0.1938527226448059\n",
      "\tLoss: 0.0705767571926117\n",
      "\tLoss: 0.1891317367553711\n",
      "\tLoss: 0.06180746853351593\n",
      "\tLoss: 0.10251343250274658\n",
      "\tLoss: 0.1303761899471283\n",
      "\tLoss: 0.12183506786823273\n",
      "\tLoss: 0.08778607845306396\n",
      "\tLoss: 0.23153270781040192\n",
      "\tLoss: 0.11167271435260773\n",
      "\tLoss: 0.1851111650466919\n",
      "\tLoss: 0.12052242457866669\n",
      "\tLoss: 0.09461504966020584\n",
      "\tLoss: 0.144923597574234\n",
      "\tLoss: 0.1455405056476593\n",
      "\tLoss: 0.12992019951343536\n",
      "\tLoss: 0.14898189902305603\n",
      "\tLoss: 0.15449944138526917\n",
      "\tLoss: 0.17412570118904114\n",
      "\tLoss: 0.0672847181558609\n",
      "\tLoss: 0.11025533825159073\n",
      "\tLoss: 0.15793094038963318\n",
      "\tLoss: 0.14909961819648743\n",
      "\tLoss: 0.09054520726203918\n",
      "\tLoss: 0.11341915279626846\n",
      "\tLoss: 0.07538211345672607\n",
      "\tLoss: 0.1892150342464447\n",
      "\tLoss: 0.19004186987876892\n",
      "\tLoss: 0.13174819946289062\n",
      "\tLoss: 0.1605350375175476\n",
      "\tLoss: 0.11816851049661636\n",
      "\tLoss: 0.18010763823986053\n",
      "\tLoss: 0.17926612496376038\n",
      "\tLoss: 0.16689755022525787\n",
      "\tLoss: 0.14024756848812103\n",
      "[time] Epoch 9: 447.8169669662602s = 7.463616116104337m\n",
      "\n",
      "Epoch 10...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.2532331943511963\n",
      "\tLoss: 0.17060095071792603\n",
      "\tLoss: 0.14688251912593842\n",
      "\tLoss: 0.20358161628246307\n",
      "\tLoss: 0.0979001596570015\n",
      "\tLoss: 0.1713658571243286\n",
      "\tLoss: 0.09351475536823273\n",
      "\tLoss: 0.10150608420372009\n",
      "\tLoss: 0.11659309267997742\n",
      "\tLoss: 0.1937292516231537\n",
      "\tLoss: 0.16290798783302307\n",
      "\tLoss: 0.1602487862110138\n",
      "\tLoss: 0.09827685356140137\n",
      "\tLoss: 0.11507877707481384\n",
      "\tLoss: 0.10714110732078552\n",
      "\tLoss: 0.18822062015533447\n",
      "\tLoss: 0.1424667239189148\n",
      "\tLoss: 0.16530665755271912\n",
      "\tLoss: 0.1320832371711731\n",
      "\tLoss: 0.132859006524086\n",
      "\tLoss: 0.19278904795646667\n",
      "\tLoss: 0.07697656005620956\n",
      "\tLoss: 0.15258267521858215\n",
      "\tLoss: 0.10446640849113464\n",
      "\tLoss: 0.09396689385175705\n",
      "\tLoss: 0.18422460556030273\n",
      "\tLoss: 0.11991363763809204\n",
      "\tLoss: 0.1354512870311737\n",
      "\tLoss: 0.1588256061077118\n",
      "\tLoss: 0.12281586229801178\n",
      "\tLoss: 0.09800489246845245\n",
      "\tLoss: 0.12955661118030548\n",
      "\tLoss: 0.1765025556087494\n",
      "\tLoss: 0.1372348964214325\n",
      "\tLoss: 0.16458562016487122\n",
      "\tLoss: 0.10580086708068848\n",
      "\tLoss: 0.2457352876663208\n",
      "\tLoss: 0.13440299034118652\n",
      "\tLoss: 0.12808777391910553\n",
      "\tLoss: 0.1495445966720581\n",
      "\tLoss: 0.15555045008659363\n",
      "\tLoss: 0.18504583835601807\n",
      "\tLoss: 0.16741961240768433\n",
      "\tLoss: 0.08517619967460632\n",
      "\tLoss: 0.06632964313030243\n",
      "\tLoss: 0.1473182737827301\n",
      "\tLoss: 0.10826295614242554\n",
      "\tLoss: 0.17765504121780396\n",
      "\tLoss: 0.10726980119943619\n",
      "\tLoss: 0.1105584129691124\n",
      "\tLoss: 0.14529316127300262\n",
      "\tLoss: 0.08517654240131378\n",
      "\tLoss: 0.16936618089675903\n",
      "\tLoss: 0.09710121154785156\n",
      "\tLoss: 0.11676518619060516\n",
      "\tLoss: 0.16754454374313354\n",
      "\tLoss: 0.11790768057107925\n",
      "\tLoss: 0.1644590049982071\n",
      "\tLoss: 0.14346647262573242\n",
      "\tLoss: 0.09731326252222061\n",
      "\tLoss: 0.12352877855300903\n",
      "\tLoss: 0.09554216265678406\n",
      "\tLoss: 0.16747480630874634\n",
      "\tLoss: 0.12647274136543274\n",
      "\tLoss: 0.17480336129665375\n",
      "\tLoss: 0.13753271102905273\n",
      "\tLoss: 0.12259809672832489\n",
      "\tLoss: 0.17356213927268982\n",
      "\tLoss: 0.12212301790714264\n",
      "\tLoss: 0.13184326887130737\n",
      "\tLoss: 0.12173763662576675\n",
      "\tLoss: 0.11462028324604034\n",
      "\tLoss: 0.16612811386585236\n",
      "\tLoss: 0.1190100833773613\n",
      "\tLoss: 0.2130652368068695\n",
      "\tLoss: 0.12054625153541565\n",
      "\tLoss: 0.09334605187177658\n",
      "\tLoss: 0.13516494631767273\n",
      "\tLoss: 0.13816870748996735\n",
      "\tLoss: 0.1492472141981125\n",
      "\tLoss: 0.08547285944223404\n",
      "\tLoss: 0.10685350745916367\n",
      "\tLoss: 0.15977084636688232\n",
      "\tLoss: 0.14775170385837555\n",
      "\tLoss: 0.08107244223356247\n",
      "\tLoss: 0.17844077944755554\n",
      "\tLoss: 0.15545162558555603\n",
      "\tLoss: 0.16253423690795898\n",
      "\tLoss: 0.09317174553871155\n",
      "\tLoss: 0.1462896168231964\n",
      "\tLoss: 0.11651264876127243\n",
      "\tLoss: 0.1988196074962616\n",
      "\tLoss: 0.07422013580799103\n",
      "\tLoss: 0.08873671293258667\n",
      "\tLoss: 0.1524810791015625\n",
      "\tLoss: 0.16684287786483765\n",
      "\tLoss: 0.149231418967247\n",
      "\tLoss: 0.17254775762557983\n",
      "\tLoss: 0.09801581501960754\n",
      "\tLoss: 0.20156854391098022\n",
      "\tLoss: 0.12559621036052704\n",
      "\tLoss: 0.15303097665309906\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.1825755536556244\n",
      "\tLoss: 0.13886503875255585\n",
      "\tLoss: 0.07950165867805481\n",
      "\tLoss: 0.16814345121383667\n",
      "\tLoss: 0.12225714325904846\n",
      "\tLoss: 0.14694932103157043\n",
      "\tLoss: 0.16118545830249786\n",
      "\tLoss: 0.09181113541126251\n",
      "\tLoss: 0.07076676934957504\n",
      "\tLoss: 0.07933153957128525\n",
      "\tLoss: 0.09973946213722229\n",
      "\tLoss: 0.16637001931667328\n",
      "\tLoss: 0.11876671016216278\n",
      "\tLoss: 0.06154436618089676\n",
      "\tLoss: 0.12127243727445602\n",
      "\tLoss: 0.11548839509487152\n",
      "\tLoss: 0.07301941514015198\n",
      "\tLoss: 0.08572410047054291\n",
      "\tLoss: 0.1179741770029068\n",
      "\tLoss: 0.14856424927711487\n",
      "\tLoss: 0.11488319933414459\n",
      "\tLoss: 0.16029569506645203\n",
      "\tLoss: 0.15340054035186768\n",
      "\tLoss: 0.09556406736373901\n",
      "\tLoss: 0.12456683814525604\n",
      "\tLoss: 0.14012162387371063\n",
      "\tLoss: 0.0969102680683136\n",
      "\tLoss: 0.11394823342561722\n",
      "\tLoss: 0.11057628691196442\n",
      "\tLoss: 0.14378786087036133\n",
      "\tLoss: 0.10021563619375229\n",
      "\tLoss: 0.09999494254589081\n",
      "\tLoss: 0.10778800398111343\n",
      "\tLoss: 0.14841611683368683\n",
      "\tLoss: 0.11815579980611801\n",
      "\tLoss: 0.14369362592697144\n",
      "\tLoss: 0.08819204568862915\n",
      "\tLoss: 0.13616976141929626\n",
      "\tLoss: 0.12340959906578064\n",
      "\tLoss: 0.15455645322799683\n",
      "\tLoss: 0.14449442923069\n",
      "\tLoss: 0.19054661691188812\n",
      "\tLoss: 0.17617079615592957\n",
      "\tLoss: 0.14218410849571228\n",
      "\tLoss: 0.08374229073524475\n",
      "\tLoss: 0.18102718889713287\n",
      "\tLoss: 0.07817117124795914\n",
      "\tLoss: 0.1367274820804596\n",
      "\tLoss: 0.14227578043937683\n",
      "\tLoss: 0.08696343749761581\n",
      "\tLoss: 0.1353365182876587\n",
      "\tLoss: 0.10535024851560593\n",
      "\tLoss: 0.11277710646390915\n",
      "\tLoss: 0.08690783381462097\n",
      "\tLoss: 0.09949338436126709\n",
      "\tLoss: 0.14063619077205658\n",
      "\tLoss: 0.13082243502140045\n",
      "\tLoss: 0.1935604214668274\n",
      "\tLoss: 0.1303429901599884\n",
      "\tLoss: 0.16289342939853668\n",
      "\tLoss: 0.15203753113746643\n",
      "\tLoss: 0.0739557147026062\n",
      "\tLoss: 0.17138686776161194\n",
      "\tLoss: 0.13122157752513885\n",
      "\tLoss: 0.1123463436961174\n",
      "\tLoss: 0.15400320291519165\n",
      "\tLoss: 0.1210111677646637\n",
      "\tLoss: 0.14078623056411743\n",
      "\tLoss: 0.12709179520606995\n",
      "\tLoss: 0.2199440598487854\n",
      "\tLoss: 0.16470783948898315\n",
      "\tLoss: 0.10016487538814545\n",
      "\tLoss: 0.14372697472572327\n",
      "\tLoss: 0.16533340513706207\n",
      "\tLoss: 0.1414426565170288\n",
      "\tLoss: 0.13696832954883575\n",
      "\tLoss: 0.1105721965432167\n",
      "\tLoss: 0.08859618008136749\n",
      "\tLoss: 0.10091572254896164\n",
      "\tLoss: 0.11029961705207825\n",
      "\tLoss: 0.1085728257894516\n",
      "\tLoss: 0.16021624207496643\n",
      "\tLoss: 0.13867318630218506\n",
      "\tLoss: 0.133636936545372\n",
      "\tLoss: 0.14552778005599976\n",
      "\tLoss: 0.0999312698841095\n",
      "\tLoss: 0.110164575278759\n",
      "\tLoss: 0.11730495095252991\n",
      "\tLoss: 0.18022966384887695\n",
      "\tLoss: 0.12827536463737488\n",
      "\tLoss: 0.1013985201716423\n",
      "\tLoss: 0.13333553075790405\n",
      "\tLoss: 0.2104172557592392\n",
      "\tLoss: 0.1584784984588623\n",
      "\tLoss: 0.15126991271972656\n",
      "\tLoss: 0.1299837976694107\n",
      "\tLoss: 0.09902188926935196\n",
      "\tLoss: 0.1304059773683548\n",
      "\tLoss: 0.16096557676792145\n",
      "\tLoss: 0.20709294080734253\n",
      "\tLoss: 0.1808202862739563\n",
      "\tLoss: 0.1245628297328949\n",
      "[time] Epoch 10: 446.6178374947049s = 7.443630624911748m\n",
      "\n",
      "Epoch 11...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.13904444873332977\n",
      "\tLoss: 0.11173222213983536\n",
      "\tLoss: 0.10430508852005005\n",
      "\tLoss: 0.07205114513635635\n",
      "\tLoss: 0.09650103002786636\n",
      "\tLoss: 0.14683139324188232\n",
      "\tLoss: 0.0976703017950058\n",
      "\tLoss: 0.13129204511642456\n",
      "\tLoss: 0.19032171368598938\n",
      "\tLoss: 0.1057882234454155\n",
      "\tLoss: 0.10426319390535355\n",
      "\tLoss: 0.1274188756942749\n",
      "\tLoss: 0.08734950423240662\n",
      "\tLoss: 0.15520131587982178\n",
      "\tLoss: 0.13299262523651123\n",
      "\tLoss: 0.06563427299261093\n",
      "\tLoss: 0.1193213164806366\n",
      "\tLoss: 0.11668162047863007\n",
      "\tLoss: 0.10034780949354172\n",
      "\tLoss: 0.13027793169021606\n",
      "\tLoss: 0.10616004467010498\n",
      "\tLoss: 0.09962141513824463\n",
      "\tLoss: 0.11785172671079636\n",
      "\tLoss: 0.1233203262090683\n",
      "\tLoss: 0.08195404708385468\n",
      "\tLoss: 0.11491204798221588\n",
      "\tLoss: 0.10970975458621979\n",
      "\tLoss: 0.11730565875768661\n",
      "\tLoss: 0.15384237468242645\n",
      "\tLoss: 0.11311399936676025\n",
      "\tLoss: 0.12916280329227448\n",
      "\tLoss: 0.1712670922279358\n",
      "\tLoss: 0.1114901527762413\n",
      "\tLoss: 0.18424656987190247\n",
      "\tLoss: 0.18799977004528046\n",
      "\tLoss: 0.0901017114520073\n",
      "\tLoss: 0.1394423246383667\n",
      "\tLoss: 0.08338044583797455\n",
      "\tLoss: 0.09927648305892944\n",
      "\tLoss: 0.08238784968852997\n",
      "\tLoss: 0.09647491574287415\n",
      "\tLoss: 0.09765411168336868\n",
      "\tLoss: 0.03257676959037781\n",
      "\tLoss: 0.11859892308712006\n",
      "\tLoss: 0.11179128289222717\n",
      "\tLoss: 0.14124271273612976\n",
      "\tLoss: 0.13920338451862335\n",
      "\tLoss: 0.11043794453144073\n",
      "\tLoss: 0.18323387205600739\n",
      "\tLoss: 0.12214644253253937\n",
      "\tLoss: 0.12762197852134705\n",
      "\tLoss: 0.1572737991809845\n",
      "\tLoss: 0.11051775515079498\n",
      "\tLoss: 0.10484063625335693\n",
      "\tLoss: 0.09145048260688782\n",
      "\tLoss: 0.1149088442325592\n",
      "\tLoss: 0.14791074395179749\n",
      "\tLoss: 0.13397705554962158\n",
      "\tLoss: 0.12390124797821045\n",
      "\tLoss: 0.1728944182395935\n",
      "\tLoss: 0.11371003091335297\n",
      "\tLoss: 0.08222165703773499\n",
      "\tLoss: 0.0884648859500885\n",
      "\tLoss: 0.12415006756782532\n",
      "\tLoss: 0.08401815593242645\n",
      "\tLoss: 0.13759580254554749\n",
      "\tLoss: 0.12285246700048447\n",
      "\tLoss: 0.09535321593284607\n",
      "\tLoss: 0.1772317886352539\n",
      "\tLoss: 0.12504664063453674\n",
      "\tLoss: 0.11009864509105682\n",
      "\tLoss: 0.120555579662323\n",
      "\tLoss: 0.11883829534053802\n",
      "\tLoss: 0.20996052026748657\n",
      "\tLoss: 0.11196482926607132\n",
      "\tLoss: 0.07416471093893051\n",
      "\tLoss: 0.15259188413619995\n",
      "\tLoss: 0.1668546348810196\n",
      "\tLoss: 0.17298561334609985\n",
      "\tLoss: 0.1324390172958374\n",
      "\tLoss: 0.11335930228233337\n",
      "\tLoss: 0.09626668691635132\n",
      "\tLoss: 0.1395682394504547\n",
      "\tLoss: 0.128033846616745\n",
      "\tLoss: 0.11665631085634232\n",
      "\tLoss: 0.12402479350566864\n",
      "\tLoss: 0.16206061840057373\n",
      "\tLoss: 0.07600495219230652\n",
      "\tLoss: 0.17341098189353943\n",
      "\tLoss: 0.11717750877141953\n",
      "\tLoss: 0.19069963693618774\n",
      "\tLoss: 0.08037814497947693\n",
      "\tLoss: 0.11501078307628632\n",
      "\tLoss: 0.1349903792142868\n",
      "\tLoss: 0.05800545588135719\n",
      "\tLoss: 0.08876754343509674\n",
      "\tLoss: 0.13905489444732666\n",
      "\tLoss: 0.12351420521736145\n",
      "\tLoss: 0.19506902992725372\n",
      "\tLoss: 0.1112503632903099\n",
      "\tLoss: 0.06297356635332108\n",
      "\tLoss: 0.10314810276031494\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.11978836357593536\n",
      "\tLoss: 0.1365312933921814\n",
      "\tLoss: 0.1679205298423767\n",
      "\tLoss: 0.17366114258766174\n",
      "\tLoss: 0.13902689516544342\n",
      "\tLoss: 0.09566020965576172\n",
      "\tLoss: 0.20971204340457916\n",
      "\tLoss: 0.10684865713119507\n",
      "\tLoss: 0.19074496626853943\n",
      "\tLoss: 0.12519106268882751\n",
      "\tLoss: 0.14773693680763245\n",
      "\tLoss: 0.09709873795509338\n",
      "\tLoss: 0.06593362241983414\n",
      "\tLoss: 0.12757475674152374\n",
      "\tLoss: 0.11896125227212906\n",
      "\tLoss: 0.13531455397605896\n",
      "\tLoss: 0.11854282021522522\n",
      "\tLoss: 0.1108330488204956\n",
      "\tLoss: 0.14005295932292938\n",
      "\tLoss: 0.1379590630531311\n",
      "\tLoss: 0.10297592729330063\n",
      "\tLoss: 0.11472366750240326\n",
      "\tLoss: 0.10405884683132172\n",
      "\tLoss: 0.09090600162744522\n",
      "\tLoss: 0.10251467674970627\n",
      "\tLoss: 0.1534496247768402\n",
      "\tLoss: 0.08419843018054962\n",
      "\tLoss: 0.11433486640453339\n",
      "\tLoss: 0.12430378049612045\n",
      "\tLoss: 0.1254366636276245\n",
      "\tLoss: 0.08069832623004913\n",
      "\tLoss: 0.16528913378715515\n",
      "\tLoss: 0.17792315781116486\n",
      "\tLoss: 0.10584989190101624\n",
      "\tLoss: 0.153098002076149\n",
      "\tLoss: 0.11105131357908249\n",
      "\tLoss: 0.09365694224834442\n",
      "\tLoss: 0.08201460540294647\n",
      "\tLoss: 0.13660533726215363\n",
      "\tLoss: 0.09854090213775635\n",
      "\tLoss: 0.13562646508216858\n",
      "\tLoss: 0.16043931245803833\n",
      "\tLoss: 0.1799907386302948\n",
      "\tLoss: 0.12503349781036377\n",
      "\tLoss: 0.17013686895370483\n",
      "\tLoss: 0.06818027794361115\n",
      "\tLoss: 0.1528087854385376\n",
      "\tLoss: 0.14988312125205994\n",
      "\tLoss: 0.11910536885261536\n",
      "\tLoss: 0.11413764953613281\n",
      "\tLoss: 0.10224514454603195\n",
      "\tLoss: 0.11308924853801727\n",
      "\tLoss: 0.18892242014408112\n",
      "\tLoss: 0.10168246924877167\n",
      "\tLoss: 0.11535613238811493\n",
      "\tLoss: 0.10887555032968521\n",
      "\tLoss: 0.12359879165887833\n",
      "\tLoss: 0.15051302313804626\n",
      "\tLoss: 0.11736232042312622\n",
      "\tLoss: 0.13413256406784058\n",
      "\tLoss: 0.17498943209648132\n",
      "\tLoss: 0.11199143528938293\n",
      "\tLoss: 0.08547016978263855\n",
      "\tLoss: 0.1225072517991066\n",
      "\tLoss: 0.06601724773645401\n",
      "\tLoss: 0.16250665485858917\n",
      "\tLoss: 0.13590633869171143\n",
      "\tLoss: 0.10754076391458511\n",
      "\tLoss: 0.09451447427272797\n",
      "\tLoss: 0.17152078449726105\n",
      "\tLoss: 0.1182098388671875\n",
      "\tLoss: 0.16670848429203033\n",
      "\tLoss: 0.13764551281929016\n",
      "\tLoss: 0.10997794568538666\n",
      "\tLoss: 0.10233094543218613\n",
      "\tLoss: 0.10612596571445465\n",
      "\tLoss: 0.15783631801605225\n",
      "\tLoss: 0.24055135250091553\n",
      "\tLoss: 0.09988261759281158\n",
      "\tLoss: 0.12920384109020233\n",
      "\tLoss: 0.0824914425611496\n",
      "\tLoss: 0.16977493464946747\n",
      "\tLoss: 0.1481020152568817\n",
      "\tLoss: 0.10707448422908783\n",
      "\tLoss: 0.05767427012324333\n",
      "\tLoss: 0.21019265055656433\n",
      "\tLoss: 0.12700533866882324\n",
      "\tLoss: 0.1689019799232483\n",
      "\tLoss: 0.12406723946332932\n",
      "\tLoss: 0.13181526958942413\n",
      "\tLoss: 0.22072447836399078\n",
      "\tLoss: 0.08443133533000946\n",
      "\tLoss: 0.11468511819839478\n",
      "\tLoss: 0.16421864926815033\n",
      "\tLoss: 0.2051715850830078\n",
      "\tLoss: 0.07265491783618927\n",
      "\tLoss: 0.0899730771780014\n",
      "\tLoss: 0.11567006260156631\n",
      "\tLoss: 0.14316898584365845\n",
      "\tLoss: 0.17391034960746765\n",
      "\tLoss: 0.16836459934711456\n",
      "\tLoss: 0.18557412922382355\n",
      "[time] Epoch 11: 443.40554137481377s = 7.390092356246896m\n",
      "\n",
      "Epoch 12...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.08256995677947998\n",
      "\tLoss: 0.153449147939682\n",
      "\tLoss: 0.13720139861106873\n",
      "\tLoss: 0.1385829746723175\n",
      "\tLoss: 0.11612045764923096\n",
      "\tLoss: 0.12019874155521393\n",
      "\tLoss: 0.0985776036977768\n",
      "\tLoss: 0.14667409658432007\n",
      "\tLoss: 0.18727463483810425\n",
      "\tLoss: 0.12686270475387573\n",
      "\tLoss: 0.10959799587726593\n",
      "\tLoss: 0.124974824488163\n",
      "\tLoss: 0.1753508746623993\n",
      "\tLoss: 0.09431730210781097\n",
      "\tLoss: 0.12588465213775635\n",
      "\tLoss: 0.06751690804958344\n",
      "\tLoss: 0.0747389942407608\n",
      "\tLoss: 0.1427379846572876\n",
      "\tLoss: 0.12826257944107056\n",
      "\tLoss: 0.11510781198740005\n",
      "\tLoss: 0.10142355412244797\n",
      "\tLoss: 0.17681550979614258\n",
      "\tLoss: 0.08939303457736969\n",
      "\tLoss: 0.21172061562538147\n",
      "\tLoss: 0.17552483081817627\n",
      "\tLoss: 0.10132479667663574\n",
      "\tLoss: 0.13935771584510803\n",
      "\tLoss: 0.1283056139945984\n",
      "\tLoss: 0.1444946676492691\n",
      "\tLoss: 0.1722942590713501\n",
      "\tLoss: 0.1256914883852005\n",
      "\tLoss: 0.11642435938119888\n",
      "\tLoss: 0.11975659430027008\n",
      "\tLoss: 0.12319228053092957\n",
      "\tLoss: 0.2246938943862915\n",
      "\tLoss: 0.126967191696167\n",
      "\tLoss: 0.11844254285097122\n",
      "\tLoss: 0.075044184923172\n",
      "\tLoss: 0.10353495925664902\n",
      "\tLoss: 0.08451882749795914\n",
      "\tLoss: 0.08643385767936707\n",
      "\tLoss: 0.1348872184753418\n",
      "\tLoss: 0.12120701372623444\n",
      "\tLoss: 0.2079208344221115\n",
      "\tLoss: 0.12400539219379425\n",
      "\tLoss: 0.1600644886493683\n",
      "\tLoss: 0.13432542979717255\n",
      "\tLoss: 0.194667249917984\n",
      "\tLoss: 0.15691977739334106\n",
      "\tLoss: 0.09016643464565277\n",
      "\tLoss: 0.12499327957630157\n",
      "\tLoss: 0.13030457496643066\n",
      "\tLoss: 0.14384466409683228\n",
      "\tLoss: 0.241749107837677\n",
      "\tLoss: 0.13529907166957855\n",
      "\tLoss: 0.0806906521320343\n",
      "\tLoss: 0.16828714311122894\n",
      "\tLoss: 0.08857132494449615\n",
      "\tLoss: 0.05166696757078171\n",
      "\tLoss: 0.07177804410457611\n",
      "\tLoss: 0.16420742869377136\n",
      "\tLoss: 0.10015340149402618\n",
      "\tLoss: 0.10925746709108353\n",
      "\tLoss: 0.10959529876708984\n",
      "\tLoss: 0.18554304540157318\n",
      "\tLoss: 0.1909509003162384\n",
      "\tLoss: 0.13778650760650635\n",
      "\tLoss: 0.13078612089157104\n",
      "\tLoss: 0.08284713327884674\n",
      "\tLoss: 0.11295115947723389\n",
      "\tLoss: 0.11966213583946228\n",
      "\tLoss: 0.12011075019836426\n",
      "\tLoss: 0.10619525611400604\n",
      "\tLoss: 0.07637231051921844\n",
      "\tLoss: 0.16133108735084534\n",
      "\tLoss: 0.1393754780292511\n",
      "\tLoss: 0.10262451320886612\n",
      "\tLoss: 0.1877465695142746\n",
      "\tLoss: 0.13574352860450745\n",
      "\tLoss: 0.1314965784549713\n",
      "\tLoss: 0.08709608018398285\n",
      "\tLoss: 0.1375783085823059\n",
      "\tLoss: 0.12904000282287598\n",
      "\tLoss: 0.12906163930892944\n",
      "\tLoss: 0.16214153170585632\n",
      "\tLoss: 0.1408877819776535\n",
      "\tLoss: 0.19452540576457977\n",
      "\tLoss: 0.16000300645828247\n",
      "\tLoss: 0.08234430104494095\n",
      "\tLoss: 0.13656750321388245\n",
      "\tLoss: 0.1294797658920288\n",
      "\tLoss: 0.18587127327919006\n",
      "\tLoss: 0.09427908807992935\n",
      "\tLoss: 0.1851344108581543\n",
      "\tLoss: 0.17069341242313385\n",
      "\tLoss: 0.08237588405609131\n",
      "\tLoss: 0.14777979254722595\n",
      "\tLoss: 0.1378726065158844\n",
      "\tLoss: 0.1835555136203766\n",
      "\tLoss: 0.14797647297382355\n",
      "\tLoss: 0.13269074261188507\n",
      "\tLoss: 0.181950181722641\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.061867427080869675\n",
      "\tLoss: 0.16683202981948853\n",
      "\tLoss: 0.12188577651977539\n",
      "\tLoss: 0.13138538599014282\n",
      "\tLoss: 0.0999450534582138\n",
      "\tLoss: 0.0845329612493515\n",
      "\tLoss: 0.11648332327604294\n",
      "\tLoss: 0.10596777498722076\n",
      "\tLoss: 0.09408411383628845\n",
      "\tLoss: 0.12131017446517944\n",
      "\tLoss: 0.12786215543746948\n",
      "\tLoss: 0.13592009246349335\n",
      "\tLoss: 0.1648174524307251\n",
      "\tLoss: 0.09659310430288315\n",
      "\tLoss: 0.13050955533981323\n",
      "\tLoss: 0.1348876953125\n",
      "\tLoss: 0.08801567554473877\n",
      "\tLoss: 0.11327549070119858\n",
      "\tLoss: 0.13076600432395935\n",
      "\tLoss: 0.17921088635921478\n",
      "\tLoss: 0.12938763201236725\n",
      "\tLoss: 0.13177995383739471\n",
      "\tLoss: 0.17021320760250092\n",
      "\tLoss: 0.17947450280189514\n",
      "\tLoss: 0.11637882143259048\n",
      "\tLoss: 0.12438350915908813\n",
      "\tLoss: 0.10998785495758057\n",
      "\tLoss: 0.11119157075881958\n",
      "\tLoss: 0.12339217215776443\n",
      "\tLoss: 0.13880398869514465\n",
      "\tLoss: 0.09860510379076004\n",
      "\tLoss: 0.10388042032718658\n",
      "\tLoss: 0.1589023619890213\n",
      "\tLoss: 0.11677461117506027\n",
      "\tLoss: 0.14474189281463623\n",
      "\tLoss: 0.12531915307044983\n",
      "\tLoss: 0.18496805429458618\n",
      "\tLoss: 0.09603220224380493\n",
      "\tLoss: 0.11969183385372162\n",
      "\tLoss: 0.1448848992586136\n",
      "\tLoss: 0.0721215009689331\n",
      "\tLoss: 0.11418701708316803\n",
      "\tLoss: 0.10008158534765244\n",
      "\tLoss: 0.10787900537252426\n",
      "\tLoss: 0.11221449822187424\n",
      "\tLoss: 0.1468493938446045\n",
      "\tLoss: 0.10018233209848404\n",
      "\tLoss: 0.11996016651391983\n",
      "\tLoss: 0.14190396666526794\n",
      "\tLoss: 0.09329654276371002\n",
      "\tLoss: 0.09989634156227112\n",
      "\tLoss: 0.15499697625637054\n",
      "\tLoss: 0.13233430683612823\n",
      "\tLoss: 0.13594889640808105\n",
      "\tLoss: 0.1190326139330864\n",
      "\tLoss: 0.07220795750617981\n",
      "\tLoss: 0.1420489102602005\n",
      "\tLoss: 0.0906628742814064\n",
      "\tLoss: 0.1538279950618744\n",
      "\tLoss: 0.13113829493522644\n",
      "\tLoss: 0.13375723361968994\n",
      "\tLoss: 0.13120096921920776\n",
      "\tLoss: 0.11133921891450882\n",
      "\tLoss: 0.14388003945350647\n",
      "\tLoss: 0.11865682154893875\n",
      "\tLoss: 0.1530720293521881\n",
      "\tLoss: 0.14544865489006042\n",
      "\tLoss: 0.09622757881879807\n",
      "\tLoss: 0.08635178208351135\n",
      "\tLoss: 0.08655732125043869\n",
      "\tLoss: 0.18014061450958252\n",
      "\tLoss: 0.1099233329296112\n",
      "\tLoss: 0.17793256044387817\n",
      "\tLoss: 0.11790884286165237\n",
      "\tLoss: 0.15696261823177338\n",
      "\tLoss: 0.07472211867570877\n",
      "\tLoss: 0.13109928369522095\n",
      "\tLoss: 0.11965645849704742\n",
      "\tLoss: 0.0934394896030426\n",
      "\tLoss: 0.13969430327415466\n",
      "\tLoss: 0.08149412274360657\n",
      "\tLoss: 0.1622825562953949\n",
      "\tLoss: 0.1340414434671402\n",
      "\tLoss: 0.19177746772766113\n",
      "\tLoss: 0.12910893559455872\n",
      "\tLoss: 0.13989846408367157\n",
      "\tLoss: 0.11940285563468933\n",
      "\tLoss: 0.1803617775440216\n",
      "\tLoss: 0.13433337211608887\n",
      "\tLoss: 0.1905982345342636\n",
      "\tLoss: 0.19261294603347778\n",
      "\tLoss: 0.12855061888694763\n",
      "\tLoss: 0.14574643969535828\n",
      "\tLoss: 0.14316287636756897\n",
      "\tLoss: 0.10063515603542328\n",
      "\tLoss: 0.11355047672986984\n",
      "\tLoss: 0.12517696619033813\n",
      "\tLoss: 0.16934895515441895\n",
      "\tLoss: 0.13256129622459412\n",
      "\tLoss: 0.12017577141523361\n",
      "\tLoss: 0.13067440688610077\n",
      "\tLoss: 0.11203902214765549\n",
      "[time] Epoch 12: 434.8167573623359s = 7.246945956038932m\n",
      "\n",
      "Epoch 13...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.19451791048049927\n",
      "\tLoss: 0.13917960226535797\n",
      "\tLoss: 0.08959194272756577\n",
      "\tLoss: 0.10436851531267166\n",
      "\tLoss: 0.19837939739227295\n",
      "\tLoss: 0.15342438220977783\n",
      "\tLoss: 0.13163770735263824\n",
      "\tLoss: 0.10676111280918121\n",
      "\tLoss: 0.15107640624046326\n",
      "\tLoss: 0.11577440053224564\n",
      "\tLoss: 0.0807109996676445\n",
      "\tLoss: 0.16591309010982513\n",
      "\tLoss: 0.08770529925823212\n",
      "\tLoss: 0.12835709750652313\n",
      "\tLoss: 0.12246648967266083\n",
      "\tLoss: 0.11232773959636688\n",
      "\tLoss: 0.10842181742191315\n",
      "\tLoss: 0.12195262312889099\n",
      "\tLoss: 0.20131388306617737\n",
      "\tLoss: 0.10237358510494232\n",
      "\tLoss: 0.1313892900943756\n",
      "\tLoss: 0.13327698409557343\n",
      "\tLoss: 0.11512133479118347\n",
      "\tLoss: 0.15102660655975342\n",
      "\tLoss: 0.1555359959602356\n",
      "\tLoss: 0.16039809584617615\n",
      "\tLoss: 0.14186584949493408\n",
      "\tLoss: 0.1637263149023056\n",
      "\tLoss: 0.12426884472370148\n",
      "\tLoss: 0.18130414187908173\n",
      "\tLoss: 0.12514278292655945\n",
      "\tLoss: 0.07439260929822922\n",
      "\tLoss: 0.10369358956813812\n",
      "\tLoss: 0.1313701570034027\n",
      "\tLoss: 0.16551625728607178\n",
      "\tLoss: 0.17281508445739746\n",
      "\tLoss: 0.1798429638147354\n",
      "\tLoss: 0.1440902054309845\n",
      "\tLoss: 0.11151283979415894\n",
      "\tLoss: 0.11174209415912628\n",
      "\tLoss: 0.09910207986831665\n",
      "\tLoss: 0.1013394370675087\n",
      "\tLoss: 0.09177456796169281\n",
      "\tLoss: 0.12788328528404236\n",
      "\tLoss: 0.11163944005966187\n",
      "\tLoss: 0.11074042320251465\n",
      "\tLoss: 0.10146155208349228\n",
      "\tLoss: 0.10586292296648026\n",
      "\tLoss: 0.19860881567001343\n",
      "\tLoss: 0.15685978531837463\n",
      "\tLoss: 0.20031750202178955\n",
      "\tLoss: 0.11146590113639832\n",
      "\tLoss: 0.14500021934509277\n",
      "\tLoss: 0.12363657355308533\n",
      "\tLoss: 0.18838617205619812\n",
      "\tLoss: 0.1730622500181198\n",
      "\tLoss: 0.13067421317100525\n",
      "\tLoss: 0.080205038189888\n",
      "\tLoss: 0.13867950439453125\n",
      "\tLoss: 0.16019660234451294\n",
      "\tLoss: 0.16476237773895264\n",
      "\tLoss: 0.1023724377155304\n",
      "\tLoss: 0.11987185478210449\n",
      "\tLoss: 0.08480044454336166\n",
      "\tLoss: 0.11495476216077805\n",
      "\tLoss: 0.1609208583831787\n",
      "\tLoss: 0.11260898411273956\n",
      "\tLoss: 0.1206364780664444\n",
      "\tLoss: 0.15041479468345642\n",
      "\tLoss: 0.1912069469690323\n",
      "\tLoss: 0.11605870723724365\n",
      "\tLoss: 0.10449934750795364\n",
      "\tLoss: 0.10920341312885284\n",
      "\tLoss: 0.16429509222507477\n",
      "\tLoss: 0.09159143269062042\n",
      "\tLoss: 0.13054797053337097\n",
      "\tLoss: 0.07576210796833038\n",
      "\tLoss: 0.12236671894788742\n",
      "\tLoss: 0.19269558787345886\n",
      "\tLoss: 0.09917540103197098\n",
      "\tLoss: 0.16443616151809692\n",
      "\tLoss: 0.12246526777744293\n",
      "\tLoss: 0.106740802526474\n",
      "\tLoss: 0.1523747444152832\n",
      "\tLoss: 0.16060718894004822\n",
      "\tLoss: 0.18144169449806213\n",
      "\tLoss: 0.15848784148693085\n",
      "\tLoss: 0.09851738810539246\n",
      "\tLoss: 0.1210358589887619\n",
      "\tLoss: 0.12347544729709625\n",
      "\tLoss: 0.10926875472068787\n",
      "\tLoss: 0.14177314937114716\n",
      "\tLoss: 0.16102173924446106\n",
      "\tLoss: 0.06850112229585648\n",
      "\tLoss: 0.10309317708015442\n",
      "\tLoss: 0.09671711176633835\n",
      "\tLoss: 0.09787305444478989\n",
      "\tLoss: 0.11960071325302124\n",
      "\tLoss: 0.13430653512477875\n",
      "\tLoss: 0.11519128084182739\n",
      "\tLoss: 0.1192835122346878\n",
      "\tLoss: 0.09531084448099136\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.1346714198589325\n",
      "\tLoss: 0.13036032021045685\n",
      "\tLoss: 0.13158002495765686\n",
      "\tLoss: 0.20522180199623108\n",
      "\tLoss: 0.07258628308773041\n",
      "\tLoss: 0.1845402717590332\n",
      "\tLoss: 0.1642330288887024\n",
      "\tLoss: 0.08521051704883575\n",
      "\tLoss: 0.12512138485908508\n",
      "\tLoss: 0.10260988026857376\n",
      "\tLoss: 0.1400338113307953\n",
      "\tLoss: 0.15083733201026917\n",
      "\tLoss: 0.21410709619522095\n",
      "\tLoss: 0.12020652741193771\n",
      "\tLoss: 0.10937151312828064\n",
      "\tLoss: 0.1429867148399353\n",
      "\tLoss: 0.12470335513353348\n",
      "\tLoss: 0.12297985702753067\n",
      "\tLoss: 0.12734781205654144\n",
      "\tLoss: 0.12584513425827026\n",
      "\tLoss: 0.15648165345191956\n",
      "\tLoss: 0.14964185655117035\n",
      "\tLoss: 0.11296728253364563\n",
      "\tLoss: 0.11391619592905045\n",
      "\tLoss: 0.11267393827438354\n",
      "\tLoss: 0.1395764946937561\n",
      "\tLoss: 0.06974665820598602\n",
      "\tLoss: 0.0859484151005745\n",
      "\tLoss: 0.1467161774635315\n",
      "\tLoss: 0.09942814707756042\n",
      "\tLoss: 0.1741792857646942\n",
      "\tLoss: 0.09747637808322906\n",
      "\tLoss: 0.144049271941185\n",
      "\tLoss: 0.139032244682312\n",
      "\tLoss: 0.08718977123498917\n",
      "\tLoss: 0.0991847962141037\n",
      "\tLoss: 0.16716887056827545\n",
      "\tLoss: 0.11970528960227966\n",
      "\tLoss: 0.10408923029899597\n",
      "\tLoss: 0.1665608286857605\n",
      "\tLoss: 0.13811227679252625\n",
      "\tLoss: 0.11174722015857697\n",
      "\tLoss: 0.16332407295703888\n",
      "\tLoss: 0.0996980145573616\n",
      "\tLoss: 0.14664125442504883\n",
      "\tLoss: 0.1682395488023758\n",
      "\tLoss: 0.12031988054513931\n",
      "\tLoss: 0.09756382554769516\n",
      "\tLoss: 0.183273583650589\n",
      "\tLoss: 0.11751984059810638\n",
      "\tLoss: 0.13330477476119995\n",
      "\tLoss: 0.11650563776493073\n",
      "\tLoss: 0.15457403659820557\n",
      "\tLoss: 0.20788860321044922\n",
      "\tLoss: 0.1369519829750061\n",
      "\tLoss: 0.13175618648529053\n",
      "\tLoss: 0.1499285250902176\n",
      "\tLoss: 0.16584622859954834\n",
      "\tLoss: 0.13722598552703857\n",
      "\tLoss: 0.09308605641126633\n",
      "\tLoss: 0.08981449902057648\n",
      "\tLoss: 0.13225553929805756\n",
      "\tLoss: 0.15915730595588684\n",
      "\tLoss: 0.14204424619674683\n",
      "\tLoss: 0.17166973650455475\n",
      "\tLoss: 0.11831395328044891\n",
      "\tLoss: 0.07998229563236237\n",
      "\tLoss: 0.1131245344877243\n",
      "\tLoss: 0.15864042937755585\n",
      "\tLoss: 0.10984312742948532\n",
      "\tLoss: 0.12441641092300415\n",
      "\tLoss: 0.09934588521718979\n",
      "\tLoss: 0.07592318207025528\n",
      "\tLoss: 0.12969718873500824\n",
      "\tLoss: 0.17433148622512817\n",
      "\tLoss: 0.07455216348171234\n",
      "\tLoss: 0.14290861785411835\n",
      "\tLoss: 0.19493506848812103\n",
      "\tLoss: 0.095458984375\n",
      "\tLoss: 0.1179191917181015\n",
      "\tLoss: 0.14947599172592163\n",
      "\tLoss: 0.13484907150268555\n",
      "\tLoss: 0.20390009880065918\n",
      "\tLoss: 0.1626933068037033\n",
      "\tLoss: 0.11611885577440262\n",
      "\tLoss: 0.1312507688999176\n",
      "\tLoss: 0.13817742466926575\n",
      "\tLoss: 0.11776106059551239\n",
      "\tLoss: 0.08149638772010803\n",
      "\tLoss: 0.13412350416183472\n",
      "\tLoss: 0.09760159254074097\n",
      "\tLoss: 0.12523895502090454\n",
      "\tLoss: 0.1426536738872528\n",
      "\tLoss: 0.07443685084581375\n",
      "\tLoss: 0.08979636430740356\n",
      "\tLoss: 0.12740826606750488\n",
      "\tLoss: 0.13653215765953064\n",
      "\tLoss: 0.17423170804977417\n",
      "\tLoss: 0.11597941070795059\n",
      "\tLoss: 0.13789910078048706\n",
      "\tLoss: 0.2135477364063263\n",
      "\tLoss: 0.1185721680521965\n",
      "[time] Epoch 13: 437.939655427821s = 7.298994257130349m\n",
      "\n",
      "Epoch 14...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.10479722172021866\n",
      "\tLoss: 0.20043553411960602\n",
      "\tLoss: 0.12720203399658203\n",
      "\tLoss: 0.11443280428647995\n",
      "\tLoss: 0.1151641234755516\n",
      "\tLoss: 0.15209969878196716\n",
      "\tLoss: 0.1430840939283371\n",
      "\tLoss: 0.1592157483100891\n",
      "\tLoss: 0.14826840162277222\n",
      "\tLoss: 0.14014899730682373\n",
      "\tLoss: 0.09617824852466583\n",
      "\tLoss: 0.08969055861234665\n",
      "\tLoss: 0.09600518643856049\n",
      "\tLoss: 0.1648596227169037\n",
      "\tLoss: 0.18695227801799774\n",
      "\tLoss: 0.1531040221452713\n",
      "\tLoss: 0.1281927227973938\n",
      "\tLoss: 0.17966797947883606\n",
      "\tLoss: 0.1425984650850296\n",
      "\tLoss: 0.09339898079633713\n",
      "\tLoss: 0.10978623479604721\n",
      "\tLoss: 0.13349321484565735\n",
      "\tLoss: 0.1495085060596466\n",
      "\tLoss: 0.1213274747133255\n",
      "\tLoss: 0.08255042880773544\n",
      "\tLoss: 0.13853494822978973\n",
      "\tLoss: 0.16053339838981628\n",
      "\tLoss: 0.1893502175807953\n",
      "\tLoss: 0.10392388701438904\n",
      "\tLoss: 0.11033792793750763\n",
      "\tLoss: 0.1336086392402649\n",
      "\tLoss: 0.12327184528112411\n",
      "\tLoss: 0.15221059322357178\n",
      "\tLoss: 0.07361408323049545\n",
      "\tLoss: 0.09573729336261749\n",
      "\tLoss: 0.17748919129371643\n",
      "\tLoss: 0.16185691952705383\n",
      "\tLoss: 0.09599249809980392\n",
      "\tLoss: 0.10878205299377441\n",
      "\tLoss: 0.15308168530464172\n",
      "\tLoss: 0.14158011972904205\n",
      "\tLoss: 0.16802003979682922\n",
      "\tLoss: 0.11758673936128616\n",
      "\tLoss: 0.19300472736358643\n",
      "\tLoss: 0.09077946841716766\n",
      "\tLoss: 0.1271296739578247\n",
      "\tLoss: 0.10830019414424896\n",
      "\tLoss: 0.07917460799217224\n",
      "\tLoss: 0.1351546049118042\n",
      "\tLoss: 0.10917583853006363\n",
      "\tLoss: 0.1536465585231781\n",
      "\tLoss: 0.1569509208202362\n",
      "\tLoss: 0.10903953015804291\n",
      "\tLoss: 0.1010778620839119\n",
      "\tLoss: 0.1627110093832016\n",
      "\tLoss: 0.12161870300769806\n",
      "\tLoss: 0.1867717206478119\n",
      "\tLoss: 0.08273430913686752\n",
      "\tLoss: 0.13984276354312897\n",
      "\tLoss: 0.08492106199264526\n",
      "\tLoss: 0.1479474902153015\n",
      "\tLoss: 0.12334483861923218\n",
      "\tLoss: 0.08312089741230011\n",
      "\tLoss: 0.1449800729751587\n",
      "\tLoss: 0.12486173212528229\n",
      "\tLoss: 0.16250431537628174\n",
      "\tLoss: 0.1344575583934784\n",
      "\tLoss: 0.09002505242824554\n",
      "\tLoss: 0.10911047458648682\n",
      "\tLoss: 0.08677367866039276\n",
      "\tLoss: 0.11544091254472733\n",
      "\tLoss: 0.14812412858009338\n",
      "\tLoss: 0.18715745210647583\n",
      "\tLoss: 0.09317223727703094\n",
      "\tLoss: 0.0916518121957779\n",
      "\tLoss: 0.1609831154346466\n",
      "\tLoss: 0.09289330244064331\n",
      "\tLoss: 0.1897045224905014\n",
      "\tLoss: 0.16761478781700134\n",
      "\tLoss: 0.10579997301101685\n",
      "\tLoss: 0.06760130822658539\n",
      "\tLoss: 0.15005497634410858\n",
      "\tLoss: 0.09264090657234192\n",
      "\tLoss: 0.18576452136039734\n",
      "\tLoss: 0.0880684033036232\n",
      "\tLoss: 0.12073467671871185\n",
      "\tLoss: 0.10848170518875122\n",
      "\tLoss: 0.1451430320739746\n",
      "\tLoss: 0.1687081903219223\n",
      "\tLoss: 0.12276461720466614\n",
      "\tLoss: 0.08330339193344116\n",
      "\tLoss: 0.0906706154346466\n",
      "\tLoss: 0.14157059788703918\n",
      "\tLoss: 0.09926614165306091\n",
      "\tLoss: 0.11260413378477097\n",
      "\tLoss: 0.0779784619808197\n",
      "\tLoss: 0.1422262340784073\n",
      "\tLoss: 0.13898172974586487\n",
      "\tLoss: 0.16602164506912231\n",
      "\tLoss: 0.1893215775489807\n",
      "\tLoss: 0.14021676778793335\n",
      "\tLoss: 0.12624217569828033\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.13320118188858032\n",
      "\tLoss: 0.08587511628866196\n",
      "\tLoss: 0.14674347639083862\n",
      "\tLoss: 0.10214170813560486\n",
      "\tLoss: 0.18386155366897583\n",
      "\tLoss: 0.11301504820585251\n",
      "\tLoss: 0.11989343911409378\n",
      "\tLoss: 0.08985979110002518\n",
      "\tLoss: 0.1425369381904602\n",
      "\tLoss: 0.0929945707321167\n",
      "\tLoss: 0.1609927862882614\n",
      "\tLoss: 0.11933425068855286\n",
      "\tLoss: 0.13677096366882324\n",
      "\tLoss: 0.07989298552274704\n",
      "\tLoss: 0.08647029101848602\n",
      "\tLoss: 0.12699854373931885\n",
      "\tLoss: 0.12658269703388214\n",
      "\tLoss: 0.1156260296702385\n",
      "\tLoss: 0.12365437299013138\n",
      "\tLoss: 0.10694864392280579\n",
      "\tLoss: 0.06258892267942429\n",
      "\tLoss: 0.14276203513145447\n",
      "\tLoss: 0.0712229534983635\n",
      "\tLoss: 0.15924619138240814\n",
      "\tLoss: 0.11708921939134598\n",
      "\tLoss: 0.14969104528427124\n",
      "\tLoss: 0.153917133808136\n",
      "\tLoss: 0.1037282794713974\n",
      "\tLoss: 0.16601042449474335\n",
      "\tLoss: 0.08536285161972046\n",
      "\tLoss: 0.13581179082393646\n",
      "\tLoss: 0.08971458673477173\n",
      "\tLoss: 0.11431067436933517\n",
      "\tLoss: 0.08423184603452682\n",
      "\tLoss: 0.06997282803058624\n",
      "\tLoss: 0.1592196375131607\n",
      "\tLoss: 0.11025982350111008\n",
      "\tLoss: 0.13078677654266357\n",
      "\tLoss: 0.10243608057498932\n",
      "\tLoss: 0.14032384753227234\n",
      "\tLoss: 0.10608770698308945\n",
      "\tLoss: 0.16553576290607452\n",
      "\tLoss: 0.1492244005203247\n",
      "\tLoss: 0.12190653383731842\n",
      "\tLoss: 0.09085667133331299\n",
      "\tLoss: 0.13216820359230042\n",
      "\tLoss: 0.0970410481095314\n",
      "\tLoss: 0.11042241752147675\n",
      "\tLoss: 0.16572293639183044\n",
      "\tLoss: 0.1192457526922226\n",
      "\tLoss: 0.14590829610824585\n",
      "\tLoss: 0.13338851928710938\n",
      "\tLoss: 0.17091771960258484\n",
      "\tLoss: 0.15719033777713776\n",
      "\tLoss: 0.11951832473278046\n",
      "\tLoss: 0.16361191868782043\n",
      "\tLoss: 0.11206653714179993\n",
      "\tLoss: 0.1871201992034912\n",
      "\tLoss: 0.1110716387629509\n",
      "\tLoss: 0.16473181545734406\n",
      "\tLoss: 0.15059152245521545\n",
      "\tLoss: 0.10770571231842041\n",
      "\tLoss: 0.1463364064693451\n",
      "\tLoss: 0.17063838243484497\n",
      "\tLoss: 0.14608485996723175\n",
      "\tLoss: 0.13242405652999878\n",
      "\tLoss: 0.12310410290956497\n",
      "\tLoss: 0.08688072860240936\n",
      "\tLoss: 0.0642208456993103\n",
      "\tLoss: 0.08992309123277664\n",
      "\tLoss: 0.18551865220069885\n",
      "\tLoss: 0.10154694318771362\n",
      "\tLoss: 0.09179163724184036\n",
      "\tLoss: 0.133230060338974\n",
      "\tLoss: 0.12545102834701538\n",
      "\tLoss: 0.05826878547668457\n",
      "\tLoss: 0.15824459493160248\n",
      "\tLoss: 0.17125791311264038\n",
      "\tLoss: 0.062334299087524414\n",
      "\tLoss: 0.09497368335723877\n",
      "\tLoss: 0.16948345303535461\n",
      "\tLoss: 0.11293154209852219\n",
      "\tLoss: 0.16756996512413025\n",
      "\tLoss: 0.1605360358953476\n",
      "\tLoss: 0.16296154260635376\n",
      "\tLoss: 0.12067277729511261\n",
      "\tLoss: 0.162748783826828\n",
      "\tLoss: 0.08127302676439285\n",
      "\tLoss: 0.11735247075557709\n",
      "\tLoss: 0.17299842834472656\n",
      "\tLoss: 0.08880876004695892\n",
      "\tLoss: 0.14806735515594482\n",
      "\tLoss: 0.15525180101394653\n",
      "\tLoss: 0.14194722473621368\n",
      "\tLoss: 0.1601811647415161\n",
      "\tLoss: 0.14212432503700256\n",
      "\tLoss: 0.11065268516540527\n",
      "\tLoss: 0.21616332232952118\n",
      "\tLoss: 0.11123895645141602\n",
      "\tLoss: 0.1626078337430954\n",
      "\tLoss: 0.14583465456962585\n",
      "\tLoss: 0.1137363538146019\n",
      "[time] Epoch 14: 431.8200779571198s = 7.19700129928533m\n",
      "\n",
      "Epoch 15...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.09206069260835648\n",
      "\tLoss: 0.08393310010433197\n",
      "\tLoss: 0.11948510259389877\n",
      "\tLoss: 0.06700510531663895\n",
      "\tLoss: 0.12033777683973312\n",
      "\tLoss: 0.11716942489147186\n",
      "\tLoss: 0.17243421077728271\n",
      "\tLoss: 0.09697536379098892\n",
      "\tLoss: 0.08203809708356857\n",
      "\tLoss: 0.1501365303993225\n",
      "\tLoss: 0.14629808068275452\n",
      "\tLoss: 0.11228571832180023\n",
      "\tLoss: 0.15357497334480286\n",
      "\tLoss: 0.12485311180353165\n",
      "\tLoss: 0.11308035254478455\n",
      "\tLoss: 0.09054842591285706\n",
      "\tLoss: 0.10931287705898285\n",
      "\tLoss: 0.13390053808689117\n",
      "\tLoss: 0.1545822024345398\n",
      "\tLoss: 0.15522107481956482\n",
      "\tLoss: 0.12832534313201904\n",
      "\tLoss: 0.07125148177146912\n",
      "\tLoss: 0.1345265656709671\n",
      "\tLoss: 0.0944363921880722\n",
      "\tLoss: 0.08596841990947723\n",
      "\tLoss: 0.16955609619617462\n",
      "\tLoss: 0.07637233287096024\n",
      "\tLoss: 0.1527692675590515\n",
      "\tLoss: 0.12909629940986633\n",
      "\tLoss: 0.1496662199497223\n",
      "\tLoss: 0.21126911044120789\n",
      "\tLoss: 0.11964219808578491\n",
      "\tLoss: 0.1363440304994583\n",
      "\tLoss: 0.11160953342914581\n",
      "\tLoss: 0.09796395897865295\n",
      "\tLoss: 0.15308135747909546\n",
      "\tLoss: 0.07618002593517303\n",
      "\tLoss: 0.11666755378246307\n",
      "\tLoss: 0.13434040546417236\n",
      "\tLoss: 0.15617096424102783\n",
      "\tLoss: 0.13903245329856873\n",
      "\tLoss: 0.18399132788181305\n",
      "\tLoss: 0.15760844945907593\n",
      "\tLoss: 0.08881598711013794\n",
      "\tLoss: 0.10308727622032166\n",
      "\tLoss: 0.11548536270856857\n",
      "\tLoss: 0.1598844826221466\n",
      "\tLoss: 0.15902841091156006\n",
      "\tLoss: 0.0937170684337616\n",
      "\tLoss: 0.12408424913883209\n",
      "\tLoss: 0.08538877964019775\n",
      "\tLoss: 0.07908155024051666\n",
      "\tLoss: 0.11259294301271439\n",
      "\tLoss: 0.19103798270225525\n",
      "\tLoss: 0.17375274002552032\n",
      "\tLoss: 0.14603610336780548\n",
      "\tLoss: 0.11919142305850983\n",
      "\tLoss: 0.1254485547542572\n",
      "\tLoss: 0.1141570657491684\n",
      "\tLoss: 0.09727182984352112\n",
      "\tLoss: 0.1444682776927948\n",
      "\tLoss: 0.14792586863040924\n",
      "\tLoss: 0.18526512384414673\n",
      "\tLoss: 0.08543243259191513\n",
      "\tLoss: 0.12744323909282684\n",
      "\tLoss: 0.12005091458559036\n",
      "\tLoss: 0.10190267860889435\n",
      "\tLoss: 0.14388449490070343\n",
      "\tLoss: 0.13950102031230927\n",
      "\tLoss: 0.13214007019996643\n",
      "\tLoss: 0.09168446063995361\n",
      "\tLoss: 0.16167211532592773\n",
      "\tLoss: 0.11471159756183624\n",
      "\tLoss: 0.11692158132791519\n",
      "\tLoss: 0.15440621972084045\n",
      "\tLoss: 0.1016068309545517\n",
      "\tLoss: 0.15767116844654083\n",
      "\tLoss: 0.10471615195274353\n",
      "\tLoss: 0.19507896900177002\n",
      "\tLoss: 0.12044030427932739\n",
      "\tLoss: 0.19568710029125214\n",
      "\tLoss: 0.20592401921749115\n",
      "\tLoss: 0.156446173787117\n",
      "\tLoss: 0.1893681287765503\n",
      "\tLoss: 0.10024268925189972\n",
      "\tLoss: 0.09480687975883484\n",
      "\tLoss: 0.14753428101539612\n",
      "\tLoss: 0.13472719490528107\n",
      "\tLoss: 0.1955987513065338\n",
      "\tLoss: 0.11756765842437744\n",
      "\tLoss: 0.12399808317422867\n",
      "\tLoss: 0.10642075538635254\n",
      "\tLoss: 0.14405259490013123\n",
      "\tLoss: 0.13586236536502838\n",
      "\tLoss: 0.1381063163280487\n",
      "\tLoss: 0.11340077221393585\n",
      "\tLoss: 0.08950590342283249\n",
      "\tLoss: 0.16060572862625122\n",
      "\tLoss: 0.0988352820277214\n",
      "\tLoss: 0.14812636375427246\n",
      "\tLoss: 0.1184801384806633\n",
      "\tLoss: 0.12973739206790924\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.15883991122245789\n",
      "\tLoss: 0.22253689169883728\n",
      "\tLoss: 0.1499195545911789\n",
      "\tLoss: 0.152695894241333\n",
      "\tLoss: 0.12134893983602524\n",
      "\tLoss: 0.16323721408843994\n",
      "\tLoss: 0.1134786307811737\n",
      "\tLoss: 0.15615524351596832\n",
      "\tLoss: 0.11045807600021362\n",
      "\tLoss: 0.1219559758901596\n",
      "\tLoss: 0.14852425456047058\n",
      "\tLoss: 0.15288975834846497\n",
      "\tLoss: 0.10531166940927505\n",
      "\tLoss: 0.13586370646953583\n",
      "\tLoss: 0.0956055223941803\n",
      "\tLoss: 0.1391131430864334\n",
      "\tLoss: 0.056066691875457764\n",
      "\tLoss: 0.11782526969909668\n",
      "\tLoss: 0.10028474777936935\n",
      "\tLoss: 0.12700961530208588\n",
      "\tLoss: 0.1464727520942688\n",
      "\tLoss: 0.13385674357414246\n",
      "\tLoss: 0.14555902779102325\n",
      "\tLoss: 0.12965989112854004\n",
      "\tLoss: 0.10782621800899506\n",
      "\tLoss: 0.1056174635887146\n",
      "\tLoss: 0.12199050188064575\n",
      "\tLoss: 0.08845137059688568\n",
      "\tLoss: 0.14177580177783966\n",
      "\tLoss: 0.18633539974689484\n",
      "\tLoss: 0.14161118865013123\n",
      "\tLoss: 0.15740612149238586\n",
      "\tLoss: 0.13467979431152344\n",
      "\tLoss: 0.1916147768497467\n",
      "\tLoss: 0.09217878431081772\n",
      "\tLoss: 0.11265473812818527\n",
      "\tLoss: 0.12853755056858063\n",
      "\tLoss: 0.10811178386211395\n",
      "\tLoss: 0.14512178301811218\n",
      "\tLoss: 0.18068906664848328\n",
      "\tLoss: 0.11934871226549149\n",
      "\tLoss: 0.11292491853237152\n",
      "\tLoss: 0.12691080570220947\n",
      "\tLoss: 0.14388416707515717\n",
      "\tLoss: 0.13836556673049927\n",
      "\tLoss: 0.10286173969507217\n",
      "\tLoss: 0.11512796580791473\n",
      "\tLoss: 0.16351737082004547\n",
      "\tLoss: 0.06736301630735397\n",
      "\tLoss: 0.13534973561763763\n",
      "\tLoss: 0.10690819472074509\n",
      "\tLoss: 0.1302766650915146\n",
      "\tLoss: 0.10484004020690918\n",
      "\tLoss: 0.17761659622192383\n",
      "\tLoss: 0.12058942019939423\n",
      "\tLoss: 0.1732405424118042\n",
      "\tLoss: 0.1801360696554184\n",
      "\tLoss: 0.134906604886055\n",
      "\tLoss: 0.15083229541778564\n",
      "\tLoss: 0.1068686991930008\n",
      "\tLoss: 0.08569920063018799\n",
      "\tLoss: 0.1764771044254303\n",
      "\tLoss: 0.1213839128613472\n",
      "\tLoss: 0.1276167333126068\n",
      "\tLoss: 0.11084100604057312\n",
      "\tLoss: 0.1448969542980194\n",
      "\tLoss: 0.18622556328773499\n",
      "\tLoss: 0.0986304059624672\n",
      "\tLoss: 0.17482131719589233\n",
      "\tLoss: 0.09956790506839752\n",
      "\tLoss: 0.13880661129951477\n",
      "\tLoss: 0.10696408897638321\n",
      "\tLoss: 0.10273654758930206\n",
      "\tLoss: 0.13185849785804749\n",
      "\tLoss: 0.1882886439561844\n",
      "\tLoss: 0.12044018507003784\n",
      "\tLoss: 0.09304480254650116\n",
      "\tLoss: 0.10214503854513168\n",
      "\tLoss: 0.1351463794708252\n",
      "\tLoss: 0.1512918472290039\n",
      "\tLoss: 0.12032558768987656\n",
      "\tLoss: 0.09708182513713837\n",
      "\tLoss: 0.09116670489311218\n",
      "\tLoss: 0.09031009674072266\n",
      "\tLoss: 0.08629505336284637\n",
      "\tLoss: 0.1568637192249298\n",
      "\tLoss: 0.16395288705825806\n",
      "\tLoss: 0.09690359234809875\n",
      "\tLoss: 0.10236089676618576\n",
      "\tLoss: 0.19979797303676605\n",
      "\tLoss: 0.0903518870472908\n",
      "\tLoss: 0.13781079649925232\n",
      "\tLoss: 0.14142021536827087\n",
      "\tLoss: 0.12896379828453064\n",
      "\tLoss: 0.12672097980976105\n",
      "\tLoss: 0.15469640493392944\n",
      "\tLoss: 0.055374227464199066\n",
      "\tLoss: 0.09512829780578613\n",
      "\tLoss: 0.11910650134086609\n",
      "\tLoss: 0.12730923295021057\n",
      "\tLoss: 0.14151310920715332\n",
      "\tLoss: 0.08020338416099548\n",
      "[time] Epoch 15: 435.5438044583425s = 7.259063407639042m\n",
      "\n",
      "Epoch 16...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.09308657050132751\n",
      "\tLoss: 0.12055458873510361\n",
      "\tLoss: 0.11469565331935883\n",
      "\tLoss: 0.17662888765335083\n",
      "\tLoss: 0.15325595438480377\n",
      "\tLoss: 0.10503685474395752\n",
      "\tLoss: 0.19588494300842285\n",
      "\tLoss: 0.12525780498981476\n",
      "\tLoss: 0.07637985795736313\n",
      "\tLoss: 0.08491069823503494\n",
      "\tLoss: 0.12167589366436005\n",
      "\tLoss: 0.14895179867744446\n",
      "\tLoss: 0.08780631422996521\n",
      "\tLoss: 0.1504591405391693\n",
      "\tLoss: 0.12955820560455322\n",
      "\tLoss: 0.09543417394161224\n",
      "\tLoss: 0.06262058019638062\n",
      "\tLoss: 0.15839286148548126\n",
      "\tLoss: 0.13178935647010803\n",
      "\tLoss: 0.11227178573608398\n",
      "\tLoss: 0.09777870029211044\n",
      "\tLoss: 0.11293523013591766\n",
      "\tLoss: 0.09021137654781342\n",
      "\tLoss: 0.21457821130752563\n",
      "\tLoss: 0.10793065279722214\n",
      "\tLoss: 0.1324961930513382\n",
      "\tLoss: 0.14733976125717163\n",
      "\tLoss: 0.17070874571800232\n",
      "\tLoss: 0.0920630469918251\n",
      "\tLoss: 0.12865369021892548\n",
      "\tLoss: 0.15944071114063263\n",
      "\tLoss: 0.12426232546567917\n",
      "\tLoss: 0.14116771519184113\n",
      "\tLoss: 0.10117858648300171\n",
      "\tLoss: 0.17120060324668884\n",
      "\tLoss: 0.11880428344011307\n",
      "\tLoss: 0.16975054144859314\n",
      "\tLoss: 0.13534337282180786\n",
      "\tLoss: 0.10753943026065826\n",
      "\tLoss: 0.08093807101249695\n",
      "\tLoss: 0.08341915905475616\n",
      "\tLoss: 0.16836139559745789\n",
      "\tLoss: 0.13659626245498657\n",
      "\tLoss: 0.09501051157712936\n",
      "\tLoss: 0.11298935860395432\n",
      "\tLoss: 0.1457282453775406\n",
      "\tLoss: 0.10253776609897614\n",
      "\tLoss: 0.09660054743289948\n",
      "\tLoss: 0.10779211670160294\n",
      "\tLoss: 0.13824492692947388\n",
      "\tLoss: 0.09465082734823227\n",
      "\tLoss: 0.11462675034999847\n",
      "\tLoss: 0.11973316967487335\n",
      "\tLoss: 0.11395144462585449\n",
      "\tLoss: 0.1494157314300537\n",
      "\tLoss: 0.11496393382549286\n",
      "\tLoss: 0.11178233474493027\n",
      "\tLoss: 0.0859435647726059\n",
      "\tLoss: 0.1094227135181427\n",
      "\tLoss: 0.15573135018348694\n",
      "\tLoss: 0.09767549484968185\n",
      "\tLoss: 0.1480204463005066\n",
      "\tLoss: 0.13422372937202454\n",
      "\tLoss: 0.14768922328948975\n",
      "\tLoss: 0.1425010859966278\n",
      "\tLoss: 0.06350977718830109\n",
      "\tLoss: 0.1563432812690735\n",
      "\tLoss: 0.10397478938102722\n",
      "\tLoss: 0.09696565568447113\n",
      "\tLoss: 0.16742199659347534\n",
      "\tLoss: 0.09395016729831696\n",
      "\tLoss: 0.11174704879522324\n",
      "\tLoss: 0.15721489489078522\n",
      "\tLoss: 0.15286609530448914\n",
      "\tLoss: 0.11600089818239212\n",
      "\tLoss: 0.10467357933521271\n",
      "\tLoss: 0.16577628254890442\n",
      "\tLoss: 0.09694045782089233\n",
      "\tLoss: 0.13544045388698578\n",
      "\tLoss: 0.06455392390489578\n",
      "\tLoss: 0.1263841837644577\n",
      "\tLoss: 0.11299867928028107\n",
      "\tLoss: 0.11409047991037369\n",
      "\tLoss: 0.09723301231861115\n",
      "\tLoss: 0.11854982376098633\n",
      "\tLoss: 0.12470293045043945\n",
      "\tLoss: 0.14487910270690918\n",
      "\tLoss: 0.12125785648822784\n",
      "\tLoss: 0.12249332666397095\n",
      "\tLoss: 0.12522141635417938\n",
      "\tLoss: 0.14253412187099457\n",
      "\tLoss: 0.14774903655052185\n",
      "\tLoss: 0.08286896347999573\n",
      "\tLoss: 0.14406856894493103\n",
      "\tLoss: 0.11029339581727982\n",
      "\tLoss: 0.14364835619926453\n",
      "\tLoss: 0.05813472345471382\n",
      "\tLoss: 0.10144142061471939\n",
      "\tLoss: 0.06995761394500732\n",
      "\tLoss: 0.15026724338531494\n",
      "\tLoss: 0.08635423332452774\n",
      "\tLoss: 0.12067236006259918\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.11716102063655853\n",
      "\tLoss: 0.15181569755077362\n",
      "\tLoss: 0.09973622858524323\n",
      "\tLoss: 0.1392378956079483\n",
      "\tLoss: 0.13590863347053528\n",
      "\tLoss: 0.09602120518684387\n",
      "\tLoss: 0.10174794495105743\n",
      "\tLoss: 0.13731545209884644\n",
      "\tLoss: 0.0790906548500061\n",
      "\tLoss: 0.11559467017650604\n",
      "\tLoss: 0.12514597177505493\n",
      "\tLoss: 0.12975838780403137\n",
      "\tLoss: 0.10554871708154678\n",
      "\tLoss: 0.1357221007347107\n",
      "\tLoss: 0.1407431960105896\n",
      "\tLoss: 0.11308854818344116\n",
      "\tLoss: 0.13061293959617615\n",
      "\tLoss: 0.1614292711019516\n",
      "\tLoss: 0.11346198618412018\n",
      "\tLoss: 0.13370966911315918\n",
      "\tLoss: 0.1170826405286789\n",
      "\tLoss: 0.14373064041137695\n",
      "\tLoss: 0.09692808985710144\n",
      "\tLoss: 0.11633101105690002\n",
      "\tLoss: 0.12123444676399231\n",
      "\tLoss: 0.14306622743606567\n",
      "\tLoss: 0.105380117893219\n",
      "\tLoss: 0.14278273284435272\n",
      "\tLoss: 0.18821051716804504\n",
      "\tLoss: 0.11352389305830002\n",
      "\tLoss: 0.1392858922481537\n",
      "\tLoss: 0.0812673345208168\n",
      "\tLoss: 0.10568809509277344\n",
      "\tLoss: 0.16513380408287048\n",
      "\tLoss: 0.14064240455627441\n",
      "\tLoss: 0.14920082688331604\n",
      "\tLoss: 0.15903356671333313\n",
      "\tLoss: 0.09808890521526337\n",
      "\tLoss: 0.1333903819322586\n",
      "\tLoss: 0.14381065964698792\n",
      "\tLoss: 0.1323966085910797\n",
      "\tLoss: 0.13248924911022186\n",
      "\tLoss: 0.1602078676223755\n",
      "\tLoss: 0.07925575971603394\n",
      "\tLoss: 0.19192193448543549\n",
      "\tLoss: 0.12450259923934937\n",
      "\tLoss: 0.2155250459909439\n",
      "\tLoss: 0.09714843332767487\n",
      "\tLoss: 0.06583043932914734\n",
      "\tLoss: 0.10866552591323853\n",
      "\tLoss: 0.15326108038425446\n",
      "\tLoss: 0.15272019803524017\n",
      "\tLoss: 0.12079767882823944\n",
      "\tLoss: 0.12834012508392334\n",
      "\tLoss: 0.11083532124757767\n",
      "\tLoss: 0.09863421320915222\n",
      "\tLoss: 0.11340011656284332\n",
      "\tLoss: 0.10405206680297852\n",
      "\tLoss: 0.11543141305446625\n",
      "\tLoss: 0.13660353422164917\n",
      "\tLoss: 0.11487437784671783\n",
      "\tLoss: 0.13277369737625122\n",
      "\tLoss: 0.1561388522386551\n",
      "\tLoss: 0.08921206742525101\n",
      "\tLoss: 0.09713680297136307\n",
      "\tLoss: 0.11314018070697784\n",
      "\tLoss: 0.1414531022310257\n",
      "\tLoss: 0.1497555822134018\n",
      "\tLoss: 0.10436904430389404\n",
      "\tLoss: 0.10183210670948029\n",
      "\tLoss: 0.06312090903520584\n",
      "\tLoss: 0.12673383951187134\n",
      "\tLoss: 0.11890137195587158\n",
      "\tLoss: 0.09780752658843994\n",
      "\tLoss: 0.14700472354888916\n",
      "\tLoss: 0.12991923093795776\n",
      "\tLoss: 0.09162771701812744\n",
      "\tLoss: 0.13107943534851074\n",
      "\tLoss: 0.10845182090997696\n",
      "\tLoss: 0.12148865312337875\n",
      "\tLoss: 0.17440912127494812\n",
      "\tLoss: 0.20297616720199585\n",
      "\tLoss: 0.09440489858388901\n",
      "\tLoss: 0.13766378164291382\n",
      "\tLoss: 0.20385265350341797\n",
      "\tLoss: 0.13646763563156128\n",
      "\tLoss: 0.12077808380126953\n",
      "\tLoss: 0.10540845990180969\n",
      "\tLoss: 0.10899651050567627\n",
      "\tLoss: 0.16723844408988953\n",
      "\tLoss: 0.1349896639585495\n",
      "\tLoss: 0.09590592980384827\n",
      "\tLoss: 0.14812973141670227\n",
      "\tLoss: 0.1365426480770111\n",
      "\tLoss: 0.1318153291940689\n",
      "\tLoss: 0.1159282922744751\n",
      "\tLoss: 0.10041172057390213\n",
      "\tLoss: 0.07563977688550949\n",
      "\tLoss: 0.12694703042507172\n",
      "\tLoss: 0.09157856553792953\n",
      "\tLoss: 0.1946561336517334\n",
      "\tLoss: 0.15539231896400452\n",
      "[time] Epoch 16: 427.58611508691683s = 7.126435251448614m\n",
      "\n",
      "Epoch 17...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.09846233576536179\n",
      "\tLoss: 0.11484724283218384\n",
      "\tLoss: 0.07268212735652924\n",
      "\tLoss: 0.11397983133792877\n",
      "\tLoss: 0.1262023150920868\n",
      "\tLoss: 0.15364894270896912\n",
      "\tLoss: 0.10685092210769653\n",
      "\tLoss: 0.10987509042024612\n",
      "\tLoss: 0.14869703352451324\n",
      "\tLoss: 0.13446374237537384\n",
      "\tLoss: 0.11923197656869888\n",
      "\tLoss: 0.07565020769834518\n",
      "\tLoss: 0.1563408374786377\n",
      "\tLoss: 0.20420518517494202\n",
      "\tLoss: 0.13537639379501343\n",
      "\tLoss: 0.10954871028661728\n",
      "\tLoss: 0.19622421264648438\n",
      "\tLoss: 0.15888360142707825\n",
      "\tLoss: 0.11013361811637878\n",
      "\tLoss: 0.14488956332206726\n",
      "\tLoss: 0.13929562270641327\n",
      "\tLoss: 0.15882152318954468\n",
      "\tLoss: 0.1673721969127655\n",
      "\tLoss: 0.1178976446390152\n",
      "\tLoss: 0.0929170772433281\n",
      "\tLoss: 0.09632427245378494\n",
      "\tLoss: 0.13315948843955994\n",
      "\tLoss: 0.14930590987205505\n",
      "\tLoss: 0.17529520392417908\n",
      "\tLoss: 0.1928757280111313\n",
      "\tLoss: 0.12316039204597473\n",
      "\tLoss: 0.15602916479110718\n",
      "\tLoss: 0.17857958376407623\n",
      "\tLoss: 0.11720263957977295\n",
      "\tLoss: 0.17520877718925476\n",
      "\tLoss: 0.10473372042179108\n",
      "\tLoss: 0.1908717304468155\n",
      "\tLoss: 0.12022794783115387\n",
      "\tLoss: 0.16204477846622467\n",
      "\tLoss: 0.1302180290222168\n",
      "\tLoss: 0.10359083861112595\n",
      "\tLoss: 0.12735587358474731\n",
      "\tLoss: 0.10974167287349701\n",
      "\tLoss: 0.15678057074546814\n",
      "\tLoss: 0.17817266285419464\n",
      "\tLoss: 0.11905798316001892\n",
      "\tLoss: 0.03210458904504776\n",
      "\tLoss: 0.09654706716537476\n",
      "\tLoss: 0.10334205627441406\n",
      "\tLoss: 0.13196849822998047\n",
      "\tLoss: 0.13490146398544312\n",
      "\tLoss: 0.10036704689264297\n",
      "\tLoss: 0.10101718455553055\n",
      "\tLoss: 0.06750082969665527\n",
      "\tLoss: 0.06319466233253479\n",
      "\tLoss: 0.12769417464733124\n",
      "\tLoss: 0.12319683283567429\n",
      "\tLoss: 0.19903743267059326\n",
      "\tLoss: 0.0984644740819931\n",
      "\tLoss: 0.08495499938726425\n",
      "\tLoss: 0.1770487278699875\n",
      "\tLoss: 0.14149169623851776\n",
      "\tLoss: 0.13685522973537445\n",
      "\tLoss: 0.09636977314949036\n",
      "\tLoss: 0.11696343868970871\n",
      "\tLoss: 0.1525699943304062\n",
      "\tLoss: 0.1026298925280571\n",
      "\tLoss: 0.17745135724544525\n",
      "\tLoss: 0.13277073204517365\n",
      "\tLoss: 0.11181905120611191\n",
      "\tLoss: 0.08137916028499603\n",
      "\tLoss: 0.11212708055973053\n",
      "\tLoss: 0.1119309812784195\n",
      "\tLoss: 0.08716058731079102\n",
      "\tLoss: 0.13986313343048096\n",
      "\tLoss: 0.16037246584892273\n",
      "\tLoss: 0.11652825772762299\n",
      "\tLoss: 0.13164262473583221\n",
      "\tLoss: 0.22302916646003723\n",
      "\tLoss: 0.07519248127937317\n",
      "\tLoss: 0.08193755149841309\n",
      "\tLoss: 0.1628541797399521\n",
      "\tLoss: 0.0557047501206398\n",
      "\tLoss: 0.11978720128536224\n",
      "\tLoss: 0.08048725128173828\n",
      "\tLoss: 0.13793586194515228\n",
      "\tLoss: 0.1263626217842102\n",
      "\tLoss: 0.1364639699459076\n",
      "\tLoss: 0.09097537398338318\n",
      "\tLoss: 0.06417141854763031\n",
      "\tLoss: 0.08378066122531891\n",
      "\tLoss: 0.12524312734603882\n",
      "\tLoss: 0.1609450876712799\n",
      "\tLoss: 0.13329096138477325\n",
      "\tLoss: 0.18501557409763336\n",
      "\tLoss: 0.17581602931022644\n",
      "\tLoss: 0.14407657086849213\n",
      "\tLoss: 0.09928494691848755\n",
      "\tLoss: 0.1512945145368576\n",
      "\tLoss: 0.14723935723304749\n",
      "\tLoss: 0.1331651508808136\n",
      "\tLoss: 0.11740922927856445\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.14965206384658813\n",
      "\tLoss: 0.1278732419013977\n",
      "\tLoss: 0.07121872156858444\n",
      "\tLoss: 0.15916067361831665\n",
      "\tLoss: 0.06671436131000519\n",
      "\tLoss: 0.13300295174121857\n",
      "\tLoss: 0.12062789499759674\n",
      "\tLoss: 0.1401653289794922\n",
      "\tLoss: 0.09524056315422058\n",
      "\tLoss: 0.04438559338450432\n",
      "\tLoss: 0.13803723454475403\n",
      "\tLoss: 0.0819869190454483\n",
      "\tLoss: 0.07847277820110321\n",
      "\tLoss: 0.12640294432640076\n",
      "\tLoss: 0.11262287944555283\n",
      "\tLoss: 0.13068854808807373\n",
      "\tLoss: 0.10102558135986328\n",
      "\tLoss: 0.16442477703094482\n",
      "\tLoss: 0.09901802986860275\n",
      "\tLoss: 0.09723244607448578\n",
      "\tLoss: 0.1270570158958435\n",
      "\tLoss: 0.15265360474586487\n",
      "\tLoss: 0.164511039853096\n",
      "\tLoss: 0.1039985716342926\n",
      "\tLoss: 0.08704063296318054\n",
      "\tLoss: 0.0761004239320755\n",
      "\tLoss: 0.13068388402462006\n",
      "\tLoss: 0.08155310899019241\n",
      "\tLoss: 0.13390469551086426\n",
      "\tLoss: 0.11309407651424408\n",
      "\tLoss: 0.17463448643684387\n",
      "\tLoss: 0.18554984033107758\n",
      "\tLoss: 0.16902759671211243\n",
      "\tLoss: 0.14353924989700317\n",
      "\tLoss: 0.08313461393117905\n",
      "\tLoss: 0.13248679041862488\n",
      "\tLoss: 0.08279769122600555\n",
      "\tLoss: 0.11064617335796356\n",
      "\tLoss: 0.10709045827388763\n",
      "\tLoss: 0.142078697681427\n",
      "\tLoss: 0.10045672208070755\n",
      "\tLoss: 0.09299144893884659\n",
      "\tLoss: 0.15224811434745789\n",
      "\tLoss: 0.10406658053398132\n",
      "\tLoss: 0.13599635660648346\n",
      "\tLoss: 0.09374246001243591\n",
      "\tLoss: 0.15102359652519226\n",
      "\tLoss: 0.10176253318786621\n",
      "\tLoss: 0.11107397079467773\n",
      "\tLoss: 0.12798430025577545\n",
      "\tLoss: 0.10117883235216141\n",
      "\tLoss: 0.1627630591392517\n",
      "\tLoss: 0.10809531807899475\n",
      "\tLoss: 0.11070306599140167\n",
      "\tLoss: 0.13474327325820923\n",
      "\tLoss: 0.06504572927951813\n",
      "\tLoss: 0.12038003653287888\n",
      "\tLoss: 0.21950408816337585\n",
      "\tLoss: 0.10193181037902832\n",
      "\tLoss: 0.1002790704369545\n",
      "\tLoss: 0.14240436255931854\n",
      "\tLoss: 0.1373518705368042\n",
      "\tLoss: 0.07129085063934326\n",
      "\tLoss: 0.13863801956176758\n",
      "\tLoss: 0.13250505924224854\n",
      "\tLoss: 0.12158913910388947\n",
      "\tLoss: 0.08829974383115768\n",
      "\tLoss: 0.08670248091220856\n",
      "\tLoss: 0.1343851089477539\n",
      "\tLoss: 0.062333691865205765\n",
      "\tLoss: 0.12320158630609512\n",
      "\tLoss: 0.1150631457567215\n",
      "\tLoss: 0.06793545931577682\n",
      "\tLoss: 0.16304892301559448\n",
      "\tLoss: 0.12027409672737122\n",
      "\tLoss: 0.13902516663074493\n",
      "\tLoss: 0.11313818395137787\n",
      "\tLoss: 0.08772249519824982\n",
      "\tLoss: 0.129719540476799\n",
      "\tLoss: 0.1323578655719757\n",
      "\tLoss: 0.13379371166229248\n",
      "\tLoss: 0.11265087872743607\n",
      "\tLoss: 0.1493133306503296\n",
      "\tLoss: 0.11882463842630386\n",
      "\tLoss: 0.15905281901359558\n",
      "\tLoss: 0.21429228782653809\n",
      "\tLoss: 0.08787664771080017\n",
      "\tLoss: 0.12628403306007385\n",
      "\tLoss: 0.055909208953380585\n",
      "\tLoss: 0.194830983877182\n",
      "\tLoss: 0.08969064056873322\n",
      "\tLoss: 0.12476206570863724\n",
      "\tLoss: 0.17091992497444153\n",
      "\tLoss: 0.09362179040908813\n",
      "\tLoss: 0.10717819631099701\n",
      "\tLoss: 0.08652284741401672\n",
      "\tLoss: 0.14164596796035767\n",
      "\tLoss: 0.07806892693042755\n",
      "\tLoss: 0.15857911109924316\n",
      "\tLoss: 0.16455204784870148\n",
      "\tLoss: 0.18365749716758728\n",
      "\tLoss: 0.16574881970882416\n",
      "[time] Epoch 17: 432.63220798410475s = 7.210536799735079m\n",
      "\n",
      "Epoch 18...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.13612233102321625\n",
      "\tLoss: 0.1248968094587326\n",
      "\tLoss: 0.08218725025653839\n",
      "\tLoss: 0.1794796586036682\n",
      "\tLoss: 0.16383114457130432\n",
      "\tLoss: 0.11080746352672577\n",
      "\tLoss: 0.1453917771577835\n",
      "\tLoss: 0.09260192513465881\n",
      "\tLoss: 0.18650159239768982\n",
      "\tLoss: 0.1228121966123581\n",
      "\tLoss: 0.13542038202285767\n",
      "\tLoss: 0.09816618263721466\n",
      "\tLoss: 0.09420214593410492\n",
      "\tLoss: 0.12167426943778992\n",
      "\tLoss: 0.20986635982990265\n",
      "\tLoss: 0.1263982504606247\n",
      "\tLoss: 0.06637889891862869\n",
      "\tLoss: 0.10212572664022446\n",
      "\tLoss: 0.15673944354057312\n",
      "\tLoss: 0.0918758437037468\n",
      "\tLoss: 0.09735700488090515\n",
      "\tLoss: 0.15510296821594238\n",
      "\tLoss: 0.16736893355846405\n",
      "\tLoss: 0.11137515306472778\n",
      "\tLoss: 0.1190522313117981\n",
      "\tLoss: 0.044820114970207214\n",
      "\tLoss: 0.12932105362415314\n",
      "\tLoss: 0.08511626720428467\n",
      "\tLoss: 0.11408452689647675\n",
      "\tLoss: 0.17063193023204803\n",
      "\tLoss: 0.1633506417274475\n",
      "\tLoss: 0.11599999666213989\n",
      "\tLoss: 0.1909520924091339\n",
      "\tLoss: 0.14735616743564606\n",
      "\tLoss: 0.14205555617809296\n",
      "\tLoss: 0.11245238780975342\n",
      "\tLoss: 0.18058206140995026\n",
      "\tLoss: 0.10808483511209488\n",
      "\tLoss: 0.10086774080991745\n",
      "\tLoss: 0.10901053994894028\n",
      "\tLoss: 0.0809275358915329\n",
      "\tLoss: 0.056549716740846634\n",
      "\tLoss: 0.13532759249210358\n",
      "\tLoss: 0.10342776775360107\n",
      "\tLoss: 0.12800824642181396\n",
      "\tLoss: 0.15320797264575958\n",
      "\tLoss: 0.12318326532840729\n",
      "\tLoss: 0.10819484293460846\n",
      "\tLoss: 0.11574957519769669\n",
      "\tLoss: 0.10852548480033875\n",
      "\tLoss: 0.12547706067562103\n",
      "\tLoss: 0.15345898270606995\n",
      "\tLoss: 0.14013801515102386\n",
      "\tLoss: 0.09037770330905914\n",
      "\tLoss: 0.17105810344219208\n",
      "\tLoss: 0.11670223623514175\n",
      "\tLoss: 0.07502981275320053\n",
      "\tLoss: 0.1238226667046547\n",
      "\tLoss: 0.09937432408332825\n",
      "\tLoss: 0.11482751369476318\n",
      "\tLoss: 0.08948919922113419\n",
      "\tLoss: 0.19016121327877045\n",
      "\tLoss: 0.13240140676498413\n",
      "\tLoss: 0.20048788189888\n",
      "\tLoss: 0.18411284685134888\n",
      "\tLoss: 0.10643023997545242\n",
      "\tLoss: 0.0915856659412384\n",
      "\tLoss: 0.09053458273410797\n",
      "\tLoss: 0.1096235066652298\n",
      "\tLoss: 0.08697841316461563\n",
      "\tLoss: 0.14575177431106567\n",
      "\tLoss: 0.13285274803638458\n",
      "\tLoss: 0.12940579652786255\n",
      "\tLoss: 0.11887061595916748\n",
      "\tLoss: 0.12205269932746887\n",
      "\tLoss: 0.14984433352947235\n",
      "\tLoss: 0.13466015458106995\n",
      "\tLoss: 0.13104695081710815\n",
      "\tLoss: 0.08035296201705933\n",
      "\tLoss: 0.14315524697303772\n",
      "\tLoss: 0.1259998381137848\n",
      "\tLoss: 0.16212844848632812\n",
      "\tLoss: 0.09833073616027832\n",
      "\tLoss: 0.045317281037569046\n",
      "\tLoss: 0.13889583945274353\n",
      "\tLoss: 0.0983896553516388\n",
      "\tLoss: 0.12979739904403687\n",
      "\tLoss: 0.14911839365959167\n",
      "\tLoss: 0.10315190255641937\n",
      "\tLoss: 0.10227906703948975\n",
      "\tLoss: 0.05722663924098015\n",
      "\tLoss: 0.15490877628326416\n",
      "\tLoss: 0.08334923535585403\n",
      "\tLoss: 0.09752246737480164\n",
      "\tLoss: 0.1528691053390503\n",
      "\tLoss: 0.1426454782485962\n",
      "\tLoss: 0.1750975102186203\n",
      "\tLoss: 0.13062937557697296\n",
      "\tLoss: 0.060135215520858765\n",
      "\tLoss: 0.1272350549697876\n",
      "\tLoss: 0.11691363900899887\n",
      "\tLoss: 0.12720102071762085\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.11806857585906982\n",
      "\tLoss: 0.1058625727891922\n",
      "\tLoss: 0.1103442832827568\n",
      "\tLoss: 0.0881192609667778\n",
      "\tLoss: 0.19523011147975922\n",
      "\tLoss: 0.2119760513305664\n",
      "\tLoss: 0.12620212137699127\n",
      "\tLoss: 0.0893634557723999\n",
      "\tLoss: 0.11105874925851822\n",
      "\tLoss: 0.07350881397724152\n",
      "\tLoss: 0.12016503512859344\n",
      "\tLoss: 0.11255709826946259\n",
      "\tLoss: 0.0925607979297638\n",
      "\tLoss: 0.10850206017494202\n",
      "\tLoss: 0.08250914514064789\n",
      "\tLoss: 0.13721659779548645\n",
      "\tLoss: 0.0818677693605423\n",
      "\tLoss: 0.09283994138240814\n",
      "\tLoss: 0.09941750764846802\n",
      "\tLoss: 0.16629302501678467\n",
      "\tLoss: 0.10450536012649536\n",
      "\tLoss: 0.11365388333797455\n",
      "\tLoss: 0.08836807310581207\n",
      "\tLoss: 0.1046387180685997\n",
      "\tLoss: 0.18563705682754517\n",
      "\tLoss: 0.06987660378217697\n",
      "\tLoss: 0.10255169868469238\n",
      "\tLoss: 0.09633071720600128\n",
      "\tLoss: 0.15519286692142487\n",
      "\tLoss: 0.08321592956781387\n",
      "\tLoss: 0.0851459950208664\n",
      "\tLoss: 0.07002702355384827\n",
      "\tLoss: 0.17845892906188965\n",
      "\tLoss: 0.08465096354484558\n",
      "\tLoss: 0.16553837060928345\n",
      "\tLoss: 0.14842760562896729\n",
      "\tLoss: 0.19578489661216736\n",
      "\tLoss: 0.10107352584600449\n",
      "\tLoss: 0.09247703105211258\n",
      "\tLoss: 0.1117718368768692\n",
      "\tLoss: 0.10930435359477997\n",
      "\tLoss: 0.15547853708267212\n",
      "\tLoss: 0.12939593195915222\n",
      "\tLoss: 0.14981260895729065\n",
      "\tLoss: 0.15088459849357605\n",
      "\tLoss: 0.12328259646892548\n",
      "\tLoss: 0.13853418827056885\n",
      "\tLoss: 0.09399522095918655\n",
      "\tLoss: 0.12272367626428604\n",
      "\tLoss: 0.08386609703302383\n",
      "\tLoss: 0.05465516448020935\n",
      "\tLoss: 0.0887618437409401\n",
      "\tLoss: 0.12785542011260986\n",
      "\tLoss: 0.18075838685035706\n",
      "\tLoss: 0.15413539111614227\n",
      "\tLoss: 0.09816401451826096\n",
      "\tLoss: 0.12409469485282898\n",
      "\tLoss: 0.15496566891670227\n",
      "\tLoss: 0.12557677924633026\n",
      "\tLoss: 0.09619443863630295\n",
      "\tLoss: 0.2039889097213745\n",
      "\tLoss: 0.11346257477998734\n",
      "\tLoss: 0.07396339625120163\n",
      "\tLoss: 0.059426479041576385\n",
      "\tLoss: 0.15765053033828735\n",
      "\tLoss: 0.12503288686275482\n",
      "\tLoss: 0.10687959939241409\n",
      "\tLoss: 0.11971891671419144\n",
      "\tLoss: 0.13004454970359802\n",
      "\tLoss: 0.1174347773194313\n",
      "\tLoss: 0.1262136995792389\n",
      "\tLoss: 0.09532645344734192\n",
      "\tLoss: 0.09083588421344757\n",
      "\tLoss: 0.12549158930778503\n",
      "\tLoss: 0.14793044328689575\n",
      "\tLoss: 0.12693315744400024\n",
      "\tLoss: 0.10941116511821747\n",
      "\tLoss: 0.1416938602924347\n",
      "\tLoss: 0.12999291718006134\n",
      "\tLoss: 0.12284625321626663\n",
      "\tLoss: 0.1207803264260292\n",
      "\tLoss: 0.09166054427623749\n",
      "\tLoss: 0.12668149173259735\n",
      "\tLoss: 0.12474684417247772\n",
      "\tLoss: 0.09940172731876373\n",
      "\tLoss: 0.13919587433338165\n",
      "\tLoss: 0.13705608248710632\n",
      "\tLoss: 0.18558019399642944\n",
      "\tLoss: 0.12658625841140747\n",
      "\tLoss: 0.1706341952085495\n",
      "\tLoss: 0.11211640387773514\n",
      "\tLoss: 0.09133771061897278\n",
      "\tLoss: 0.10654358565807343\n",
      "\tLoss: 0.0714225172996521\n",
      "\tLoss: 0.10257482528686523\n",
      "\tLoss: 0.09902724623680115\n",
      "\tLoss: 0.05545494705438614\n",
      "\tLoss: 0.15112686157226562\n",
      "\tLoss: 0.14881518483161926\n",
      "\tLoss: 0.12501057982444763\n",
      "\tLoss: 0.15264764428138733\n",
      "\tLoss: 0.17524558305740356\n",
      "[time] Epoch 18: 427.3607190940529s = 7.122678651567549m\n",
      "\n",
      "Epoch 19...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.07727867364883423\n",
      "\tLoss: 0.08321671187877655\n",
      "\tLoss: 0.14285749197006226\n",
      "\tLoss: 0.10405673831701279\n",
      "\tLoss: 0.07111486792564392\n",
      "\tLoss: 0.09852990508079529\n",
      "\tLoss: 0.15170316398143768\n",
      "\tLoss: 0.07479379326105118\n",
      "\tLoss: 0.18968906998634338\n",
      "\tLoss: 0.07993977516889572\n",
      "\tLoss: 0.12086173892021179\n",
      "\tLoss: 0.0903090313076973\n",
      "\tLoss: 0.12677258253097534\n",
      "\tLoss: 0.12778660655021667\n",
      "\tLoss: 0.14464259147644043\n",
      "\tLoss: 0.13488253951072693\n",
      "\tLoss: 0.1171427071094513\n",
      "\tLoss: 0.16417938470840454\n",
      "\tLoss: 0.0678771585226059\n",
      "\tLoss: 0.11054018884897232\n",
      "\tLoss: 0.13745540380477905\n",
      "\tLoss: 0.09010916203260422\n",
      "\tLoss: 0.11844787001609802\n",
      "\tLoss: 0.14374086260795593\n",
      "\tLoss: 0.1290547400712967\n",
      "\tLoss: 0.09463542699813843\n",
      "\tLoss: 0.055792540311813354\n",
      "\tLoss: 0.1064678207039833\n",
      "\tLoss: 0.08613225817680359\n",
      "\tLoss: 0.14038440585136414\n",
      "\tLoss: 0.10401029139757156\n",
      "\tLoss: 0.12083490937948227\n",
      "\tLoss: 0.056499943137168884\n",
      "\tLoss: 0.11555942893028259\n",
      "\tLoss: 0.1331094205379486\n",
      "\tLoss: 0.09920074790716171\n",
      "\tLoss: 0.087483249604702\n",
      "\tLoss: 0.1505400836467743\n",
      "\tLoss: 0.08818328380584717\n",
      "\tLoss: 0.07593461871147156\n",
      "\tLoss: 0.10511672496795654\n",
      "\tLoss: 0.09277749806642532\n",
      "\tLoss: 0.09900355339050293\n",
      "\tLoss: 0.10338610410690308\n",
      "\tLoss: 0.14922963082790375\n",
      "\tLoss: 0.17316633462905884\n",
      "\tLoss: 0.07976679503917694\n",
      "\tLoss: 0.10226207971572876\n",
      "\tLoss: 0.12649407982826233\n",
      "\tLoss: 0.14058895409107208\n",
      "\tLoss: 0.14961624145507812\n",
      "\tLoss: 0.06952841579914093\n",
      "\tLoss: 0.1070471704006195\n",
      "\tLoss: 0.10599962621927261\n",
      "\tLoss: 0.1547498106956482\n",
      "\tLoss: 0.14414560794830322\n",
      "\tLoss: 0.11677490174770355\n",
      "\tLoss: 0.09900549054145813\n",
      "\tLoss: 0.16724780201911926\n",
      "\tLoss: 0.13296997547149658\n",
      "\tLoss: 0.12124590575695038\n",
      "\tLoss: 0.11927077174186707\n",
      "\tLoss: 0.08392363041639328\n",
      "\tLoss: 0.09239130467176437\n",
      "\tLoss: 0.08898797631263733\n",
      "\tLoss: 0.09545303136110306\n",
      "\tLoss: 0.1398441642522812\n",
      "\tLoss: 0.1453230232000351\n",
      "\tLoss: 0.14097881317138672\n",
      "\tLoss: 0.11972954869270325\n",
      "\tLoss: 0.11749529093503952\n",
      "\tLoss: 0.11417041718959808\n",
      "\tLoss: 0.128179132938385\n",
      "\tLoss: 0.14735311269760132\n",
      "\tLoss: 0.126506969332695\n",
      "\tLoss: 0.13169929385185242\n",
      "\tLoss: 0.12077756226062775\n",
      "\tLoss: 0.14540347456932068\n",
      "\tLoss: 0.11209602653980255\n",
      "\tLoss: 0.10458879172801971\n",
      "\tLoss: 0.16308453679084778\n",
      "\tLoss: 0.1005314514040947\n",
      "\tLoss: 0.08539719134569168\n",
      "\tLoss: 0.18882951140403748\n",
      "\tLoss: 0.12523570656776428\n",
      "\tLoss: 0.1428569257259369\n",
      "\tLoss: 0.1730664074420929\n",
      "\tLoss: 0.1410093754529953\n",
      "\tLoss: 0.10003852099180222\n",
      "\tLoss: 0.1515560746192932\n",
      "\tLoss: 0.09255530685186386\n",
      "\tLoss: 0.19415995478630066\n",
      "\tLoss: 0.12725946307182312\n",
      "\tLoss: 0.10684999078512192\n",
      "\tLoss: 0.1498723328113556\n",
      "\tLoss: 0.1540088951587677\n",
      "\tLoss: 0.12376894801855087\n",
      "\tLoss: 0.15252256393432617\n",
      "\tLoss: 0.12535804510116577\n",
      "\tLoss: 0.1708999127149582\n",
      "\tLoss: 0.08049174398183823\n",
      "\tLoss: 0.15315192937850952\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.11230454593896866\n",
      "\tLoss: 0.18577584624290466\n",
      "\tLoss: 0.10828203707933426\n",
      "\tLoss: 0.13072802126407623\n",
      "\tLoss: 0.11809344589710236\n",
      "\tLoss: 0.13394618034362793\n",
      "\tLoss: 0.09380333125591278\n",
      "\tLoss: 0.14220573008060455\n",
      "\tLoss: 0.11442958563566208\n",
      "\tLoss: 0.1484433114528656\n",
      "\tLoss: 0.14002089202404022\n",
      "\tLoss: 0.0867898017168045\n",
      "\tLoss: 0.1071808710694313\n",
      "\tLoss: 0.10279418528079987\n",
      "\tLoss: 0.14102649688720703\n",
      "\tLoss: 0.127688467502594\n",
      "\tLoss: 0.08839403092861176\n",
      "\tLoss: 0.12749910354614258\n",
      "\tLoss: 0.09784437716007233\n",
      "\tLoss: 0.09722119569778442\n",
      "\tLoss: 0.09463097155094147\n",
      "\tLoss: 0.10058578848838806\n",
      "\tLoss: 0.1233024001121521\n",
      "\tLoss: 0.1214536651968956\n",
      "\tLoss: 0.1441691815853119\n",
      "\tLoss: 0.1641480177640915\n",
      "\tLoss: 0.06884800642728806\n",
      "\tLoss: 0.13338881731033325\n",
      "\tLoss: 0.12761086225509644\n",
      "\tLoss: 0.11388935148715973\n",
      "\tLoss: 0.11011464893817902\n",
      "\tLoss: 0.14788702130317688\n",
      "\tLoss: 0.1603630930185318\n",
      "\tLoss: 0.15613842010498047\n",
      "\tLoss: 0.10939298570156097\n",
      "\tLoss: 0.1436695158481598\n",
      "\tLoss: 0.13280785083770752\n",
      "\tLoss: 0.10092633962631226\n",
      "\tLoss: 0.10646028816699982\n",
      "\tLoss: 0.11548147350549698\n",
      "\tLoss: 0.12454109638929367\n",
      "\tLoss: 0.12204121798276901\n",
      "\tLoss: 0.12314064055681229\n",
      "\tLoss: 0.10518255829811096\n",
      "\tLoss: 0.0780242532491684\n",
      "\tLoss: 0.11074038594961166\n",
      "\tLoss: 0.17674286663532257\n",
      "\tLoss: 0.14011900126934052\n",
      "\tLoss: 0.1399446725845337\n",
      "\tLoss: 0.0922030657529831\n",
      "\tLoss: 0.12563587725162506\n",
      "\tLoss: 0.13214991986751556\n",
      "\tLoss: 0.1464872658252716\n",
      "\tLoss: 0.13454604148864746\n",
      "\tLoss: 0.11849835515022278\n",
      "\tLoss: 0.07532575726509094\n",
      "\tLoss: 0.13779319822788239\n",
      "\tLoss: 0.09560654312372208\n",
      "\tLoss: 0.13117364048957825\n",
      "\tLoss: 0.12153047323226929\n",
      "\tLoss: 0.13190396130084991\n",
      "\tLoss: 0.09713056683540344\n",
      "\tLoss: 0.10666497051715851\n",
      "\tLoss: 0.10466868430376053\n",
      "\tLoss: 0.12707293033599854\n",
      "\tLoss: 0.09672064334154129\n",
      "\tLoss: 0.12456019222736359\n",
      "\tLoss: 0.1484723687171936\n",
      "\tLoss: 0.06733225286006927\n",
      "\tLoss: 0.14186301827430725\n",
      "\tLoss: 0.1696797013282776\n",
      "\tLoss: 0.07710131257772446\n",
      "\tLoss: 0.12692686915397644\n",
      "\tLoss: 0.16128697991371155\n",
      "\tLoss: 0.1296660602092743\n",
      "\tLoss: 0.11995694041252136\n",
      "\tLoss: 0.15728884935379028\n",
      "\tLoss: 0.13470762968063354\n",
      "\tLoss: 0.14238616824150085\n",
      "\tLoss: 0.14450247585773468\n",
      "\tLoss: 0.14843228459358215\n",
      "\tLoss: 0.10566150397062302\n",
      "\tLoss: 0.15894700586795807\n",
      "\tLoss: 0.10551949590444565\n",
      "\tLoss: 0.12426648288965225\n",
      "\tLoss: 0.10532689094543457\n",
      "\tLoss: 0.14636269211769104\n",
      "\tLoss: 0.09539762884378433\n",
      "\tLoss: 0.10922232270240784\n",
      "\tLoss: 0.11183030903339386\n",
      "\tLoss: 0.1445106714963913\n",
      "\tLoss: 0.10909652709960938\n",
      "\tLoss: 0.08846913278102875\n",
      "\tLoss: 0.13827629387378693\n",
      "\tLoss: 0.06212899461388588\n",
      "\tLoss: 0.15773116052150726\n",
      "\tLoss: 0.09870012104511261\n",
      "\tLoss: 0.10747987031936646\n",
      "\tLoss: 0.14859986305236816\n",
      "\tLoss: 0.11630218476057053\n",
      "\tLoss: 0.14363262057304382\n",
      "\tLoss: 0.06719595193862915\n",
      "[time] Epoch 19: 426.75026418222114s = 7.112504403037019m\n",
      "\n",
      "Epoch 20...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.1632964313030243\n",
      "\tLoss: 0.1725444197654724\n",
      "\tLoss: 0.17918768525123596\n",
      "\tLoss: 0.13705933094024658\n",
      "\tLoss: 0.11323954164981842\n",
      "\tLoss: 0.11290355026721954\n",
      "\tLoss: 0.12382368743419647\n",
      "\tLoss: 0.090309277176857\n",
      "\tLoss: 0.16536402702331543\n",
      "\tLoss: 0.12555712461471558\n",
      "\tLoss: 0.10869862139225006\n",
      "\tLoss: 0.09281821548938751\n",
      "\tLoss: 0.12499578297138214\n",
      "\tLoss: 0.07600601017475128\n",
      "\tLoss: 0.09497745335102081\n",
      "\tLoss: 0.11548975855112076\n",
      "\tLoss: 0.0716204047203064\n",
      "\tLoss: 0.12356121838092804\n",
      "\tLoss: 0.13645106554031372\n",
      "\tLoss: 0.19277898967266083\n",
      "\tLoss: 0.10454417765140533\n",
      "\tLoss: 0.19515082240104675\n",
      "\tLoss: 0.137542724609375\n",
      "\tLoss: 0.1261289417743683\n",
      "\tLoss: 0.13632258772850037\n",
      "\tLoss: 0.2061481773853302\n",
      "\tLoss: 0.13764497637748718\n",
      "\tLoss: 0.10348337143659592\n",
      "\tLoss: 0.07378420233726501\n",
      "\tLoss: 0.07498457282781601\n",
      "\tLoss: 0.14617273211479187\n",
      "\tLoss: 0.13371895253658295\n",
      "\tLoss: 0.14744475483894348\n",
      "\tLoss: 0.1365145742893219\n",
      "\tLoss: 0.08363769203424454\n",
      "\tLoss: 0.1335192173719406\n",
      "\tLoss: 0.11496108770370483\n",
      "\tLoss: 0.14781802892684937\n",
      "\tLoss: 0.11173158138990402\n",
      "\tLoss: 0.1033170074224472\n",
      "\tLoss: 0.11991295963525772\n",
      "\tLoss: 0.13630135357379913\n",
      "\tLoss: 0.09789074957370758\n",
      "\tLoss: 0.16242152452468872\n",
      "\tLoss: 0.15622399747371674\n",
      "\tLoss: 0.15236064791679382\n",
      "\tLoss: 0.11474496126174927\n",
      "\tLoss: 0.08780597150325775\n",
      "\tLoss: 0.11913315206766129\n",
      "\tLoss: 0.09917531907558441\n",
      "\tLoss: 0.16089455783367157\n",
      "\tLoss: 0.09572192281484604\n",
      "\tLoss: 0.17819233238697052\n",
      "\tLoss: 0.11093050241470337\n",
      "\tLoss: 0.1160041093826294\n",
      "\tLoss: 0.16635310649871826\n",
      "\tLoss: 0.1740502119064331\n",
      "\tLoss: 0.11411557346582413\n",
      "\tLoss: 0.17880378663539886\n",
      "\tLoss: 0.1285226047039032\n",
      "\tLoss: 0.13776375353336334\n",
      "\tLoss: 0.08082664757966995\n",
      "\tLoss: 0.1514986753463745\n",
      "\tLoss: 0.0899026095867157\n",
      "\tLoss: 0.14688073098659515\n",
      "\tLoss: 0.10053826868534088\n",
      "\tLoss: 0.1047717034816742\n",
      "\tLoss: 0.09138891100883484\n",
      "\tLoss: 0.17279687523841858\n",
      "\tLoss: 0.16934961080551147\n",
      "\tLoss: 0.09987390041351318\n",
      "\tLoss: 0.09760522842407227\n",
      "\tLoss: 0.07545462250709534\n",
      "\tLoss: 0.10125298798084259\n",
      "\tLoss: 0.10538344085216522\n",
      "\tLoss: 0.07152283936738968\n",
      "\tLoss: 0.15501223504543304\n",
      "\tLoss: 0.09009907394647598\n",
      "\tLoss: 0.165541410446167\n",
      "\tLoss: 0.13530610501766205\n",
      "\tLoss: 0.10715766996145248\n",
      "\tLoss: 0.07887613773345947\n",
      "\tLoss: 0.09692241251468658\n",
      "\tLoss: 0.12495250254869461\n",
      "\tLoss: 0.15706020593643188\n",
      "\tLoss: 0.11327038705348969\n",
      "\tLoss: 0.13937263190746307\n",
      "\tLoss: 0.10159066319465637\n",
      "\tLoss: 0.06250032782554626\n",
      "\tLoss: 0.09799973666667938\n",
      "\tLoss: 0.1153683066368103\n",
      "\tLoss: 0.13206669688224792\n",
      "\tLoss: 0.12798994779586792\n",
      "\tLoss: 0.06853438913822174\n",
      "\tLoss: 0.09504983574151993\n",
      "\tLoss: 0.11503926664590836\n",
      "\tLoss: 0.146108016371727\n",
      "\tLoss: 0.08060990273952484\n",
      "\tLoss: 0.1421418935060501\n",
      "\tLoss: 0.17891281843185425\n",
      "\tLoss: 0.07966800779104233\n",
      "\tLoss: 0.1279142200946808\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.10757044702768326\n",
      "\tLoss: 0.15293657779693604\n",
      "\tLoss: 0.11202917248010635\n",
      "\tLoss: 0.10299523174762726\n",
      "\tLoss: 0.1421860158443451\n",
      "\tLoss: 0.13762488961219788\n",
      "\tLoss: 0.1145140677690506\n",
      "\tLoss: 0.10560886561870575\n",
      "\tLoss: 0.08340653777122498\n",
      "\tLoss: 0.12253756821155548\n",
      "\tLoss: 0.1479348987340927\n",
      "\tLoss: 0.21101725101470947\n",
      "\tLoss: 0.07227502763271332\n",
      "\tLoss: 0.06162022799253464\n",
      "\tLoss: 0.10139431804418564\n",
      "\tLoss: 0.10675974190235138\n",
      "\tLoss: 0.11524046957492828\n",
      "\tLoss: 0.105536088347435\n",
      "\tLoss: 0.08769375085830688\n",
      "\tLoss: 0.13521230220794678\n",
      "\tLoss: 0.09857550263404846\n",
      "\tLoss: 0.12965434789657593\n",
      "\tLoss: 0.10985254496335983\n",
      "\tLoss: 0.06928496062755585\n",
      "\tLoss: 0.1435997188091278\n",
      "\tLoss: 0.1421162486076355\n",
      "\tLoss: 0.1432129144668579\n",
      "\tLoss: 0.16281872987747192\n",
      "\tLoss: 0.19283045828342438\n",
      "\tLoss: 0.15389007329940796\n",
      "\tLoss: 0.10883910953998566\n",
      "\tLoss: 0.11313268542289734\n",
      "\tLoss: 0.1385582685470581\n",
      "\tLoss: 0.06686296314001083\n",
      "\tLoss: 0.20848873257637024\n",
      "\tLoss: 0.13129788637161255\n",
      "\tLoss: 0.13881193101406097\n",
      "\tLoss: 0.09594789147377014\n",
      "\tLoss: 0.07966715097427368\n",
      "\tLoss: 0.14691327512264252\n",
      "\tLoss: 0.09925642609596252\n",
      "\tLoss: 0.11502823233604431\n",
      "\tLoss: 0.15627025067806244\n",
      "\tLoss: 0.10679972171783447\n",
      "\tLoss: 0.1248701959848404\n",
      "\tLoss: 0.10236822813749313\n",
      "\tLoss: 0.11753355711698532\n",
      "\tLoss: 0.13953930139541626\n",
      "\tLoss: 0.18582555651664734\n",
      "\tLoss: 0.08589375019073486\n",
      "\tLoss: 0.09448260813951492\n",
      "\tLoss: 0.12233109772205353\n",
      "\tLoss: 0.12716251611709595\n",
      "\tLoss: 0.15574005246162415\n",
      "\tLoss: 0.09357717633247375\n",
      "\tLoss: 0.10261023789644241\n",
      "\tLoss: 0.08662079274654388\n",
      "\tLoss: 0.12779709696769714\n",
      "\tLoss: 0.09176500141620636\n",
      "\tLoss: 0.15879128873348236\n",
      "\tLoss: 0.12317685782909393\n",
      "\tLoss: 0.12065878510475159\n",
      "\tLoss: 0.1720036268234253\n",
      "\tLoss: 0.12278951704502106\n",
      "\tLoss: 0.07739377021789551\n",
      "\tLoss: 0.08571618795394897\n",
      "\tLoss: 0.10030003637075424\n",
      "\tLoss: 0.16670432686805725\n",
      "\tLoss: 0.1421772837638855\n",
      "\tLoss: 0.18420222401618958\n",
      "\tLoss: 0.12681066989898682\n",
      "\tLoss: 0.18679179251194\n",
      "\tLoss: 0.07699350267648697\n",
      "\tLoss: 0.11983070522546768\n",
      "\tLoss: 0.10928352922201157\n",
      "\tLoss: 0.13528792560100555\n",
      "\tLoss: 0.09339874237775803\n",
      "\tLoss: 0.09533409774303436\n",
      "\tLoss: 0.1071782112121582\n",
      "\tLoss: 0.18818701803684235\n",
      "\tLoss: 0.17811255156993866\n",
      "\tLoss: 0.11135119199752808\n",
      "\tLoss: 0.14731433987617493\n",
      "\tLoss: 0.10161653161048889\n",
      "\tLoss: 0.138644278049469\n",
      "\tLoss: 0.08329005539417267\n",
      "\tLoss: 0.10533692687749863\n",
      "\tLoss: 0.10224361717700958\n",
      "\tLoss: 0.14693501591682434\n",
      "\tLoss: 0.08884185552597046\n",
      "\tLoss: 0.16827955842018127\n",
      "\tLoss: 0.07238952815532684\n",
      "\tLoss: 0.17605072259902954\n",
      "\tLoss: 0.11025495827198029\n",
      "\tLoss: 0.18725034594535828\n",
      "\tLoss: 0.14296123385429382\n",
      "\tLoss: 0.1403389275074005\n",
      "\tLoss: 0.11411146819591522\n",
      "\tLoss: 0.1271175742149353\n",
      "\tLoss: 0.11892841011285782\n",
      "\tLoss: 0.13501521944999695\n",
      "\tLoss: 0.1828685700893402\n",
      "[time] Epoch 20: 427.0390078481287s = 7.117316797468812m\n",
      "\n",
      "Epoch 21...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.13389648497104645\n",
      "\tLoss: 0.06628607958555222\n",
      "\tLoss: 0.06819990277290344\n",
      "\tLoss: 0.1175680011510849\n",
      "\tLoss: 0.11730554699897766\n",
      "\tLoss: 0.08997966349124908\n",
      "\tLoss: 0.11130650341510773\n",
      "\tLoss: 0.1632956862449646\n",
      "\tLoss: 0.07060689479112625\n",
      "\tLoss: 0.10843998193740845\n",
      "\tLoss: 0.10154937207698822\n",
      "\tLoss: 0.13775920867919922\n",
      "\tLoss: 0.10753218084573746\n",
      "\tLoss: 0.11555289477109909\n",
      "\tLoss: 0.11815982311964035\n",
      "\tLoss: 0.11993707716464996\n",
      "\tLoss: 0.08284921944141388\n",
      "\tLoss: 0.08157337456941605\n",
      "\tLoss: 0.08852779120206833\n",
      "\tLoss: 0.13207687437534332\n",
      "\tLoss: 0.12177982181310654\n",
      "\tLoss: 0.1079234704375267\n",
      "\tLoss: 0.1804417222738266\n",
      "\tLoss: 0.06151166930794716\n",
      "\tLoss: 0.08331654965877533\n",
      "\tLoss: 0.11035428941249847\n",
      "\tLoss: 0.10358214378356934\n",
      "\tLoss: 0.10013484209775925\n",
      "\tLoss: 0.1325226128101349\n",
      "\tLoss: 0.07380455732345581\n",
      "\tLoss: 0.06761881709098816\n",
      "\tLoss: 0.15006834268569946\n",
      "\tLoss: 0.12703198194503784\n",
      "\tLoss: 0.1107795387506485\n",
      "\tLoss: 0.09863164275884628\n",
      "\tLoss: 0.1276625394821167\n",
      "\tLoss: 0.09949826449155807\n",
      "\tLoss: 0.11120646446943283\n",
      "\tLoss: 0.10631583631038666\n",
      "\tLoss: 0.12769362330436707\n",
      "\tLoss: 0.12713715434074402\n",
      "\tLoss: 0.08218055218458176\n",
      "\tLoss: 0.15809942781925201\n",
      "\tLoss: 0.11875685304403305\n",
      "\tLoss: 0.13015827536582947\n",
      "\tLoss: 0.14034327864646912\n",
      "\tLoss: 0.1366477757692337\n",
      "\tLoss: 0.16092991828918457\n",
      "\tLoss: 0.12975428998470306\n",
      "\tLoss: 0.13235458731651306\n",
      "\tLoss: 0.12152978777885437\n",
      "\tLoss: 0.11997439712285995\n",
      "\tLoss: 0.14502331614494324\n",
      "\tLoss: 0.10514934360980988\n",
      "\tLoss: 0.14970681071281433\n",
      "\tLoss: 0.11912937462329865\n",
      "\tLoss: 0.09141673147678375\n",
      "\tLoss: 0.13815376162528992\n",
      "\tLoss: 0.13191677629947662\n",
      "\tLoss: 0.15688413381576538\n",
      "\tLoss: 0.14428402483463287\n",
      "\tLoss: 0.10085135698318481\n",
      "\tLoss: 0.11575612425804138\n",
      "\tLoss: 0.10736890882253647\n",
      "\tLoss: 0.10672275722026825\n",
      "\tLoss: 0.1116747111082077\n",
      "\tLoss: 0.1465737521648407\n",
      "\tLoss: 0.1337093561887741\n",
      "\tLoss: 0.11283008754253387\n",
      "\tLoss: 0.15046896040439606\n",
      "\tLoss: 0.08937795460224152\n",
      "\tLoss: 0.09496422111988068\n",
      "\tLoss: 0.0641455128788948\n",
      "\tLoss: 0.15074513852596283\n",
      "\tLoss: 0.13180939853191376\n",
      "\tLoss: 0.10383699089288712\n",
      "\tLoss: 0.08261007070541382\n",
      "\tLoss: 0.09686078131198883\n",
      "\tLoss: 0.1156354695558548\n",
      "\tLoss: 0.06575502455234528\n",
      "\tLoss: 0.09240059554576874\n",
      "\tLoss: 0.08524879813194275\n",
      "\tLoss: 0.12164382636547089\n",
      "\tLoss: 0.06428075581789017\n",
      "\tLoss: 0.1110704317688942\n",
      "\tLoss: 0.1812145709991455\n",
      "\tLoss: 0.05107448995113373\n",
      "\tLoss: 0.10531191527843475\n",
      "\tLoss: 0.14188915491104126\n",
      "\tLoss: 0.1358395218849182\n",
      "\tLoss: 0.16587845981121063\n",
      "\tLoss: 0.1370595097541809\n",
      "\tLoss: 0.11986265331506729\n",
      "\tLoss: 0.13983803987503052\n",
      "\tLoss: 0.13380396366119385\n",
      "\tLoss: 0.1930272877216339\n",
      "\tLoss: 0.11025920510292053\n",
      "\tLoss: 0.15128417313098907\n",
      "\tLoss: 0.1401708573102951\n",
      "\tLoss: 0.15235179662704468\n",
      "\tLoss: 0.10398757457733154\n",
      "\tLoss: 0.10303275287151337\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.11982971429824829\n",
      "\tLoss: 0.09574796259403229\n",
      "\tLoss: 0.11737943440675735\n",
      "\tLoss: 0.086268350481987\n",
      "\tLoss: 0.16132648289203644\n",
      "\tLoss: 0.12562890350818634\n",
      "\tLoss: 0.09174048900604248\n",
      "\tLoss: 0.17817121744155884\n",
      "\tLoss: 0.0996861532330513\n",
      "\tLoss: 0.0690988302230835\n",
      "\tLoss: 0.06560087203979492\n",
      "\tLoss: 0.13843309879302979\n",
      "\tLoss: 0.13583043217658997\n",
      "\tLoss: 0.08377374708652496\n",
      "\tLoss: 0.18767714500427246\n",
      "\tLoss: 0.13526076078414917\n",
      "\tLoss: 0.12259221822023392\n",
      "\tLoss: 0.09732740372419357\n",
      "\tLoss: 0.06418453902006149\n",
      "\tLoss: 0.11114829778671265\n",
      "\tLoss: 0.11062750965356827\n",
      "\tLoss: 0.09659595787525177\n",
      "\tLoss: 0.11614784598350525\n",
      "\tLoss: 0.10200097411870956\n",
      "\tLoss: 0.12210026383399963\n",
      "\tLoss: 0.09682789444923401\n",
      "\tLoss: 0.13382044434547424\n",
      "\tLoss: 0.1217515841126442\n",
      "\tLoss: 0.14673128724098206\n",
      "\tLoss: 0.14773160219192505\n",
      "\tLoss: 0.12924528121948242\n",
      "\tLoss: 0.15669837594032288\n",
      "\tLoss: 0.10597199201583862\n",
      "\tLoss: 0.13122981786727905\n",
      "\tLoss: 0.11298653483390808\n",
      "\tLoss: 0.11509127914905548\n",
      "\tLoss: 0.11709080636501312\n",
      "\tLoss: 0.14242640137672424\n",
      "\tLoss: 0.08662855625152588\n",
      "\tLoss: 0.08588746190071106\n",
      "\tLoss: 0.11047658324241638\n",
      "\tLoss: 0.10760217905044556\n",
      "\tLoss: 0.16076554358005524\n",
      "\tLoss: 0.19837120175361633\n",
      "\tLoss: 0.09127774834632874\n",
      "\tLoss: 0.0989350900053978\n",
      "\tLoss: 0.22661349177360535\n",
      "\tLoss: 0.13053278625011444\n",
      "\tLoss: 0.13725727796554565\n",
      "\tLoss: 0.06561771035194397\n",
      "\tLoss: 0.09824127703905106\n",
      "\tLoss: 0.0892810970544815\n",
      "\tLoss: 0.1331789493560791\n",
      "\tLoss: 0.09662818908691406\n",
      "\tLoss: 0.1088389977812767\n",
      "\tLoss: 0.10303453356027603\n",
      "\tLoss: 0.09391690790653229\n",
      "\tLoss: 0.0982225090265274\n",
      "\tLoss: 0.1200452595949173\n",
      "\tLoss: 0.16039307415485382\n",
      "\tLoss: 0.11588706821203232\n",
      "\tLoss: 0.09952148795127869\n",
      "\tLoss: 0.13790367543697357\n",
      "\tLoss: 0.14911538362503052\n",
      "\tLoss: 0.10557730495929718\n",
      "\tLoss: 0.14303530752658844\n",
      "\tLoss: 0.1045297160744667\n",
      "\tLoss: 0.14138178527355194\n",
      "\tLoss: 0.17194581031799316\n",
      "\tLoss: 0.10303367674350739\n",
      "\tLoss: 0.20623454451560974\n",
      "\tLoss: 0.10273423790931702\n",
      "\tLoss: 0.11324512213468552\n",
      "\tLoss: 0.160763680934906\n",
      "\tLoss: 0.08133558928966522\n",
      "\tLoss: 0.12714320421218872\n",
      "\tLoss: 0.136265367269516\n",
      "\tLoss: 0.1728591024875641\n",
      "\tLoss: 0.13014522194862366\n",
      "\tLoss: 0.09045950323343277\n",
      "\tLoss: 0.11154112219810486\n",
      "\tLoss: 0.11854920536279678\n",
      "\tLoss: 0.10825672745704651\n",
      "\tLoss: 0.07135127484798431\n",
      "\tLoss: 0.06811400502920151\n",
      "\tLoss: 0.08440408110618591\n",
      "\tLoss: 0.15058749914169312\n",
      "\tLoss: 0.11509449779987335\n",
      "\tLoss: 0.14377310872077942\n",
      "\tLoss: 0.12137387692928314\n",
      "\tLoss: 0.11855372786521912\n",
      "\tLoss: 0.09347105771303177\n",
      "\tLoss: 0.08200456202030182\n",
      "\tLoss: 0.11754333227872849\n",
      "\tLoss: 0.12899890542030334\n",
      "\tLoss: 0.08814823627471924\n",
      "\tLoss: 0.10794561356306076\n",
      "\tLoss: 0.12373897433280945\n",
      "\tLoss: 0.11569425463676453\n",
      "\tLoss: 0.10932011157274246\n",
      "\tLoss: 0.12422794103622437\n",
      "\tLoss: 0.16455793380737305\n",
      "[time] Epoch 21: 420.7032583048567s = 7.011720971747612m\n",
      "\n",
      "Epoch 22...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.1171594113111496\n",
      "\tLoss: 0.09819720685482025\n",
      "\tLoss: 0.11884463578462601\n",
      "\tLoss: 0.09442292153835297\n",
      "\tLoss: 0.15593118965625763\n",
      "\tLoss: 0.17666593194007874\n",
      "\tLoss: 0.09109094738960266\n",
      "\tLoss: 0.15730020403862\n",
      "\tLoss: 0.10761509835720062\n",
      "\tLoss: 0.1251615583896637\n",
      "\tLoss: 0.13660332560539246\n",
      "\tLoss: 0.11820687353610992\n",
      "\tLoss: 0.07928058505058289\n",
      "\tLoss: 0.11513836681842804\n",
      "\tLoss: 0.1028963029384613\n",
      "\tLoss: 0.14452698826789856\n",
      "\tLoss: 0.11965131759643555\n",
      "\tLoss: 0.15549953281879425\n",
      "\tLoss: 0.12502320110797882\n",
      "\tLoss: 0.15949568152427673\n",
      "\tLoss: 0.09795849770307541\n",
      "\tLoss: 0.09241049736738205\n",
      "\tLoss: 0.16213268041610718\n",
      "\tLoss: 0.1510079950094223\n",
      "\tLoss: 0.09683472663164139\n",
      "\tLoss: 0.1678447276353836\n",
      "\tLoss: 0.13888736069202423\n",
      "\tLoss: 0.12306272983551025\n",
      "\tLoss: 0.12205423414707184\n",
      "\tLoss: 0.14018657803535461\n",
      "\tLoss: 0.06307897716760635\n",
      "\tLoss: 0.11166386306285858\n",
      "\tLoss: 0.10543544590473175\n",
      "\tLoss: 0.08234123140573502\n",
      "\tLoss: 0.11547260731458664\n",
      "\tLoss: 0.09859149158000946\n",
      "\tLoss: 0.1889738142490387\n",
      "\tLoss: 0.15124234557151794\n",
      "\tLoss: 0.08795147389173508\n",
      "\tLoss: 0.1429262012243271\n",
      "\tLoss: 0.1822415143251419\n",
      "\tLoss: 0.06950554251670837\n",
      "\tLoss: 0.09241492301225662\n",
      "\tLoss: 0.14709222316741943\n",
      "\tLoss: 0.0772116556763649\n",
      "\tLoss: 0.12324193120002747\n",
      "\tLoss: 0.10657991468906403\n",
      "\tLoss: 0.14752422273159027\n",
      "\tLoss: 0.18506360054016113\n",
      "\tLoss: 0.09772295504808426\n",
      "\tLoss: 0.12167590856552124\n",
      "\tLoss: 0.15407948195934296\n",
      "\tLoss: 0.11050061136484146\n",
      "\tLoss: 0.15320640802383423\n",
      "\tLoss: 0.1427336186170578\n",
      "\tLoss: 0.10547050833702087\n",
      "\tLoss: 0.054584961384534836\n",
      "\tLoss: 0.1127626895904541\n",
      "\tLoss: 0.12633264064788818\n",
      "\tLoss: 0.0930740237236023\n",
      "\tLoss: 0.14793327450752258\n",
      "\tLoss: 0.08584867417812347\n",
      "\tLoss: 0.09682115912437439\n",
      "\tLoss: 0.08540098369121552\n",
      "\tLoss: 0.09141965210437775\n",
      "\tLoss: 0.13983750343322754\n",
      "\tLoss: 0.11037468910217285\n",
      "\tLoss: 0.09180108457803726\n",
      "\tLoss: 0.09120543301105499\n",
      "\tLoss: 0.13303473591804504\n",
      "\tLoss: 0.07376916706562042\n",
      "\tLoss: 0.11041313409805298\n",
      "\tLoss: 0.13251082599163055\n",
      "\tLoss: 0.111544668674469\n",
      "\tLoss: 0.1707143634557724\n",
      "\tLoss: 0.10096792131662369\n",
      "\tLoss: 0.06693918257951736\n",
      "\tLoss: 0.11313973367214203\n",
      "\tLoss: 0.08429557830095291\n",
      "\tLoss: 0.15431073307991028\n",
      "\tLoss: 0.07693921774625778\n",
      "\tLoss: 0.10620521754026413\n",
      "\tLoss: 0.06752458214759827\n",
      "\tLoss: 0.08162753283977509\n",
      "\tLoss: 0.09201733022928238\n",
      "\tLoss: 0.08131881803274155\n",
      "\tLoss: 0.14727210998535156\n",
      "\tLoss: 0.15528200566768646\n",
      "\tLoss: 0.07835164666175842\n",
      "\tLoss: 0.185337632894516\n",
      "\tLoss: 0.1418396532535553\n",
      "\tLoss: 0.15445750951766968\n",
      "\tLoss: 0.09290837496519089\n",
      "\tLoss: 0.13645324110984802\n",
      "\tLoss: 0.11562363058328629\n",
      "\tLoss: 0.11295054852962494\n",
      "\tLoss: 0.09839310497045517\n",
      "\tLoss: 0.08081622421741486\n",
      "\tLoss: 0.10139797627925873\n",
      "\tLoss: 0.12075944244861603\n",
      "\tLoss: 0.06658212840557098\n",
      "\tLoss: 0.09282751381397247\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.08139275014400482\n",
      "\tLoss: 0.11073361337184906\n",
      "\tLoss: 0.11785061657428741\n",
      "\tLoss: 0.11316400021314621\n",
      "\tLoss: 0.12375646829605103\n",
      "\tLoss: 0.08506754785776138\n",
      "\tLoss: 0.08831215649843216\n",
      "\tLoss: 0.07076731324195862\n",
      "\tLoss: 0.1125592365860939\n",
      "\tLoss: 0.14000225067138672\n",
      "\tLoss: 0.10284949839115143\n",
      "\tLoss: 0.09778593480587006\n",
      "\tLoss: 0.11651664972305298\n",
      "\tLoss: 0.13678672909736633\n",
      "\tLoss: 0.14133340120315552\n",
      "\tLoss: 0.13029736280441284\n",
      "\tLoss: 0.15236027538776398\n",
      "\tLoss: 0.08612899482250214\n",
      "\tLoss: 0.14239095151424408\n",
      "\tLoss: 0.13924726843833923\n",
      "\tLoss: 0.08700966835021973\n",
      "\tLoss: 0.16865399479866028\n",
      "\tLoss: 0.09068699181079865\n",
      "\tLoss: 0.11265982687473297\n",
      "\tLoss: 0.10564592480659485\n",
      "\tLoss: 0.08900319039821625\n",
      "\tLoss: 0.11490871757268906\n",
      "\tLoss: 0.15206703543663025\n",
      "\tLoss: 0.18028822541236877\n",
      "\tLoss: 0.10715887695550919\n",
      "\tLoss: 0.14960698783397675\n",
      "\tLoss: 0.10034202039241791\n",
      "\tLoss: 0.13315331935882568\n",
      "\tLoss: 0.14525961875915527\n",
      "\tLoss: 0.0802089124917984\n",
      "\tLoss: 0.13052202761173248\n",
      "\tLoss: 0.1288415938615799\n",
      "\tLoss: 0.13673925399780273\n",
      "\tLoss: 0.13560271263122559\n",
      "\tLoss: 0.0922652930021286\n",
      "\tLoss: 0.13379760086536407\n",
      "\tLoss: 0.13186661899089813\n",
      "\tLoss: 0.09976142644882202\n",
      "\tLoss: 0.12568235397338867\n",
      "\tLoss: 0.14088627696037292\n",
      "\tLoss: 0.12384459376335144\n",
      "\tLoss: 0.08460499346256256\n",
      "\tLoss: 0.08372008800506592\n",
      "\tLoss: 0.09632530808448792\n",
      "\tLoss: 0.09904973208904266\n",
      "\tLoss: 0.12352301180362701\n",
      "\tLoss: 0.1590486764907837\n",
      "\tLoss: 0.12209613621234894\n",
      "\tLoss: 0.12019412219524384\n",
      "\tLoss: 0.10511156916618347\n",
      "\tLoss: 0.14305844902992249\n",
      "\tLoss: 0.15980687737464905\n",
      "\tLoss: 0.1578279435634613\n",
      "\tLoss: 0.06269748508930206\n",
      "\tLoss: 0.098744235932827\n",
      "\tLoss: 0.2117365151643753\n",
      "\tLoss: 0.12917904555797577\n",
      "\tLoss: 0.0627659261226654\n",
      "\tLoss: 0.09352602064609528\n",
      "\tLoss: 0.09103512763977051\n",
      "\tLoss: 0.11031509935855865\n",
      "\tLoss: 0.09325666725635529\n",
      "\tLoss: 0.10778047889471054\n",
      "\tLoss: 0.0799633041024208\n",
      "\tLoss: 0.1463361531496048\n",
      "\tLoss: 0.11150528490543365\n",
      "\tLoss: 0.07151885330677032\n",
      "\tLoss: 0.12155701220035553\n",
      "\tLoss: 0.13629856705665588\n",
      "\tLoss: 0.0876537561416626\n",
      "\tLoss: 0.06513319909572601\n",
      "\tLoss: 0.12530486285686493\n",
      "\tLoss: 0.12442393600940704\n",
      "\tLoss: 0.16900920867919922\n",
      "\tLoss: 0.09594487398862839\n",
      "\tLoss: 0.12664459645748138\n",
      "\tLoss: 0.12751087546348572\n",
      "\tLoss: 0.18119803071022034\n",
      "\tLoss: 0.1370968520641327\n",
      "\tLoss: 0.08023493736982346\n",
      "\tLoss: 0.12126290798187256\n",
      "\tLoss: 0.14560173451900482\n",
      "\tLoss: 0.177618145942688\n",
      "\tLoss: 0.09099853038787842\n",
      "\tLoss: 0.18348228931427002\n",
      "\tLoss: 0.0711195170879364\n",
      "\tLoss: 0.14819778501987457\n",
      "\tLoss: 0.11867301166057587\n",
      "\tLoss: 0.19150382280349731\n",
      "\tLoss: 0.13005736470222473\n",
      "\tLoss: 0.12669110298156738\n",
      "\tLoss: 0.12801659107208252\n",
      "\tLoss: 0.15802226960659027\n",
      "\tLoss: 0.10064788162708282\n",
      "\tLoss: 0.11463569104671478\n",
      "\tLoss: 0.12671563029289246\n",
      "\tLoss: 0.15482431650161743\n",
      "[time] Epoch 22: 427.4663051920943s = 7.1244384198682384m\n",
      "\n",
      "Epoch 23...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.12863822281360626\n",
      "\tLoss: 0.1419466733932495\n",
      "\tLoss: 0.10035311430692673\n",
      "\tLoss: 0.06396378576755524\n",
      "\tLoss: 0.1394863873720169\n",
      "\tLoss: 0.08089000731706619\n",
      "\tLoss: 0.12219040095806122\n",
      "\tLoss: 0.14933684468269348\n",
      "\tLoss: 0.10373684763908386\n",
      "\tLoss: 0.06811683624982834\n",
      "\tLoss: 0.11592447757720947\n",
      "\tLoss: 0.09400653839111328\n",
      "\tLoss: 0.13275295495986938\n",
      "\tLoss: 0.16363567113876343\n",
      "\tLoss: 0.09461966902017593\n",
      "\tLoss: 0.13262872397899628\n",
      "\tLoss: 0.12363290786743164\n",
      "\tLoss: 0.07989338040351868\n",
      "\tLoss: 0.07721947878599167\n",
      "\tLoss: 0.12140519917011261\n",
      "\tLoss: 0.08146413415670395\n",
      "\tLoss: 0.09732885658740997\n",
      "\tLoss: 0.10691569745540619\n",
      "\tLoss: 0.09116077423095703\n",
      "\tLoss: 0.15027658641338348\n",
      "\tLoss: 0.1191033273935318\n",
      "\tLoss: 0.08046652376651764\n",
      "\tLoss: 0.12066537141799927\n",
      "\tLoss: 0.09835128486156464\n",
      "\tLoss: 0.12374936044216156\n",
      "\tLoss: 0.1292586624622345\n",
      "\tLoss: 0.09642795473337173\n",
      "\tLoss: 0.16451433300971985\n",
      "\tLoss: 0.08734424412250519\n",
      "\tLoss: 0.13008953630924225\n",
      "\tLoss: 0.07389552891254425\n",
      "\tLoss: 0.09039756655693054\n",
      "\tLoss: 0.16311046481132507\n",
      "\tLoss: 0.12261330336332321\n",
      "\tLoss: 0.07030465453863144\n",
      "\tLoss: 0.1337267905473709\n",
      "\tLoss: 0.0860556811094284\n",
      "\tLoss: 0.11118344962596893\n",
      "\tLoss: 0.1385289877653122\n",
      "\tLoss: 0.10655021667480469\n",
      "\tLoss: 0.15466085076332092\n",
      "\tLoss: 0.10853998363018036\n",
      "\tLoss: 0.09796090424060822\n",
      "\tLoss: 0.12064487487077713\n",
      "\tLoss: 0.12344807386398315\n",
      "\tLoss: 0.13374069333076477\n",
      "\tLoss: 0.08234712481498718\n",
      "\tLoss: 0.06709887832403183\n",
      "\tLoss: 0.1271497905254364\n",
      "\tLoss: 0.14227861166000366\n",
      "\tLoss: 0.0956985354423523\n",
      "\tLoss: 0.11125468462705612\n",
      "\tLoss: 0.10000742226839066\n",
      "\tLoss: 0.08803226053714752\n",
      "\tLoss: 0.1209544688463211\n",
      "\tLoss: 0.08899103850126266\n",
      "\tLoss: 0.09865827858448029\n",
      "\tLoss: 0.08367186039686203\n",
      "\tLoss: 0.09031936526298523\n",
      "\tLoss: 0.10107754170894623\n",
      "\tLoss: 0.08250720053911209\n",
      "\tLoss: 0.10852847993373871\n",
      "\tLoss: 0.1674283742904663\n",
      "\tLoss: 0.14290955662727356\n",
      "\tLoss: 0.12079409509897232\n",
      "\tLoss: 0.07394255697727203\n",
      "\tLoss: 0.07823897898197174\n",
      "\tLoss: 0.17698457837104797\n",
      "\tLoss: 0.10367164015769958\n",
      "\tLoss: 0.13636106252670288\n",
      "\tLoss: 0.13479618728160858\n",
      "\tLoss: 0.1097581684589386\n",
      "\tLoss: 0.12851347029209137\n",
      "\tLoss: 0.09879656136035919\n",
      "\tLoss: 0.07690498232841492\n",
      "\tLoss: 0.11887159943580627\n",
      "\tLoss: 0.10988789796829224\n",
      "\tLoss: 0.11289820075035095\n",
      "\tLoss: 0.17082813382148743\n",
      "\tLoss: 0.1389072835445404\n",
      "\tLoss: 0.10347464680671692\n",
      "\tLoss: 0.08157145977020264\n",
      "\tLoss: 0.08233281970024109\n",
      "\tLoss: 0.07939014583826065\n",
      "\tLoss: 0.06856881082057953\n",
      "\tLoss: 0.09776786714792252\n",
      "\tLoss: 0.11801686882972717\n",
      "\tLoss: 0.1160447895526886\n",
      "\tLoss: 0.11355417221784592\n",
      "\tLoss: 0.06820469349622726\n",
      "\tLoss: 0.08244381099939346\n",
      "\tLoss: 0.14389684796333313\n",
      "\tLoss: 0.09226549416780472\n",
      "\tLoss: 0.10835197567939758\n",
      "\tLoss: 0.10752033442258835\n",
      "\tLoss: 0.08162127435207367\n",
      "\tLoss: 0.13731998205184937\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.07471449673175812\n",
      "\tLoss: 0.1485358476638794\n",
      "\tLoss: 0.13794390857219696\n",
      "\tLoss: 0.1108824610710144\n",
      "\tLoss: 0.07992972433567047\n",
      "\tLoss: 0.1459532082080841\n",
      "\tLoss: 0.07778334617614746\n",
      "\tLoss: 0.10649222135543823\n",
      "\tLoss: 0.10607191175222397\n",
      "\tLoss: 0.09909898042678833\n",
      "\tLoss: 0.07778856158256531\n",
      "\tLoss: 0.12573650479316711\n",
      "\tLoss: 0.132301926612854\n",
      "\tLoss: 0.10686437785625458\n",
      "\tLoss: 0.0768631175160408\n",
      "\tLoss: 0.1844920516014099\n",
      "\tLoss: 0.09660162776708603\n",
      "\tLoss: 0.09861202538013458\n",
      "\tLoss: 0.1436799168586731\n",
      "\tLoss: 0.1505897343158722\n",
      "\tLoss: 0.1250576376914978\n",
      "\tLoss: 0.11246437579393387\n",
      "\tLoss: 0.15450519323349\n",
      "\tLoss: 0.12249214947223663\n",
      "\tLoss: 0.13951539993286133\n",
      "\tLoss: 0.11906564235687256\n",
      "\tLoss: 0.10283467173576355\n",
      "\tLoss: 0.190885990858078\n",
      "\tLoss: 0.2004750370979309\n",
      "\tLoss: 0.10517337918281555\n",
      "\tLoss: 0.14171797037124634\n",
      "\tLoss: 0.11873964965343475\n",
      "\tLoss: 0.11967778205871582\n",
      "\tLoss: 0.14297428727149963\n",
      "\tLoss: 0.09110602736473083\n",
      "\tLoss: 0.13054101169109344\n",
      "\tLoss: 0.0962848961353302\n",
      "\tLoss: 0.11919277906417847\n",
      "\tLoss: 0.07919326424598694\n",
      "\tLoss: 0.10001428425312042\n",
      "\tLoss: 0.12649188935756683\n",
      "\tLoss: 0.12090203911066055\n",
      "\tLoss: 0.1345517784357071\n",
      "\tLoss: 0.12712588906288147\n",
      "\tLoss: 0.1791035383939743\n",
      "\tLoss: 0.09965784847736359\n",
      "\tLoss: 0.10517426580190659\n",
      "\tLoss: 0.13360227644443512\n",
      "\tLoss: 0.16099654138088226\n",
      "\tLoss: 0.08061414957046509\n",
      "\tLoss: 0.1394277811050415\n",
      "\tLoss: 0.11287451535463333\n",
      "\tLoss: 0.0786823034286499\n",
      "\tLoss: 0.06825906038284302\n",
      "\tLoss: 0.10272201150655746\n",
      "\tLoss: 0.13904494047164917\n",
      "\tLoss: 0.145310640335083\n",
      "\tLoss: 0.1268320232629776\n",
      "\tLoss: 0.10143185406923294\n",
      "\tLoss: 0.09597712755203247\n",
      "\tLoss: 0.16104340553283691\n",
      "\tLoss: 0.15902259945869446\n",
      "\tLoss: 0.07948121428489685\n",
      "\tLoss: 0.10425934940576553\n",
      "\tLoss: 0.07411156594753265\n",
      "\tLoss: 0.0962124615907669\n",
      "\tLoss: 0.09586254507303238\n",
      "\tLoss: 0.10559867322444916\n",
      "\tLoss: 0.1138310432434082\n",
      "\tLoss: 0.07697845250368118\n",
      "\tLoss: 0.12026908993721008\n",
      "\tLoss: 0.16876260936260223\n",
      "\tLoss: 0.11078868806362152\n",
      "\tLoss: 0.14386799931526184\n",
      "\tLoss: 0.07683460414409637\n",
      "\tLoss: 0.07287837564945221\n",
      "\tLoss: 0.1470184624195099\n",
      "\tLoss: 0.18145465850830078\n",
      "\tLoss: 0.11436212062835693\n",
      "\tLoss: 0.09371320903301239\n",
      "\tLoss: 0.12179448455572128\n",
      "\tLoss: 0.078052818775177\n",
      "\tLoss: 0.1127152219414711\n",
      "\tLoss: 0.1339160054922104\n",
      "\tLoss: 0.10038052499294281\n",
      "\tLoss: 0.057681821286678314\n",
      "\tLoss: 0.1063542515039444\n",
      "\tLoss: 0.1129441186785698\n",
      "\tLoss: 0.12410762906074524\n",
      "\tLoss: 0.14399220049381256\n",
      "\tLoss: 0.12451979517936707\n",
      "\tLoss: 0.15041708946228027\n",
      "\tLoss: 0.09388163685798645\n",
      "\tLoss: 0.09097743034362793\n",
      "\tLoss: 0.10644830018281937\n",
      "\tLoss: 0.1210521012544632\n",
      "\tLoss: 0.07672256231307983\n",
      "\tLoss: 0.0922858715057373\n",
      "\tLoss: 0.11700467765331268\n",
      "\tLoss: 0.1542244255542755\n",
      "\tLoss: 0.1807747185230255\n",
      "\tLoss: 0.1589224934577942\n",
      "[time] Epoch 23: 422.9522283389233s = 7.049203805648721m\n",
      "\n",
      "Epoch 24...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.10457083582878113\n",
      "\tLoss: 0.16910313069820404\n",
      "\tLoss: 0.08325421810150146\n",
      "\tLoss: 0.06804631650447845\n",
      "\tLoss: 0.0981912612915039\n",
      "\tLoss: 0.1455029547214508\n",
      "\tLoss: 0.09535546600818634\n",
      "\tLoss: 0.15351469814777374\n",
      "\tLoss: 0.09885260462760925\n",
      "\tLoss: 0.0901300311088562\n",
      "\tLoss: 0.11870820820331573\n",
      "\tLoss: 0.14064086973667145\n",
      "\tLoss: 0.10424693673849106\n",
      "\tLoss: 0.0902002826333046\n",
      "\tLoss: 0.11938229203224182\n",
      "\tLoss: 0.20851629972457886\n",
      "\tLoss: 0.12001413851976395\n",
      "\tLoss: 0.08066647499799728\n",
      "\tLoss: 0.05513086915016174\n",
      "\tLoss: 0.10651266574859619\n",
      "\tLoss: 0.1089906245470047\n",
      "\tLoss: 0.12348116934299469\n",
      "\tLoss: 0.11756286770105362\n",
      "\tLoss: 0.12646952271461487\n",
      "\tLoss: 0.09954655915498734\n",
      "\tLoss: 0.1829385906457901\n",
      "\tLoss: 0.09090626984834671\n",
      "\tLoss: 0.11385872960090637\n",
      "\tLoss: 0.12782251834869385\n",
      "\tLoss: 0.09638825058937073\n",
      "\tLoss: 0.1003500372171402\n",
      "\tLoss: 0.09790254384279251\n",
      "\tLoss: 0.06164170056581497\n",
      "\tLoss: 0.11877572536468506\n",
      "\tLoss: 0.06447924673557281\n",
      "\tLoss: 0.08550090342760086\n",
      "\tLoss: 0.07847914099693298\n",
      "\tLoss: 0.10601052641868591\n",
      "\tLoss: 0.1456335335969925\n",
      "\tLoss: 0.06137185916304588\n",
      "\tLoss: 0.10044898092746735\n",
      "\tLoss: 0.12062615156173706\n",
      "\tLoss: 0.17717939615249634\n",
      "\tLoss: 0.09711036831140518\n",
      "\tLoss: 0.16224147379398346\n",
      "\tLoss: 0.04792407155036926\n",
      "\tLoss: 0.09302729368209839\n",
      "\tLoss: 0.09451723098754883\n",
      "\tLoss: 0.1049586609005928\n",
      "\tLoss: 0.1175864189863205\n",
      "\tLoss: 0.0937337726354599\n",
      "\tLoss: 0.133323073387146\n",
      "\tLoss: 0.11554810404777527\n",
      "\tLoss: 0.13729391992092133\n",
      "\tLoss: 0.11009954661130905\n",
      "\tLoss: 0.10362151265144348\n",
      "\tLoss: 0.08385913074016571\n",
      "\tLoss: 0.09334554523229599\n",
      "\tLoss: 0.14546158909797668\n",
      "\tLoss: 0.13667064905166626\n",
      "\tLoss: 0.11803805828094482\n",
      "\tLoss: 0.06260329484939575\n",
      "\tLoss: 0.1322164386510849\n",
      "\tLoss: 0.11989700794219971\n",
      "\tLoss: 0.14286814630031586\n",
      "\tLoss: 0.12525469064712524\n",
      "\tLoss: 0.09557493776082993\n",
      "\tLoss: 0.10803843289613724\n",
      "\tLoss: 0.09519220143556595\n",
      "\tLoss: 0.10651405155658722\n",
      "\tLoss: 0.1590915471315384\n",
      "\tLoss: 0.09626960009336472\n",
      "\tLoss: 0.14882537722587585\n",
      "\tLoss: 0.052782341837882996\n",
      "\tLoss: 0.12187675386667252\n",
      "\tLoss: 0.10729839652776718\n",
      "\tLoss: 0.1194036453962326\n",
      "\tLoss: 0.12054219841957092\n",
      "\tLoss: 0.1507563591003418\n",
      "\tLoss: 0.05695381015539169\n",
      "\tLoss: 0.073818638920784\n",
      "\tLoss: 0.11253704130649567\n",
      "\tLoss: 0.16022558510303497\n",
      "\tLoss: 0.0955907329916954\n",
      "\tLoss: 0.12269920110702515\n",
      "\tLoss: 0.15574273467063904\n",
      "\tLoss: 0.09077538549900055\n",
      "\tLoss: 0.09519896656274796\n",
      "\tLoss: 0.11518151313066483\n",
      "\tLoss: 0.11197304725646973\n",
      "\tLoss: 0.1258280724287033\n",
      "\tLoss: 0.08806423842906952\n",
      "\tLoss: 0.0659022405743599\n",
      "\tLoss: 0.07930346578359604\n",
      "\tLoss: 0.1269684135913849\n",
      "\tLoss: 0.1326659768819809\n",
      "\tLoss: 0.11476067453622818\n",
      "\tLoss: 0.0634508803486824\n",
      "\tLoss: 0.10309784859418869\n",
      "\tLoss: 0.08539535105228424\n",
      "\tLoss: 0.16755539178848267\n",
      "\tLoss: 0.1119670495390892\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.1032344177365303\n",
      "\tLoss: 0.12012071907520294\n",
      "\tLoss: 0.19583016633987427\n",
      "\tLoss: 0.07211516797542572\n",
      "\tLoss: 0.11699832230806351\n",
      "\tLoss: 0.0817478746175766\n",
      "\tLoss: 0.14472395181655884\n",
      "\tLoss: 0.10891294479370117\n",
      "\tLoss: 0.11469939351081848\n",
      "\tLoss: 0.1369217038154602\n",
      "\tLoss: 0.10828064382076263\n",
      "\tLoss: 0.17097894847393036\n",
      "\tLoss: 0.12452710419893265\n",
      "\tLoss: 0.09658444672822952\n",
      "\tLoss: 0.05323135480284691\n",
      "\tLoss: 0.11921028047800064\n",
      "\tLoss: 0.07129698991775513\n",
      "\tLoss: 0.07735013961791992\n",
      "\tLoss: 0.06290453672409058\n",
      "\tLoss: 0.11873674392700195\n",
      "\tLoss: 0.11772242188453674\n",
      "\tLoss: 0.0838886946439743\n",
      "\tLoss: 0.08308820426464081\n",
      "\tLoss: 0.13916391134262085\n",
      "\tLoss: 0.10497386753559113\n",
      "\tLoss: 0.13732269406318665\n",
      "\tLoss: 0.16700424253940582\n",
      "\tLoss: 0.12404489517211914\n",
      "\tLoss: 0.10569572448730469\n",
      "\tLoss: 0.10310821235179901\n",
      "\tLoss: 0.1272568702697754\n",
      "\tLoss: 0.20442019402980804\n",
      "\tLoss: 0.09957727789878845\n",
      "\tLoss: 0.11642194539308548\n",
      "\tLoss: 0.16121327877044678\n",
      "\tLoss: 0.11535361409187317\n",
      "\tLoss: 0.13780249655246735\n",
      "\tLoss: 0.09914970397949219\n",
      "\tLoss: 0.1232832670211792\n",
      "\tLoss: 0.12241119146347046\n",
      "\tLoss: 0.12290714681148529\n",
      "\tLoss: 0.08846253156661987\n",
      "\tLoss: 0.14090554416179657\n",
      "\tLoss: 0.1024356335401535\n",
      "\tLoss: 0.15624390542507172\n",
      "\tLoss: 0.13711029291152954\n",
      "\tLoss: 0.09999678283929825\n",
      "\tLoss: 0.10055769979953766\n",
      "\tLoss: 0.1374557763338089\n",
      "\tLoss: 0.07802928239107132\n",
      "\tLoss: 0.1068984717130661\n",
      "\tLoss: 0.09402255713939667\n",
      "\tLoss: 0.15669827163219452\n",
      "\tLoss: 0.13299325108528137\n",
      "\tLoss: 0.08586680889129639\n",
      "\tLoss: 0.11003004014492035\n",
      "\tLoss: 0.11784745752811432\n",
      "\tLoss: 0.0865570455789566\n",
      "\tLoss: 0.09815484285354614\n",
      "\tLoss: 0.08762189000844955\n",
      "\tLoss: 0.09019589424133301\n",
      "\tLoss: 0.08001521974802017\n",
      "\tLoss: 0.13603676855564117\n",
      "\tLoss: 0.08275870978832245\n",
      "\tLoss: 0.10287690162658691\n",
      "\tLoss: 0.11215940117835999\n",
      "\tLoss: 0.05526799336075783\n",
      "\tLoss: 0.14011292159557343\n",
      "\tLoss: 0.1082695722579956\n",
      "\tLoss: 0.10418305546045303\n",
      "\tLoss: 0.13283367455005646\n",
      "\tLoss: 0.11413317918777466\n",
      "\tLoss: 0.10775235295295715\n",
      "\tLoss: 0.09916957467794418\n",
      "\tLoss: 0.10606005787849426\n",
      "\tLoss: 0.08219822496175766\n",
      "\tLoss: 0.048563599586486816\n",
      "\tLoss: 0.1027948260307312\n",
      "\tLoss: 0.12153901904821396\n",
      "\tLoss: 0.1357913613319397\n",
      "\tLoss: 0.1382945328950882\n",
      "\tLoss: 0.12979240715503693\n",
      "\tLoss: 0.15757057070732117\n",
      "\tLoss: 0.11661530286073685\n",
      "\tLoss: 0.18644964694976807\n",
      "\tLoss: 0.14069592952728271\n",
      "\tLoss: 0.10286954790353775\n",
      "\tLoss: 0.13867032527923584\n",
      "\tLoss: 0.10409042984247208\n",
      "\tLoss: 0.0913340300321579\n",
      "\tLoss: 0.1306610405445099\n",
      "\tLoss: 0.12122151255607605\n",
      "\tLoss: 0.15231415629386902\n",
      "\tLoss: 0.15085293352603912\n",
      "\tLoss: 0.06523667275905609\n",
      "\tLoss: 0.12509113550186157\n",
      "\tLoss: 0.08815157413482666\n",
      "\tLoss: 0.15190453827381134\n",
      "\tLoss: 0.13820692896842957\n",
      "\tLoss: 0.07645662128925323\n",
      "\tLoss: 0.14193186163902283\n",
      "\tLoss: 0.09645326435565948\n",
      "[time] Epoch 24: 422.71234737103805s = 7.045205789517301m\n",
      "\n",
      "Epoch 25...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.08218991756439209\n",
      "\tLoss: 0.05942435562610626\n",
      "\tLoss: 0.0922517403960228\n",
      "\tLoss: 0.07054797559976578\n",
      "\tLoss: 0.09233922511339188\n",
      "\tLoss: 0.08427059650421143\n",
      "\tLoss: 0.08127292990684509\n",
      "\tLoss: 0.1398717164993286\n",
      "\tLoss: 0.17356154322624207\n",
      "\tLoss: 0.08976341784000397\n",
      "\tLoss: 0.1449396312236786\n",
      "\tLoss: 0.13788026571273804\n",
      "\tLoss: 0.09032590687274933\n",
      "\tLoss: 0.09854556620121002\n",
      "\tLoss: 0.1399535834789276\n",
      "\tLoss: 0.12134961038827896\n",
      "\tLoss: 0.13663747906684875\n",
      "\tLoss: 0.10237215459346771\n",
      "\tLoss: 0.1191476434469223\n",
      "\tLoss: 0.126792773604393\n",
      "\tLoss: 0.1599828600883484\n",
      "\tLoss: 0.14567974209785461\n",
      "\tLoss: 0.12056785821914673\n",
      "\tLoss: 0.12780678272247314\n",
      "\tLoss: 0.11168579012155533\n",
      "\tLoss: 0.12508037686347961\n",
      "\tLoss: 0.10305322706699371\n",
      "\tLoss: 0.04579757899045944\n",
      "\tLoss: 0.14271387457847595\n",
      "\tLoss: 0.17885494232177734\n",
      "\tLoss: 0.06812337785959244\n",
      "\tLoss: 0.09007194638252258\n",
      "\tLoss: 0.10302041471004486\n",
      "\tLoss: 0.17216911911964417\n",
      "\tLoss: 0.07948515564203262\n",
      "\tLoss: 0.1291855424642563\n",
      "\tLoss: 0.08370961248874664\n",
      "\tLoss: 0.10719294100999832\n",
      "\tLoss: 0.14297010004520416\n",
      "\tLoss: 0.07670413702726364\n",
      "\tLoss: 0.10660605132579803\n",
      "\tLoss: 0.11351840198040009\n",
      "\tLoss: 0.12844115495681763\n",
      "\tLoss: 0.07047364860773087\n",
      "\tLoss: 0.10472625494003296\n",
      "\tLoss: 0.07530452311038971\n",
      "\tLoss: 0.10853259265422821\n",
      "\tLoss: 0.11163003742694855\n",
      "\tLoss: 0.1585601568222046\n",
      "\tLoss: 0.10064531862735748\n",
      "\tLoss: 0.11565817147493362\n",
      "\tLoss: 0.09832419455051422\n",
      "\tLoss: 0.09593351185321808\n",
      "\tLoss: 0.11434254050254822\n",
      "\tLoss: 0.1573183238506317\n",
      "\tLoss: 0.1746494472026825\n",
      "\tLoss: 0.11679163575172424\n",
      "\tLoss: 0.16367314755916595\n",
      "\tLoss: 0.1772249937057495\n",
      "\tLoss: 0.1584967076778412\n",
      "\tLoss: 0.1346178650856018\n",
      "\tLoss: 0.10785239934921265\n",
      "\tLoss: 0.13168998062610626\n",
      "\tLoss: 0.1114410012960434\n",
      "\tLoss: 0.1055438295006752\n",
      "\tLoss: 0.1509285867214203\n",
      "\tLoss: 0.1767999827861786\n",
      "\tLoss: 0.12441601604223251\n",
      "\tLoss: 0.12091389298439026\n",
      "\tLoss: 0.0971779152750969\n",
      "\tLoss: 0.1644279956817627\n",
      "\tLoss: 0.12558400630950928\n",
      "\tLoss: 0.08460060507059097\n",
      "\tLoss: 0.09955843538045883\n",
      "\tLoss: 0.111299529671669\n",
      "\tLoss: 0.09479294717311859\n",
      "\tLoss: 0.07147133350372314\n",
      "\tLoss: 0.10712562501430511\n",
      "\tLoss: 0.13627368211746216\n",
      "\tLoss: 0.12792657315731049\n",
      "\tLoss: 0.10222363471984863\n",
      "\tLoss: 0.07832194119691849\n",
      "\tLoss: 0.12360360473394394\n",
      "\tLoss: 0.10176743566989899\n",
      "\tLoss: 0.12644599378108978\n",
      "\tLoss: 0.10129380971193314\n",
      "\tLoss: 0.1238778829574585\n",
      "\tLoss: 0.11365915089845657\n",
      "\tLoss: 0.08443356305360794\n",
      "\tLoss: 0.08939629048109055\n",
      "\tLoss: 0.1342390477657318\n",
      "\tLoss: 0.1528666913509369\n",
      "\tLoss: 0.12155519425868988\n",
      "\tLoss: 0.11580097675323486\n",
      "\tLoss: 0.1587749421596527\n",
      "\tLoss: 0.09972245991230011\n",
      "\tLoss: 0.1358497440814972\n",
      "\tLoss: 0.10464955866336823\n",
      "\tLoss: 0.09177657216787338\n",
      "\tLoss: 0.07751360535621643\n",
      "\tLoss: 0.12598708271980286\n",
      "\tLoss: 0.1994597315788269\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.12866264581680298\n",
      "\tLoss: 0.12393537163734436\n",
      "\tLoss: 0.07918906956911087\n",
      "\tLoss: 0.1240323930978775\n",
      "\tLoss: 0.0942571759223938\n",
      "\tLoss: 0.10396170616149902\n",
      "\tLoss: 0.0745328888297081\n",
      "\tLoss: 0.09928761422634125\n",
      "\tLoss: 0.12010614573955536\n",
      "\tLoss: 0.10059908032417297\n",
      "\tLoss: 0.07130272686481476\n",
      "\tLoss: 0.15625670552253723\n",
      "\tLoss: 0.10892940312623978\n",
      "\tLoss: 0.08425290882587433\n",
      "\tLoss: 0.10975827276706696\n",
      "\tLoss: 0.1911468505859375\n",
      "\tLoss: 0.1108657717704773\n",
      "\tLoss: 0.14376896619796753\n",
      "\tLoss: 0.11158296465873718\n",
      "\tLoss: 0.13734838366508484\n",
      "\tLoss: 0.110012486577034\n",
      "\tLoss: 0.09878727793693542\n",
      "\tLoss: 0.1797812283039093\n",
      "\tLoss: 0.1383979320526123\n",
      "\tLoss: 0.13610585033893585\n",
      "\tLoss: 0.14224031567573547\n",
      "\tLoss: 0.14183957874774933\n",
      "\tLoss: 0.10488884896039963\n",
      "\tLoss: 0.12341365963220596\n",
      "\tLoss: 0.12915948033332825\n",
      "\tLoss: 0.08800213038921356\n",
      "\tLoss: 0.15815746784210205\n",
      "\tLoss: 0.11882992088794708\n",
      "\tLoss: 0.086157888174057\n",
      "\tLoss: 0.11189312487840652\n",
      "\tLoss: 0.09240406006574631\n",
      "\tLoss: 0.0846630185842514\n",
      "\tLoss: 0.06823363900184631\n",
      "\tLoss: 0.1629335582256317\n",
      "\tLoss: 0.08942306041717529\n",
      "\tLoss: 0.11788579821586609\n",
      "\tLoss: 0.1458836793899536\n",
      "\tLoss: 0.10827547311782837\n",
      "\tLoss: 0.17234447598457336\n",
      "\tLoss: 0.10859519243240356\n",
      "\tLoss: 0.09749052673578262\n",
      "\tLoss: 0.11233201622962952\n",
      "\tLoss: 0.15256020426750183\n",
      "\tLoss: 0.0979345515370369\n",
      "\tLoss: 0.09906940162181854\n",
      "\tLoss: 0.134535551071167\n",
      "\tLoss: 0.15687356889247894\n",
      "\tLoss: 0.13944301009178162\n",
      "\tLoss: 0.10116976499557495\n",
      "\tLoss: 0.13479173183441162\n",
      "\tLoss: 0.07557875663042068\n",
      "\tLoss: 0.11986876279115677\n",
      "\tLoss: 0.16449885070323944\n",
      "\tLoss: 0.11851842701435089\n",
      "\tLoss: 0.09295522421598434\n",
      "\tLoss: 0.20879629254341125\n",
      "\tLoss: 0.1105620488524437\n",
      "\tLoss: 0.13462072610855103\n",
      "\tLoss: 0.1489962637424469\n",
      "\tLoss: 0.14702129364013672\n",
      "\tLoss: 0.1667620837688446\n",
      "\tLoss: 0.10025396943092346\n",
      "\tLoss: 0.09471004456281662\n",
      "\tLoss: 0.06134437397122383\n",
      "\tLoss: 0.13185200095176697\n",
      "\tLoss: 0.1663094013929367\n",
      "\tLoss: 0.1450619399547577\n",
      "\tLoss: 0.14853987097740173\n",
      "\tLoss: 0.10564795136451721\n",
      "\tLoss: 0.10811173915863037\n",
      "\tLoss: 0.09406337141990662\n",
      "\tLoss: 0.08805808424949646\n",
      "\tLoss: 0.13207340240478516\n",
      "\tLoss: 0.1115998923778534\n",
      "\tLoss: 0.11659624427556992\n",
      "\tLoss: 0.09168197959661484\n",
      "\tLoss: 0.09880843758583069\n",
      "\tLoss: 0.2042967677116394\n",
      "\tLoss: 0.11196377873420715\n",
      "\tLoss: 0.1336216777563095\n",
      "\tLoss: 0.22152754664421082\n",
      "\tLoss: 0.14283810555934906\n",
      "\tLoss: 0.18733051419258118\n",
      "\tLoss: 0.09672228991985321\n",
      "\tLoss: 0.09529714286327362\n",
      "\tLoss: 0.07945847511291504\n",
      "\tLoss: 0.09453694522380829\n",
      "\tLoss: 0.07133056968450546\n",
      "\tLoss: 0.0775548443198204\n",
      "\tLoss: 0.08950024098157883\n",
      "\tLoss: 0.14252793788909912\n",
      "\tLoss: 0.1552412211894989\n",
      "\tLoss: 0.08004088699817657\n",
      "\tLoss: 0.09567989408969879\n",
      "\tLoss: 0.10907141119241714\n",
      "\tLoss: 0.10897991061210632\n",
      "\tLoss: 0.08481690287590027\n",
      "[time] Epoch 25: 424.04773897118866s = 7.067462316186478m\n",
      "\n",
      "Epoch 26...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.06721655279397964\n",
      "\tLoss: 0.07432287931442261\n",
      "\tLoss: 0.16112037003040314\n",
      "\tLoss: 0.11620285362005234\n",
      "\tLoss: 0.09625887125730515\n",
      "\tLoss: 0.12437369674444199\n",
      "\tLoss: 0.10929642617702484\n",
      "\tLoss: 0.09178231656551361\n",
      "\tLoss: 0.06365491449832916\n",
      "\tLoss: 0.11657834053039551\n",
      "\tLoss: 0.08147099614143372\n",
      "\tLoss: 0.0749264508485794\n",
      "\tLoss: 0.12179645895957947\n",
      "\tLoss: 0.10767306387424469\n",
      "\tLoss: 0.1276445835828781\n",
      "\tLoss: 0.16999368369579315\n",
      "\tLoss: 0.14692403376102448\n",
      "\tLoss: 0.09327530115842819\n",
      "\tLoss: 0.11800073087215424\n",
      "\tLoss: 0.14271046221256256\n",
      "\tLoss: 0.09999425709247589\n",
      "\tLoss: 0.08745265752077103\n",
      "\tLoss: 0.11161084473133087\n",
      "\tLoss: 0.14283487200737\n",
      "\tLoss: 0.09746510535478592\n",
      "\tLoss: 0.07769729197025299\n",
      "\tLoss: 0.14616142213344574\n",
      "\tLoss: 0.14295655488967896\n",
      "\tLoss: 0.13548745214939117\n",
      "\tLoss: 0.061428096145391464\n",
      "\tLoss: 0.11960410326719284\n",
      "\tLoss: 0.1064743623137474\n",
      "\tLoss: 0.11303265392780304\n",
      "\tLoss: 0.18498432636260986\n",
      "\tLoss: 0.0681167021393776\n",
      "\tLoss: 0.09512579441070557\n",
      "\tLoss: 0.13902878761291504\n",
      "\tLoss: 0.08043279498815536\n",
      "\tLoss: 0.11197071522474289\n",
      "\tLoss: 0.15096160769462585\n",
      "\tLoss: 0.11483155190944672\n",
      "\tLoss: 0.13883092999458313\n",
      "\tLoss: 0.13517442345619202\n",
      "\tLoss: 0.10723525285720825\n",
      "\tLoss: 0.10609233379364014\n",
      "\tLoss: 0.08614957332611084\n",
      "\tLoss: 0.11650612950325012\n",
      "\tLoss: 0.10507842153310776\n",
      "\tLoss: 0.12513041496276855\n",
      "\tLoss: 0.12720097601413727\n",
      "\tLoss: 0.10281401872634888\n",
      "\tLoss: 0.09665100276470184\n",
      "\tLoss: 0.1171179786324501\n",
      "\tLoss: 0.14682532846927643\n",
      "\tLoss: 0.0826469361782074\n",
      "\tLoss: 0.11254041641950607\n",
      "\tLoss: 0.08820478618144989\n",
      "\tLoss: 0.11409171670675278\n",
      "\tLoss: 0.10728710889816284\n",
      "\tLoss: 0.12498032301664352\n",
      "\tLoss: 0.09615299105644226\n",
      "\tLoss: 0.14964988827705383\n",
      "\tLoss: 0.08735460788011551\n",
      "\tLoss: 0.07603204250335693\n",
      "\tLoss: 0.14166495203971863\n",
      "\tLoss: 0.09944336116313934\n",
      "\tLoss: 0.08679785579442978\n",
      "\tLoss: 0.0966009646654129\n",
      "\tLoss: 0.14985743165016174\n",
      "\tLoss: 0.12308083474636078\n",
      "\tLoss: 0.1132107526063919\n",
      "\tLoss: 0.10175576061010361\n",
      "\tLoss: 0.1267356276512146\n",
      "\tLoss: 0.1252213716506958\n",
      "\tLoss: 0.11172619462013245\n",
      "\tLoss: 0.18532586097717285\n",
      "\tLoss: 0.10412123799324036\n",
      "\tLoss: 0.13590095937252045\n",
      "\tLoss: 0.12627804279327393\n",
      "\tLoss: 0.11652929335832596\n",
      "\tLoss: 0.09600575268268585\n",
      "\tLoss: 0.1755986511707306\n",
      "\tLoss: 0.1618758887052536\n",
      "\tLoss: 0.13118374347686768\n",
      "\tLoss: 0.1253536492586136\n",
      "\tLoss: 0.1270594447851181\n",
      "\tLoss: 0.07086247205734253\n",
      "\tLoss: 0.1406956911087036\n",
      "\tLoss: 0.11938071995973587\n",
      "\tLoss: 0.1329754889011383\n",
      "\tLoss: 0.14358806610107422\n",
      "\tLoss: 0.13959920406341553\n",
      "\tLoss: 0.15010163187980652\n",
      "\tLoss: 0.11388178169727325\n",
      "\tLoss: 0.1006632149219513\n",
      "\tLoss: 0.11132007837295532\n",
      "\tLoss: 0.17902499437332153\n",
      "\tLoss: 0.10509195923805237\n",
      "\tLoss: 0.09851992130279541\n",
      "\tLoss: 0.10235972702503204\n",
      "\tLoss: 0.08533889055252075\n",
      "\tLoss: 0.111014224588871\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.05949728190898895\n",
      "\tLoss: 0.16272293031215668\n",
      "\tLoss: 0.12631714344024658\n",
      "\tLoss: 0.15160280466079712\n",
      "\tLoss: 0.20759141445159912\n",
      "\tLoss: 0.11273354291915894\n",
      "\tLoss: 0.07184955477714539\n",
      "\tLoss: 0.15186290442943573\n",
      "\tLoss: 0.10981990396976471\n",
      "\tLoss: 0.1243799701333046\n",
      "\tLoss: 0.10081350058317184\n",
      "\tLoss: 0.10130946338176727\n",
      "\tLoss: 0.07948823273181915\n",
      "\tLoss: 0.14037732779979706\n",
      "\tLoss: 0.12156602740287781\n",
      "\tLoss: 0.07035328447818756\n",
      "\tLoss: 0.0771184116601944\n",
      "\tLoss: 0.10254429280757904\n",
      "\tLoss: 0.10569038987159729\n",
      "\tLoss: 0.11024446040391922\n",
      "\tLoss: 0.09292413294315338\n",
      "\tLoss: 0.07914206385612488\n",
      "\tLoss: 0.12242887169122696\n",
      "\tLoss: 0.10622318089008331\n",
      "\tLoss: 0.14333869516849518\n",
      "\tLoss: 0.18564152717590332\n",
      "\tLoss: 0.10551191866397858\n",
      "\tLoss: 0.06486046314239502\n",
      "\tLoss: 0.16930824518203735\n",
      "\tLoss: 0.06314952671527863\n",
      "\tLoss: 0.08805598318576813\n",
      "\tLoss: 0.12818419933319092\n",
      "\tLoss: 0.12833072245121002\n",
      "\tLoss: 0.08114692568778992\n",
      "\tLoss: 0.07742540538311005\n",
      "\tLoss: 0.09123203158378601\n",
      "\tLoss: 0.11556585878133774\n",
      "\tLoss: 0.16763335466384888\n",
      "\tLoss: 0.169100821018219\n",
      "\tLoss: 0.07138128578662872\n",
      "\tLoss: 0.047158610075712204\n",
      "\tLoss: 0.07182882726192474\n",
      "\tLoss: 0.10651159286499023\n",
      "\tLoss: 0.13753868639469147\n",
      "\tLoss: 0.08992806077003479\n",
      "\tLoss: 0.08825813233852386\n",
      "\tLoss: 0.06790346652269363\n",
      "\tLoss: 0.14082670211791992\n",
      "\tLoss: 0.1268167793750763\n",
      "\tLoss: 0.10701609402894974\n",
      "\tLoss: 0.10292603075504303\n",
      "\tLoss: 0.11207703500986099\n",
      "\tLoss: 0.13537463545799255\n",
      "\tLoss: 0.08477689325809479\n",
      "\tLoss: 0.10881738364696503\n",
      "\tLoss: 0.12474985420703888\n",
      "\tLoss: 0.08949463069438934\n",
      "\tLoss: 0.04637540876865387\n",
      "\tLoss: 0.10381419956684113\n",
      "\tLoss: 0.11701236665248871\n",
      "\tLoss: 0.11695632338523865\n",
      "\tLoss: 0.09488445520401001\n",
      "\tLoss: 0.1526191234588623\n",
      "\tLoss: 0.10668542981147766\n",
      "\tLoss: 0.08817325532436371\n",
      "\tLoss: 0.07073524594306946\n",
      "\tLoss: 0.11671922355890274\n",
      "\tLoss: 0.07471345365047455\n",
      "\tLoss: 0.09646662324666977\n",
      "\tLoss: 0.045706018805503845\n",
      "\tLoss: 0.1607629358768463\n",
      "\tLoss: 0.10627006739377975\n",
      "\tLoss: 0.07808631658554077\n",
      "\tLoss: 0.14086341857910156\n",
      "\tLoss: 0.14704886078834534\n",
      "\tLoss: 0.08687622100114822\n",
      "\tLoss: 0.0942082330584526\n",
      "\tLoss: 0.058338284492492676\n",
      "\tLoss: 0.1340014785528183\n",
      "\tLoss: 0.10684780776500702\n",
      "\tLoss: 0.13527606427669525\n",
      "\tLoss: 0.1048756018280983\n",
      "\tLoss: 0.10419179499149323\n",
      "\tLoss: 0.11859190464019775\n",
      "\tLoss: 0.11734046041965485\n",
      "\tLoss: 0.12113188207149506\n",
      "\tLoss: 0.1057916134595871\n",
      "\tLoss: 0.11692482233047485\n",
      "\tLoss: 0.11826503276824951\n",
      "\tLoss: 0.08594784140586853\n",
      "\tLoss: 0.10550239682197571\n",
      "\tLoss: 0.09947744756937027\n",
      "\tLoss: 0.1380344033241272\n",
      "\tLoss: 0.12244068086147308\n",
      "\tLoss: 0.1386503279209137\n",
      "\tLoss: 0.08594787120819092\n",
      "\tLoss: 0.08735961467027664\n",
      "\tLoss: 0.10662255436182022\n",
      "\tLoss: 0.08689863234758377\n",
      "\tLoss: 0.19201506674289703\n",
      "\tLoss: 0.1341511458158493\n",
      "\tLoss: 0.09607821702957153\n",
      "[time] Epoch 26: 422.92620591493323s = 7.048770098582221m\n",
      "\n",
      "Epoch 27...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.07130438089370728\n",
      "\tLoss: 0.10524211823940277\n",
      "\tLoss: 0.1547059863805771\n",
      "\tLoss: 0.13454142212867737\n",
      "\tLoss: 0.16215983033180237\n",
      "\tLoss: 0.07095992565155029\n",
      "\tLoss: 0.10342651605606079\n",
      "\tLoss: 0.09345096349716187\n",
      "\tLoss: 0.10922147333621979\n",
      "\tLoss: 0.10667812824249268\n",
      "\tLoss: 0.12186068296432495\n",
      "\tLoss: 0.1554669439792633\n",
      "\tLoss: 0.08640501648187637\n",
      "\tLoss: 0.1229061484336853\n",
      "\tLoss: 0.11533136665821075\n",
      "\tLoss: 0.04964764416217804\n",
      "\tLoss: 0.0986437201499939\n",
      "\tLoss: 0.11228795349597931\n",
      "\tLoss: 0.08187775313854218\n",
      "\tLoss: 0.11708985269069672\n",
      "\tLoss: 0.14435455203056335\n",
      "\tLoss: 0.09267714619636536\n",
      "\tLoss: 0.10828813165426254\n",
      "\tLoss: 0.09673747420310974\n",
      "\tLoss: 0.09528491646051407\n",
      "\tLoss: 0.13957424461841583\n",
      "\tLoss: 0.10196112096309662\n",
      "\tLoss: 0.08142571896314621\n",
      "\tLoss: 0.13693472743034363\n",
      "\tLoss: 0.12373874336481094\n",
      "\tLoss: 0.09569723904132843\n",
      "\tLoss: 0.1424785852432251\n",
      "\tLoss: 0.134233295917511\n",
      "\tLoss: 0.0976218432188034\n",
      "\tLoss: 0.09922891855239868\n",
      "\tLoss: 0.14460636675357819\n",
      "\tLoss: 0.09885723888874054\n",
      "\tLoss: 0.12495297938585281\n",
      "\tLoss: 0.05022551864385605\n",
      "\tLoss: 0.09407583624124527\n",
      "\tLoss: 0.14559029042720795\n",
      "\tLoss: 0.09684516489505768\n",
      "\tLoss: 0.10557505488395691\n",
      "\tLoss: 0.09641391038894653\n",
      "\tLoss: 0.11195695400238037\n",
      "\tLoss: 0.11732873320579529\n",
      "\tLoss: 0.09186241030693054\n",
      "\tLoss: 0.11562268435955048\n",
      "\tLoss: 0.18010160326957703\n",
      "\tLoss: 0.10589853674173355\n",
      "\tLoss: 0.09396381676197052\n",
      "\tLoss: 0.10067551583051682\n",
      "\tLoss: 0.12850213050842285\n",
      "\tLoss: 0.16372895240783691\n",
      "\tLoss: 0.0709119588136673\n",
      "\tLoss: 0.11573968827724457\n",
      "\tLoss: 0.1037462130188942\n",
      "\tLoss: 0.11319626122713089\n",
      "\tLoss: 0.11314558982849121\n",
      "\tLoss: 0.1351560801267624\n",
      "\tLoss: 0.129637211561203\n",
      "\tLoss: 0.13024412095546722\n",
      "\tLoss: 0.11201408505439758\n",
      "\tLoss: 0.05532432347536087\n",
      "\tLoss: 0.10158300399780273\n",
      "\tLoss: 0.1514585018157959\n",
      "\tLoss: 0.176981121301651\n",
      "\tLoss: 0.1006375253200531\n",
      "\tLoss: 0.08225454390048981\n",
      "\tLoss: 0.1369839310646057\n",
      "\tLoss: 0.10268934071063995\n",
      "\tLoss: 0.09594839811325073\n",
      "\tLoss: 0.1630544811487198\n",
      "\tLoss: 0.09464478492736816\n",
      "\tLoss: 0.10177555680274963\n",
      "\tLoss: 0.18027931451797485\n",
      "\tLoss: 0.09634667634963989\n",
      "\tLoss: 0.10215029120445251\n",
      "\tLoss: 0.1360655277967453\n",
      "\tLoss: 0.1182149201631546\n",
      "\tLoss: 0.15152031183242798\n",
      "\tLoss: 0.06502485275268555\n",
      "\tLoss: 0.19774478673934937\n",
      "\tLoss: 0.08268909156322479\n",
      "\tLoss: 0.12615133821964264\n",
      "\tLoss: 0.14919638633728027\n",
      "\tLoss: 0.14224937558174133\n",
      "\tLoss: 0.14967785775661469\n",
      "\tLoss: 0.13755154609680176\n",
      "\tLoss: 0.06115052476525307\n",
      "\tLoss: 0.10174023360013962\n",
      "\tLoss: 0.07944963127374649\n",
      "\tLoss: 0.08342161029577255\n",
      "\tLoss: 0.16278186440467834\n",
      "\tLoss: 0.11373147368431091\n",
      "\tLoss: 0.0900280699133873\n",
      "\tLoss: 0.09761596471071243\n",
      "\tLoss: 0.10357096046209335\n",
      "\tLoss: 0.13507580757141113\n",
      "\tLoss: 0.13301940262317657\n",
      "\tLoss: 0.12672163546085358\n",
      "\tLoss: 0.16254489123821259\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.1910751760005951\n",
      "\tLoss: 0.13484016060829163\n",
      "\tLoss: 0.14629942178726196\n",
      "\tLoss: 0.14104175567626953\n",
      "\tLoss: 0.1334317922592163\n",
      "\tLoss: 0.11422348022460938\n",
      "\tLoss: 0.1287974864244461\n",
      "\tLoss: 0.1260663866996765\n",
      "\tLoss: 0.1166001558303833\n",
      "\tLoss: 0.09913262724876404\n",
      "\tLoss: 0.1177448183298111\n",
      "\tLoss: 0.0765715166926384\n",
      "\tLoss: 0.10378725081682205\n",
      "\tLoss: 0.0657208263874054\n",
      "\tLoss: 0.12282904237508774\n",
      "\tLoss: 0.08484089374542236\n",
      "\tLoss: 0.10275636613368988\n",
      "\tLoss: 0.15721572935581207\n",
      "\tLoss: 0.12255556881427765\n",
      "\tLoss: 0.08928509801626205\n",
      "\tLoss: 0.09163732826709747\n",
      "\tLoss: 0.11763041466474533\n",
      "\tLoss: 0.12041758000850677\n",
      "\tLoss: 0.0958506166934967\n",
      "\tLoss: 0.1746918261051178\n",
      "\tLoss: 0.0874696597456932\n",
      "\tLoss: 0.12077026814222336\n",
      "\tLoss: 0.08941824734210968\n",
      "\tLoss: 0.14786294102668762\n",
      "\tLoss: 0.10445351898670197\n",
      "\tLoss: 0.06443455070257187\n",
      "\tLoss: 0.12987414002418518\n",
      "\tLoss: 0.09562899172306061\n",
      "\tLoss: 0.12290981411933899\n",
      "\tLoss: 0.12631025910377502\n",
      "\tLoss: 0.1118253618478775\n",
      "\tLoss: 0.15090025961399078\n",
      "\tLoss: 0.07090826332569122\n",
      "\tLoss: 0.08987203240394592\n",
      "\tLoss: 0.1781836897134781\n",
      "\tLoss: 0.12137135863304138\n",
      "\tLoss: 0.10235648602247238\n",
      "\tLoss: 0.07442399114370346\n",
      "\tLoss: 0.08853578567504883\n",
      "\tLoss: 0.15761618316173553\n",
      "\tLoss: 0.06800203025341034\n",
      "\tLoss: 0.10447733104228973\n",
      "\tLoss: 0.10243317484855652\n",
      "\tLoss: 0.18697714805603027\n",
      "\tLoss: 0.08655460178852081\n",
      "\tLoss: 0.14246658980846405\n",
      "\tLoss: 0.079347625374794\n",
      "\tLoss: 0.08174492418766022\n",
      "\tLoss: 0.11408329010009766\n",
      "\tLoss: 0.04990430176258087\n",
      "\tLoss: 0.0990254208445549\n",
      "\tLoss: 0.13329067826271057\n",
      "\tLoss: 0.12969852983951569\n",
      "\tLoss: 0.08838321268558502\n",
      "\tLoss: 0.04659102112054825\n",
      "\tLoss: 0.13697132468223572\n",
      "\tLoss: 0.12933236360549927\n",
      "\tLoss: 0.14976787567138672\n",
      "\tLoss: 0.11370813846588135\n",
      "\tLoss: 0.12312468141317368\n",
      "\tLoss: 0.05905897542834282\n",
      "\tLoss: 0.11797682195901871\n",
      "\tLoss: 0.09138225764036179\n",
      "\tLoss: 0.08694208413362503\n",
      "\tLoss: 0.13925226032733917\n",
      "\tLoss: 0.15939012169837952\n",
      "\tLoss: 0.1376856118440628\n",
      "\tLoss: 0.1141929179430008\n",
      "\tLoss: 0.08733873069286346\n",
      "\tLoss: 0.12486843019723892\n",
      "\tLoss: 0.1067998856306076\n",
      "\tLoss: 0.09253550320863724\n",
      "\tLoss: 0.14745229482650757\n",
      "\tLoss: 0.12260109931230545\n",
      "\tLoss: 0.1357296258211136\n",
      "\tLoss: 0.11330149322748184\n",
      "\tLoss: 0.11820629239082336\n",
      "\tLoss: 0.10505867749452591\n",
      "\tLoss: 0.09276769310235977\n",
      "\tLoss: 0.0940161943435669\n",
      "\tLoss: 0.12965470552444458\n",
      "\tLoss: 0.14267632365226746\n",
      "\tLoss: 0.08525367826223373\n",
      "\tLoss: 0.11191309988498688\n",
      "\tLoss: 0.15810135006904602\n",
      "\tLoss: 0.1077517718076706\n",
      "\tLoss: 0.11665165424346924\n",
      "\tLoss: 0.11537472903728485\n",
      "\tLoss: 0.048333585262298584\n",
      "\tLoss: 0.12670128047466278\n",
      "\tLoss: 0.13871712982654572\n",
      "\tLoss: 0.0982925221323967\n",
      "\tLoss: 0.09456206858158112\n",
      "\tLoss: 0.11055552959442139\n",
      "\tLoss: 0.12622317671775818\n",
      "\tLoss: 0.05216047167778015\n",
      "\tLoss: 0.07111094892024994\n",
      "[time] Epoch 27: 422.4290749039501s = 7.0404845817325015m\n",
      "\n",
      "Epoch 28...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.09565387666225433\n",
      "\tLoss: 0.06874185800552368\n",
      "\tLoss: 0.09533436596393585\n",
      "\tLoss: 0.09349718689918518\n",
      "\tLoss: 0.1418958604335785\n",
      "\tLoss: 0.08596478402614594\n",
      "\tLoss: 0.07130592316389084\n",
      "\tLoss: 0.17504072189331055\n",
      "\tLoss: 0.08661729097366333\n",
      "\tLoss: 0.12256106734275818\n",
      "\tLoss: 0.11706806719303131\n",
      "\tLoss: 0.12842029333114624\n",
      "\tLoss: 0.10790728032588959\n",
      "\tLoss: 0.14167089760303497\n",
      "\tLoss: 0.10600634664297104\n",
      "\tLoss: 0.18904797732830048\n",
      "\tLoss: 0.0924239531159401\n",
      "\tLoss: 0.08163920044898987\n",
      "\tLoss: 0.07362814247608185\n",
      "\tLoss: 0.1259087175130844\n",
      "\tLoss: 0.11572803556919098\n",
      "\tLoss: 0.13962244987487793\n",
      "\tLoss: 0.1350983828306198\n",
      "\tLoss: 0.0501989983022213\n",
      "\tLoss: 0.19142350554466248\n",
      "\tLoss: 0.1334485411643982\n",
      "\tLoss: 0.08137422800064087\n",
      "\tLoss: 0.05211755633354187\n",
      "\tLoss: 0.14835920929908752\n",
      "\tLoss: 0.07141517102718353\n",
      "\tLoss: 0.08916317671537399\n",
      "\tLoss: 0.18642905354499817\n",
      "\tLoss: 0.10041971504688263\n",
      "\tLoss: 0.15120816230773926\n",
      "\tLoss: 0.17355981469154358\n",
      "\tLoss: 0.15216247737407684\n",
      "\tLoss: 0.11271002888679504\n",
      "\tLoss: 0.20158015191555023\n",
      "\tLoss: 0.12514764070510864\n",
      "\tLoss: 0.08879591524600983\n",
      "\tLoss: 0.10009263455867767\n",
      "\tLoss: 0.07128467410802841\n",
      "\tLoss: 0.08391015976667404\n",
      "\tLoss: 0.09212517738342285\n",
      "\tLoss: 0.11604048311710358\n",
      "\tLoss: 0.1761578917503357\n",
      "\tLoss: 0.05520974099636078\n",
      "\tLoss: 0.13059109449386597\n",
      "\tLoss: 0.08435982465744019\n",
      "\tLoss: 0.1487758755683899\n",
      "\tLoss: 0.08891276270151138\n",
      "\tLoss: 0.14544281363487244\n",
      "\tLoss: 0.09485335648059845\n",
      "\tLoss: 0.12169746309518814\n",
      "\tLoss: 0.13111643493175507\n",
      "\tLoss: 0.1806679219007492\n",
      "\tLoss: 0.08614590018987656\n",
      "\tLoss: 0.08663803339004517\n",
      "\tLoss: 0.10115008056163788\n",
      "\tLoss: 0.11921419203281403\n",
      "\tLoss: 0.07189770042896271\n",
      "\tLoss: 0.07967916131019592\n",
      "\tLoss: 0.07990625500679016\n",
      "\tLoss: 0.09093334525823593\n",
      "\tLoss: 0.13824555277824402\n",
      "\tLoss: 0.08842970430850983\n",
      "\tLoss: 0.14497138559818268\n",
      "\tLoss: 0.13418659567832947\n",
      "\tLoss: 0.11218106746673584\n",
      "\tLoss: 0.10066463053226471\n",
      "\tLoss: 0.1418032944202423\n",
      "\tLoss: 0.13079102337360382\n",
      "\tLoss: 0.05181782320141792\n",
      "\tLoss: 0.13544322550296783\n",
      "\tLoss: 0.10755423456430435\n",
      "\tLoss: 0.0895465686917305\n",
      "\tLoss: 0.09526024758815765\n",
      "\tLoss: 0.09437838196754456\n",
      "\tLoss: 0.1155354306101799\n",
      "\tLoss: 0.08636833727359772\n",
      "\tLoss: 0.0934612974524498\n",
      "\tLoss: 0.14312060177326202\n",
      "\tLoss: 0.11627114564180374\n",
      "\tLoss: 0.14057624340057373\n",
      "\tLoss: 0.10814378410577774\n",
      "\tLoss: 0.07042616605758667\n",
      "\tLoss: 0.07912049442529678\n",
      "\tLoss: 0.19174525141716003\n",
      "\tLoss: 0.09500019252300262\n",
      "\tLoss: 0.10760162025690079\n",
      "\tLoss: 0.04319032281637192\n",
      "\tLoss: 0.09911458194255829\n",
      "\tLoss: 0.11703133583068848\n",
      "\tLoss: 0.17663896083831787\n",
      "\tLoss: 0.10526203364133835\n",
      "\tLoss: 0.14122621715068817\n",
      "\tLoss: 0.1254264861345291\n",
      "\tLoss: 0.16422989964485168\n",
      "\tLoss: 0.10372120141983032\n",
      "\tLoss: 0.15828625857830048\n",
      "\tLoss: 0.12463391572237015\n",
      "\tLoss: 0.10183335840702057\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.11927799880504608\n",
      "\tLoss: 0.10675905644893646\n",
      "\tLoss: 0.1280573606491089\n",
      "\tLoss: 0.11843608319759369\n",
      "\tLoss: 0.09927624464035034\n",
      "\tLoss: 0.10418689250946045\n",
      "\tLoss: 0.1088872104883194\n",
      "\tLoss: 0.14032691717147827\n",
      "\tLoss: 0.15938502550125122\n",
      "\tLoss: 0.09703827649354935\n",
      "\tLoss: 0.1236494779586792\n",
      "\tLoss: 0.1200234666466713\n",
      "\tLoss: 0.10179360210895538\n",
      "\tLoss: 0.08345961570739746\n",
      "\tLoss: 0.061472270637750626\n",
      "\tLoss: 0.09488601237535477\n",
      "\tLoss: 0.10552963614463806\n",
      "\tLoss: 0.0881986990571022\n",
      "\tLoss: 0.10397978127002716\n",
      "\tLoss: 0.11984965205192566\n",
      "\tLoss: 0.11053086072206497\n",
      "\tLoss: 0.0987047404050827\n",
      "\tLoss: 0.11585673689842224\n",
      "\tLoss: 0.10386992245912552\n",
      "\tLoss: 0.10879843682050705\n",
      "\tLoss: 0.08135087043046951\n",
      "\tLoss: 0.07087120413780212\n",
      "\tLoss: 0.1386006474494934\n",
      "\tLoss: 0.11795234680175781\n",
      "\tLoss: 0.11212100088596344\n",
      "\tLoss: 0.10175032168626785\n",
      "\tLoss: 0.08577963709831238\n",
      "\tLoss: 0.07512903213500977\n",
      "\tLoss: 0.11762279272079468\n",
      "\tLoss: 0.12679535150527954\n",
      "\tLoss: 0.06629319489002228\n",
      "\tLoss: 0.10625974833965302\n",
      "\tLoss: 0.08560538291931152\n",
      "\tLoss: 0.09638042002916336\n",
      "\tLoss: 0.09120127558708191\n",
      "\tLoss: 0.13615810871124268\n",
      "\tLoss: 0.09847386181354523\n",
      "\tLoss: 0.11174032092094421\n",
      "\tLoss: 0.09204364567995071\n",
      "\tLoss: 0.03625050187110901\n",
      "\tLoss: 0.10196292400360107\n",
      "\tLoss: 0.10669142007827759\n",
      "\tLoss: 0.11021245270967484\n",
      "\tLoss: 0.11658430844545364\n",
      "\tLoss: 0.11496630311012268\n",
      "\tLoss: 0.12436632812023163\n",
      "\tLoss: 0.08385616540908813\n",
      "\tLoss: 0.10808149725198746\n",
      "\tLoss: 0.1646851897239685\n",
      "\tLoss: 0.07543856650590897\n",
      "\tLoss: 0.09035108983516693\n",
      "\tLoss: 0.1835147887468338\n",
      "\tLoss: 0.1415340006351471\n",
      "\tLoss: 0.13273689150810242\n",
      "\tLoss: 0.11774370074272156\n",
      "\tLoss: 0.14332513511180878\n",
      "\tLoss: 0.13629743456840515\n",
      "\tLoss: 0.12997794151306152\n",
      "\tLoss: 0.0837290808558464\n",
      "\tLoss: 0.13952705264091492\n",
      "\tLoss: 0.13793855905532837\n",
      "\tLoss: 0.09213753044605255\n",
      "\tLoss: 0.07492931187152863\n",
      "\tLoss: 0.10541858524084091\n",
      "\tLoss: 0.0912402868270874\n",
      "\tLoss: 0.12695665657520294\n",
      "\tLoss: 0.14326944947242737\n",
      "\tLoss: 0.06429404765367508\n",
      "\tLoss: 0.15593141317367554\n",
      "\tLoss: 0.14619755744934082\n",
      "\tLoss: 0.12201975286006927\n",
      "\tLoss: 0.0679512619972229\n",
      "\tLoss: 0.12586013972759247\n",
      "\tLoss: 0.13265052437782288\n",
      "\tLoss: 0.14404216408729553\n",
      "\tLoss: 0.08188486844301224\n",
      "\tLoss: 0.12465827912092209\n",
      "\tLoss: 0.10840760171413422\n",
      "\tLoss: 0.1742539405822754\n",
      "\tLoss: 0.12325990200042725\n",
      "\tLoss: 0.10722866654396057\n",
      "\tLoss: 0.1544087827205658\n",
      "\tLoss: 0.07208551466464996\n",
      "\tLoss: 0.10680671781301498\n",
      "\tLoss: 0.12168053537607193\n",
      "\tLoss: 0.13989248871803284\n",
      "\tLoss: 0.09121011942625046\n",
      "\tLoss: 0.11482243984937668\n",
      "\tLoss: 0.18011267483234406\n",
      "\tLoss: 0.12348507344722748\n",
      "\tLoss: 0.07380077242851257\n",
      "\tLoss: 0.1317262351512909\n",
      "\tLoss: 0.06223385035991669\n",
      "\tLoss: 0.10558871179819107\n",
      "\tLoss: 0.10458362102508545\n",
      "\tLoss: 0.1539820432662964\n",
      "\tLoss: 0.113984614610672\n",
      "[time] Epoch 28: 415.98440419882536s = 6.933073403313756m\n",
      "\n",
      "Epoch 29...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.13295726478099823\n",
      "\tLoss: 0.10501216351985931\n",
      "\tLoss: 0.1216006949543953\n",
      "\tLoss: 0.1686955988407135\n",
      "\tLoss: 0.0948256403207779\n",
      "\tLoss: 0.11832296848297119\n",
      "\tLoss: 0.14281301200389862\n",
      "\tLoss: 0.08698508143424988\n",
      "\tLoss: 0.17866867780685425\n",
      "\tLoss: 0.11782558262348175\n",
      "\tLoss: 0.09808027744293213\n",
      "\tLoss: 0.08608116209506989\n",
      "\tLoss: 0.06503906100988388\n",
      "\tLoss: 0.129927396774292\n",
      "\tLoss: 0.07819844037294388\n",
      "\tLoss: 0.1721857637166977\n",
      "\tLoss: 0.15482628345489502\n",
      "\tLoss: 0.10348935425281525\n",
      "\tLoss: 0.1185062974691391\n",
      "\tLoss: 0.10291234403848648\n",
      "\tLoss: 0.07746631652116776\n",
      "\tLoss: 0.15749499201774597\n",
      "\tLoss: 0.14178721606731415\n",
      "\tLoss: 0.10764427483081818\n",
      "\tLoss: 0.10514659434556961\n",
      "\tLoss: 0.11569870263338089\n",
      "\tLoss: 0.11247760057449341\n",
      "\tLoss: 0.1145244836807251\n",
      "\tLoss: 0.09352223575115204\n",
      "\tLoss: 0.0509847030043602\n",
      "\tLoss: 0.09740544855594635\n",
      "\tLoss: 0.14747180044651031\n",
      "\tLoss: 0.06021710857748985\n",
      "\tLoss: 0.13675858080387115\n",
      "\tLoss: 0.07315686345100403\n",
      "\tLoss: 0.13129781186580658\n",
      "\tLoss: 0.08712642639875412\n",
      "\tLoss: 0.0638476312160492\n",
      "\tLoss: 0.16640055179595947\n",
      "\tLoss: 0.11222195625305176\n",
      "\tLoss: 0.10341859608888626\n",
      "\tLoss: 0.09173798561096191\n",
      "\tLoss: 0.10778677463531494\n",
      "\tLoss: 0.12005754560232162\n",
      "\tLoss: 0.2000369429588318\n",
      "\tLoss: 0.0879909098148346\n",
      "\tLoss: 0.12247921526432037\n",
      "\tLoss: 0.06898677349090576\n",
      "\tLoss: 0.0861557275056839\n",
      "\tLoss: 0.17015032470226288\n",
      "\tLoss: 0.17170177400112152\n",
      "\tLoss: 0.12973237037658691\n",
      "\tLoss: 0.18070483207702637\n",
      "\tLoss: 0.11009374260902405\n",
      "\tLoss: 0.1007472425699234\n",
      "\tLoss: 0.11567103862762451\n",
      "\tLoss: 0.13098052144050598\n",
      "\tLoss: 0.08019018173217773\n",
      "\tLoss: 0.11344518512487411\n",
      "\tLoss: 0.14930430054664612\n",
      "\tLoss: 0.09358300268650055\n",
      "\tLoss: 0.1289171278476715\n",
      "\tLoss: 0.11372023820877075\n",
      "\tLoss: 0.0745248794555664\n",
      "\tLoss: 0.10473565757274628\n",
      "\tLoss: 0.050278596580028534\n",
      "\tLoss: 0.09141495823860168\n",
      "\tLoss: 0.08279500156641006\n",
      "\tLoss: 0.14810210466384888\n",
      "\tLoss: 0.07380744814872742\n",
      "\tLoss: 0.153669536113739\n",
      "\tLoss: 0.10660921037197113\n",
      "\tLoss: 0.10026964545249939\n",
      "\tLoss: 0.15471673011779785\n",
      "\tLoss: 0.12055118381977081\n",
      "\tLoss: 0.12511014938354492\n",
      "\tLoss: 0.09315650910139084\n",
      "\tLoss: 0.09482286870479584\n",
      "\tLoss: 0.1245088055729866\n",
      "\tLoss: 0.0735996663570404\n",
      "\tLoss: 0.15558290481567383\n",
      "\tLoss: 0.05580475926399231\n",
      "\tLoss: 0.0994495153427124\n",
      "\tLoss: 0.09726158529520035\n",
      "\tLoss: 0.11059633642435074\n",
      "\tLoss: 0.11620782315731049\n",
      "\tLoss: 0.11466878652572632\n",
      "\tLoss: 0.07775461673736572\n",
      "\tLoss: 0.12186801433563232\n",
      "\tLoss: 0.10635396838188171\n",
      "\tLoss: 0.06752438843250275\n",
      "\tLoss: 0.17312371730804443\n",
      "\tLoss: 0.083253413438797\n",
      "\tLoss: 0.11199963092803955\n",
      "\tLoss: 0.07149527221918106\n",
      "\tLoss: 0.10377787053585052\n",
      "\tLoss: 0.09429500997066498\n",
      "\tLoss: 0.12487133592367172\n",
      "\tLoss: 0.09165504574775696\n",
      "\tLoss: 0.11855817586183548\n",
      "\tLoss: 0.15569154918193817\n",
      "\tLoss: 0.11323552578687668\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.1069503128528595\n",
      "\tLoss: 0.16645163297653198\n",
      "\tLoss: 0.13979369401931763\n",
      "\tLoss: 0.10956485569477081\n",
      "\tLoss: 0.1372174322605133\n",
      "\tLoss: 0.0863770842552185\n",
      "\tLoss: 0.14121244847774506\n",
      "\tLoss: 0.1159253865480423\n",
      "\tLoss: 0.134535551071167\n",
      "\tLoss: 0.055305786430835724\n",
      "\tLoss: 0.12521342933177948\n",
      "\tLoss: 0.1036703959107399\n",
      "\tLoss: 0.1568324714899063\n",
      "\tLoss: 0.15760695934295654\n",
      "\tLoss: 0.06660620123147964\n",
      "\tLoss: 0.07163535803556442\n",
      "\tLoss: 0.11802524328231812\n",
      "\tLoss: 0.09893684834241867\n",
      "\tLoss: 0.11619893461465836\n",
      "\tLoss: 0.12126558274030685\n",
      "\tLoss: 0.058753643184900284\n",
      "\tLoss: 0.08405083417892456\n",
      "\tLoss: 0.12217505276203156\n",
      "\tLoss: 0.11874035000801086\n",
      "\tLoss: 0.0878913551568985\n",
      "\tLoss: 0.08505435287952423\n",
      "\tLoss: 0.10997895896434784\n",
      "\tLoss: 0.14674994349479675\n",
      "\tLoss: 0.09991642832756042\n",
      "\tLoss: 0.08767326176166534\n",
      "\tLoss: 0.1233556866645813\n",
      "\tLoss: 0.1745283603668213\n",
      "\tLoss: 0.09749208390712738\n",
      "\tLoss: 0.07611178606748581\n",
      "\tLoss: 0.1233895868062973\n",
      "\tLoss: 0.09635347127914429\n",
      "\tLoss: 0.09972966462373734\n",
      "\tLoss: 0.07664840668439865\n",
      "\tLoss: 0.07751898467540741\n",
      "\tLoss: 0.11200655996799469\n",
      "\tLoss: 0.0803513377904892\n",
      "\tLoss: 0.09162227064371109\n",
      "\tLoss: 0.11242294311523438\n",
      "\tLoss: 0.15023064613342285\n",
      "\tLoss: 0.09241746366024017\n",
      "\tLoss: 0.12586544454097748\n",
      "\tLoss: 0.15540528297424316\n",
      "\tLoss: 0.1313621997833252\n",
      "\tLoss: 0.15548016130924225\n",
      "\tLoss: 0.08636520057916641\n",
      "\tLoss: 0.10260871797800064\n",
      "\tLoss: 0.12024207413196564\n",
      "\tLoss: 0.12376101315021515\n",
      "\tLoss: 0.09868869185447693\n",
      "\tLoss: 0.09558630734682083\n",
      "\tLoss: 0.16629603505134583\n",
      "\tLoss: 0.12873588502407074\n",
      "\tLoss: 0.1447269767522812\n",
      "\tLoss: 0.13376718759536743\n",
      "\tLoss: 0.08602739125490189\n",
      "\tLoss: 0.11963369697332382\n",
      "\tLoss: 0.13531367480754852\n",
      "\tLoss: 0.1564735472202301\n",
      "\tLoss: 0.1190318837761879\n",
      "\tLoss: 0.13005363941192627\n",
      "\tLoss: 0.07189509272575378\n",
      "\tLoss: 0.0934140533208847\n",
      "\tLoss: 0.09412964433431625\n",
      "\tLoss: 0.14935585856437683\n",
      "\tLoss: 0.22021496295928955\n",
      "\tLoss: 0.12739944458007812\n",
      "\tLoss: 0.2025645673274994\n",
      "\tLoss: 0.07326555252075195\n",
      "\tLoss: 0.089985191822052\n",
      "\tLoss: 0.14837417006492615\n",
      "\tLoss: 0.08601722866296768\n",
      "\tLoss: 0.09275451302528381\n",
      "\tLoss: 0.10789017379283905\n",
      "\tLoss: 0.09847518801689148\n",
      "\tLoss: 0.12184701859951019\n",
      "\tLoss: 0.1028452068567276\n",
      "\tLoss: 0.12410282343626022\n",
      "\tLoss: 0.14136043190956116\n",
      "\tLoss: 0.15097631514072418\n",
      "\tLoss: 0.12001997232437134\n",
      "\tLoss: 0.08129887282848358\n",
      "\tLoss: 0.13574138283729553\n",
      "\tLoss: 0.15712805092334747\n",
      "\tLoss: 0.1293313205242157\n",
      "\tLoss: 0.10487961769104004\n",
      "\tLoss: 0.14671632647514343\n",
      "\tLoss: 0.07068098336458206\n",
      "\tLoss: 0.08260312676429749\n",
      "\tLoss: 0.146703839302063\n",
      "\tLoss: 0.05688973143696785\n",
      "\tLoss: 0.11818379163742065\n",
      "\tLoss: 0.1181180328130722\n",
      "\tLoss: 0.17661312222480774\n",
      "\tLoss: 0.1633937507867813\n",
      "\tLoss: 0.1030435860157013\n",
      "\tLoss: 0.0889793410897255\n",
      "\tLoss: 0.1420949399471283\n",
      "[time] Epoch 29: 419.1579837212339s = 6.985966395353898m\n",
      "\n",
      "Epoch 30...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.1213807687163353\n",
      "\tLoss: 0.14006304740905762\n",
      "\tLoss: 0.12721437215805054\n",
      "\tLoss: 0.08694726973772049\n",
      "\tLoss: 0.0969141498208046\n",
      "\tLoss: 0.12260320037603378\n",
      "\tLoss: 0.086030974984169\n",
      "\tLoss: 0.1237621083855629\n",
      "\tLoss: 0.12731650471687317\n",
      "\tLoss: 0.16001677513122559\n",
      "\tLoss: 0.08079986274242401\n",
      "\tLoss: 0.159200057387352\n",
      "\tLoss: 0.06845881044864655\n",
      "\tLoss: 0.12410075217485428\n",
      "\tLoss: 0.09025131911039352\n",
      "\tLoss: 0.10811099410057068\n",
      "\tLoss: 0.1545267105102539\n",
      "\tLoss: 0.11462246626615524\n",
      "\tLoss: 0.10496324300765991\n",
      "\tLoss: 0.11480773985385895\n",
      "\tLoss: 0.12043867260217667\n",
      "\tLoss: 0.09661448001861572\n",
      "\tLoss: 0.09034743160009384\n",
      "\tLoss: 0.12956500053405762\n",
      "\tLoss: 0.10701409727334976\n",
      "\tLoss: 0.18674436211585999\n",
      "\tLoss: 0.12906141579151154\n",
      "\tLoss: 0.09411609172821045\n",
      "\tLoss: 0.09447149932384491\n",
      "\tLoss: 0.1353231519460678\n",
      "\tLoss: 0.09715595096349716\n",
      "\tLoss: 0.07807333767414093\n",
      "\tLoss: 0.15305718779563904\n",
      "\tLoss: 0.08291900157928467\n",
      "\tLoss: 0.12748493254184723\n",
      "\tLoss: 0.1227540671825409\n",
      "\tLoss: 0.13181078433990479\n",
      "\tLoss: 0.09453163295984268\n",
      "\tLoss: 0.09086161851882935\n",
      "\tLoss: 0.09768568724393845\n",
      "\tLoss: 0.1300671547651291\n",
      "\tLoss: 0.13853393495082855\n",
      "\tLoss: 0.11233456432819366\n",
      "\tLoss: 0.18224090337753296\n",
      "\tLoss: 0.1001920998096466\n",
      "\tLoss: 0.20088128745555878\n",
      "\tLoss: 0.09023801237344742\n",
      "\tLoss: 0.09368382394313812\n",
      "\tLoss: 0.09280522167682648\n",
      "\tLoss: 0.11445970833301544\n",
      "\tLoss: 0.10140031576156616\n",
      "\tLoss: 0.0933247059583664\n",
      "\tLoss: 0.12672638893127441\n",
      "\tLoss: 0.17814147472381592\n",
      "\tLoss: 0.08221513032913208\n",
      "\tLoss: 0.12022051960229874\n",
      "\tLoss: 0.16401100158691406\n",
      "\tLoss: 0.15817488729953766\n",
      "\tLoss: 0.10573116689920425\n",
      "\tLoss: 0.10174998641014099\n",
      "\tLoss: 0.04802801460027695\n",
      "\tLoss: 0.12347055971622467\n",
      "\tLoss: 0.07800517976284027\n",
      "\tLoss: 0.0928255096077919\n",
      "\tLoss: 0.10483012348413467\n",
      "\tLoss: 0.1417018175125122\n",
      "\tLoss: 0.07960108667612076\n",
      "\tLoss: 0.07948873192071915\n",
      "\tLoss: 0.12601961195468903\n",
      "\tLoss: 0.08468485623598099\n",
      "\tLoss: 0.16421496868133545\n",
      "\tLoss: 0.09225880354642868\n",
      "\tLoss: 0.0993284061551094\n",
      "\tLoss: 0.09829124808311462\n",
      "\tLoss: 0.1145661249756813\n",
      "\tLoss: 0.16026030480861664\n",
      "\tLoss: 0.11391505599021912\n",
      "\tLoss: 0.09430960565805435\n",
      "\tLoss: 0.13739913702011108\n",
      "\tLoss: 0.09609515219926834\n",
      "\tLoss: 0.10591620951890945\n",
      "\tLoss: 0.10247839242219925\n",
      "\tLoss: 0.14852619171142578\n",
      "\tLoss: 0.18775638937950134\n",
      "\tLoss: 0.20286248624324799\n",
      "\tLoss: 0.0731639564037323\n",
      "\tLoss: 0.14113833010196686\n",
      "\tLoss: 0.1270986795425415\n",
      "\tLoss: 0.09079398959875107\n",
      "\tLoss: 0.07391056418418884\n",
      "\tLoss: 0.11795037984848022\n",
      "\tLoss: 0.11043651401996613\n",
      "\tLoss: 0.08836540579795837\n",
      "\tLoss: 0.1238873302936554\n",
      "\tLoss: 0.08502325415611267\n",
      "\tLoss: 0.11887019872665405\n",
      "\tLoss: 0.06681846082210541\n",
      "\tLoss: 0.07878975570201874\n",
      "\tLoss: 0.1070048063993454\n",
      "\tLoss: 0.06505142152309418\n",
      "\tLoss: 0.09502631425857544\n",
      "\tLoss: 0.10280093550682068\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.06048106402158737\n",
      "\tLoss: 0.15398812294006348\n",
      "\tLoss: 0.132435142993927\n",
      "\tLoss: 0.12866012752056122\n",
      "\tLoss: 0.14027243852615356\n",
      "\tLoss: 0.137317955493927\n",
      "\tLoss: 0.1277591437101364\n",
      "\tLoss: 0.12952059507369995\n",
      "\tLoss: 0.12484247982501984\n",
      "\tLoss: 0.06740870326757431\n",
      "\tLoss: 0.12125800549983978\n",
      "\tLoss: 0.0949702337384224\n",
      "\tLoss: 0.08979049324989319\n",
      "\tLoss: 0.09637836366891861\n",
      "\tLoss: 0.14400646090507507\n",
      "\tLoss: 0.10133190453052521\n",
      "\tLoss: 0.07609735429286957\n",
      "\tLoss: 0.0784701406955719\n",
      "\tLoss: 0.08555302768945694\n",
      "\tLoss: 0.05425262451171875\n",
      "\tLoss: 0.07269708812236786\n",
      "\tLoss: 0.22350044548511505\n",
      "\tLoss: 0.1409549117088318\n",
      "\tLoss: 0.07466699182987213\n",
      "\tLoss: 0.09417443722486496\n",
      "\tLoss: 0.08415420353412628\n",
      "\tLoss: 0.1379028856754303\n",
      "\tLoss: 0.11614886671304703\n",
      "\tLoss: 0.10203450173139572\n",
      "\tLoss: 0.1162688359618187\n",
      "\tLoss: 0.0511343888938427\n",
      "\tLoss: 0.12758994102478027\n",
      "\tLoss: 0.06409059464931488\n",
      "\tLoss: 0.07703155279159546\n",
      "\tLoss: 0.08831310272216797\n",
      "\tLoss: 0.14078611135482788\n",
      "\tLoss: 0.15929706394672394\n",
      "\tLoss: 0.13894391059875488\n",
      "\tLoss: 0.10646726936101913\n",
      "\tLoss: 0.13093943893909454\n",
      "\tLoss: 0.11715139448642731\n",
      "\tLoss: 0.08067083358764648\n",
      "\tLoss: 0.056283727288246155\n",
      "\tLoss: 0.09939076751470566\n",
      "\tLoss: 0.08561081439256668\n",
      "\tLoss: 0.1854247897863388\n",
      "\tLoss: 0.1327633261680603\n",
      "\tLoss: 0.07198530435562134\n",
      "\tLoss: 0.13239161670207977\n",
      "\tLoss: 0.10630552470684052\n",
      "\tLoss: 0.14820659160614014\n",
      "\tLoss: 0.1284254789352417\n",
      "\tLoss: 0.12640896439552307\n",
      "\tLoss: 0.1328018307685852\n",
      "\tLoss: 0.1466687023639679\n",
      "\tLoss: 0.0776023268699646\n",
      "\tLoss: 0.10100431740283966\n",
      "\tLoss: 0.11363674700260162\n",
      "\tLoss: 0.14498218894004822\n",
      "\tLoss: 0.10474582761526108\n",
      "\tLoss: 0.10219705104827881\n",
      "\tLoss: 0.16773340106010437\n",
      "\tLoss: 0.08751971274614334\n",
      "\tLoss: 0.07614675164222717\n",
      "\tLoss: 0.10178746283054352\n",
      "\tLoss: 0.13172422349452972\n",
      "\tLoss: 0.10231900215148926\n",
      "\tLoss: 0.07753325998783112\n",
      "\tLoss: 0.11555251479148865\n",
      "\tLoss: 0.08550328016281128\n",
      "\tLoss: 0.07649046182632446\n",
      "\tLoss: 0.08827163279056549\n",
      "\tLoss: 0.1334974467754364\n",
      "\tLoss: 0.1771722137928009\n",
      "\tLoss: 0.06755602359771729\n",
      "\tLoss: 0.07716076076030731\n",
      "\tLoss: 0.07860757410526276\n",
      "\tLoss: 0.08327701687812805\n",
      "\tLoss: 0.08060242235660553\n",
      "\tLoss: 0.10862430930137634\n",
      "\tLoss: 0.09200319647789001\n",
      "\tLoss: 0.13206078112125397\n",
      "\tLoss: 0.14255967736244202\n",
      "\tLoss: 0.10650333017110825\n",
      "\tLoss: 0.08635178953409195\n",
      "\tLoss: 0.13052301108837128\n",
      "\tLoss: 0.14094680547714233\n",
      "\tLoss: 0.09452977776527405\n",
      "\tLoss: 0.12024088948965073\n",
      "\tLoss: 0.12260544300079346\n",
      "\tLoss: 0.1457590013742447\n",
      "\tLoss: 0.09942246973514557\n",
      "\tLoss: 0.138417050242424\n",
      "\tLoss: 0.10947185754776001\n",
      "\tLoss: 0.1130838692188263\n",
      "\tLoss: 0.07409551739692688\n",
      "\tLoss: 0.09004449099302292\n",
      "\tLoss: 0.12395791709423065\n",
      "\tLoss: 0.10620114207267761\n",
      "\tLoss: 0.07757767289876938\n",
      "\tLoss: 0.08549094200134277\n",
      "\tLoss: 0.13273227214813232\n",
      "[time] Epoch 30: 423.2615334978327s = 7.054358891630545m\n",
      "\n",
      "Epoch 31...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.13463594019412994\n",
      "\tLoss: 0.08878687024116516\n",
      "\tLoss: 0.09769795089960098\n",
      "\tLoss: 0.12317484617233276\n",
      "\tLoss: 0.10649823397397995\n",
      "\tLoss: 0.11380566656589508\n",
      "\tLoss: 0.11732551455497742\n",
      "\tLoss: 0.12327766418457031\n",
      "\tLoss: 0.11156144738197327\n",
      "\tLoss: 0.069251149892807\n",
      "\tLoss: 0.12293652445077896\n",
      "\tLoss: 0.07175605744123459\n",
      "\tLoss: 0.13168424367904663\n",
      "\tLoss: 0.10713349282741547\n",
      "\tLoss: 0.12813183665275574\n",
      "\tLoss: 0.14541806280612946\n",
      "\tLoss: 0.11756440997123718\n",
      "\tLoss: 0.12232343852519989\n",
      "\tLoss: 0.11605249345302582\n",
      "\tLoss: 0.07184895128011703\n",
      "\tLoss: 0.16021126508712769\n",
      "\tLoss: 0.06283140927553177\n",
      "\tLoss: 0.11977896094322205\n",
      "\tLoss: 0.1237235814332962\n",
      "\tLoss: 0.08892080187797546\n",
      "\tLoss: 0.13325954973697662\n",
      "\tLoss: 0.13934005796909332\n",
      "\tLoss: 0.14507946372032166\n",
      "\tLoss: 0.0896591916680336\n",
      "\tLoss: 0.07312279939651489\n",
      "\tLoss: 0.11021138727664948\n",
      "\tLoss: 0.13695909082889557\n",
      "\tLoss: 0.10732578486204147\n",
      "\tLoss: 0.12378478050231934\n",
      "\tLoss: 0.13273507356643677\n",
      "\tLoss: 0.09440366923809052\n",
      "\tLoss: 0.13063432276248932\n",
      "\tLoss: 0.12905003130435944\n",
      "\tLoss: 0.13974130153656006\n",
      "\tLoss: 0.1319628357887268\n",
      "\tLoss: 0.10061398893594742\n",
      "\tLoss: 0.14037752151489258\n",
      "\tLoss: 0.0998915582895279\n",
      "\tLoss: 0.1388365477323532\n",
      "\tLoss: 0.17380909621715546\n",
      "\tLoss: 0.11026208102703094\n",
      "\tLoss: 0.10450530797243118\n",
      "\tLoss: 0.14018242061138153\n",
      "\tLoss: 0.11776874959468842\n",
      "\tLoss: 0.14590942859649658\n",
      "\tLoss: 0.10563630610704422\n",
      "\tLoss: 0.08336242288351059\n",
      "\tLoss: 0.16688920557498932\n",
      "\tLoss: 0.12614759802818298\n",
      "\tLoss: 0.13232684135437012\n",
      "\tLoss: 0.09674287587404251\n",
      "\tLoss: 0.09244105219841003\n",
      "\tLoss: 0.12634794414043427\n",
      "\tLoss: 0.11168241500854492\n",
      "\tLoss: 0.18149138987064362\n",
      "\tLoss: 0.10446039587259293\n",
      "\tLoss: 0.11541298031806946\n",
      "\tLoss: 0.09784601628780365\n",
      "\tLoss: 0.1316668689250946\n",
      "\tLoss: 0.1629830002784729\n",
      "\tLoss: 0.11876474320888519\n",
      "\tLoss: 0.06650200486183167\n",
      "\tLoss: 0.11419299989938736\n",
      "\tLoss: 0.10881276428699493\n",
      "\tLoss: 0.18063965439796448\n",
      "\tLoss: 0.08045868575572968\n",
      "\tLoss: 0.14532403647899628\n",
      "\tLoss: 0.1018168032169342\n",
      "\tLoss: 0.10554134100675583\n",
      "\tLoss: 0.08183116465806961\n",
      "\tLoss: 0.1011042520403862\n",
      "\tLoss: 0.12868347764015198\n",
      "\tLoss: 0.10105092823505402\n",
      "\tLoss: 0.05762646347284317\n",
      "\tLoss: 0.11272087693214417\n",
      "\tLoss: 0.11549478769302368\n",
      "\tLoss: 0.11410652101039886\n",
      "\tLoss: 0.09689712524414062\n",
      "\tLoss: 0.08373337239027023\n",
      "\tLoss: 0.10448704659938812\n",
      "\tLoss: 0.08312714099884033\n",
      "\tLoss: 0.08306793868541718\n",
      "\tLoss: 0.1126192957162857\n",
      "\tLoss: 0.11983256787061691\n",
      "\tLoss: 0.12878626585006714\n",
      "\tLoss: 0.11176102608442307\n",
      "\tLoss: 0.09967149049043655\n",
      "\tLoss: 0.12667547166347504\n",
      "\tLoss: 0.0789061039686203\n",
      "\tLoss: 0.09601263701915741\n",
      "\tLoss: 0.16734743118286133\n",
      "\tLoss: 0.0998745858669281\n",
      "\tLoss: 0.1261364072561264\n",
      "\tLoss: 0.062436532229185104\n",
      "\tLoss: 0.08166345953941345\n",
      "\tLoss: 0.05433276295661926\n",
      "\tLoss: 0.16924965381622314\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.10154189169406891\n",
      "\tLoss: 0.12159471213817596\n",
      "\tLoss: 0.12125778198242188\n",
      "\tLoss: 0.13050003349781036\n",
      "\tLoss: 0.09619684517383575\n",
      "\tLoss: 0.1120893657207489\n",
      "\tLoss: 0.1206878125667572\n",
      "\tLoss: 0.11201314628124237\n",
      "\tLoss: 0.07016699016094208\n",
      "\tLoss: 0.07316415011882782\n",
      "\tLoss: 0.08290160447359085\n",
      "\tLoss: 0.09755191206932068\n",
      "\tLoss: 0.08155810832977295\n",
      "\tLoss: 0.10001512616872787\n",
      "\tLoss: 0.08691707253456116\n",
      "\tLoss: 0.08544747531414032\n",
      "\tLoss: 0.08076614141464233\n",
      "\tLoss: 0.134235680103302\n",
      "\tLoss: 0.16789233684539795\n",
      "\tLoss: 0.14702028036117554\n",
      "\tLoss: 0.11129425466060638\n",
      "\tLoss: 0.11100010573863983\n",
      "\tLoss: 0.1580158770084381\n",
      "\tLoss: 0.09042033553123474\n",
      "\tLoss: 0.09863193333148956\n",
      "\tLoss: 0.14025533199310303\n",
      "\tLoss: 0.061594825237989426\n",
      "\tLoss: 0.09795533120632172\n",
      "\tLoss: 0.10318014025688171\n",
      "\tLoss: 0.03401786461472511\n",
      "\tLoss: 0.1228833794593811\n",
      "\tLoss: 0.11307235807180405\n",
      "\tLoss: 0.14853885769844055\n",
      "\tLoss: 0.17002199590206146\n",
      "\tLoss: 0.12472231686115265\n",
      "\tLoss: 0.0866784155368805\n",
      "\tLoss: 0.17355749011039734\n",
      "\tLoss: 0.06427794694900513\n",
      "\tLoss: 0.08194029331207275\n",
      "\tLoss: 0.15029388666152954\n",
      "\tLoss: 0.078189417719841\n",
      "\tLoss: 0.09537125378847122\n",
      "\tLoss: 0.1058867871761322\n",
      "\tLoss: 0.11851625144481659\n",
      "\tLoss: 0.08963318914175034\n",
      "\tLoss: 0.15345853567123413\n",
      "\tLoss: 0.05532899126410484\n",
      "\tLoss: 0.13520367443561554\n",
      "\tLoss: 0.1107465922832489\n",
      "\tLoss: 0.09710358083248138\n",
      "\tLoss: 0.07883525639772415\n",
      "\tLoss: 0.09357468038797379\n",
      "\tLoss: 0.09066835790872574\n",
      "\tLoss: 0.08165577054023743\n",
      "\tLoss: 0.09967190027236938\n",
      "\tLoss: 0.111636221408844\n",
      "\tLoss: 0.09201958775520325\n",
      "\tLoss: 0.11838579177856445\n",
      "\tLoss: 0.1297832429409027\n",
      "\tLoss: 0.14024969935417175\n",
      "\tLoss: 0.11439963430166245\n",
      "\tLoss: 0.13331732153892517\n",
      "\tLoss: 0.13132289052009583\n",
      "\tLoss: 0.14796751737594604\n",
      "\tLoss: 0.09575521945953369\n",
      "\tLoss: 0.11204268038272858\n",
      "\tLoss: 0.13575434684753418\n",
      "\tLoss: 0.10815063118934631\n",
      "\tLoss: 0.12357763200998306\n",
      "\tLoss: 0.1361040472984314\n",
      "\tLoss: 0.10206080973148346\n",
      "\tLoss: 0.10407254844903946\n",
      "\tLoss: 0.09259693324565887\n",
      "\tLoss: 0.09324774146080017\n",
      "\tLoss: 0.06784944236278534\n",
      "\tLoss: 0.08208151161670685\n",
      "\tLoss: 0.12574519217014313\n",
      "\tLoss: 0.12583523988723755\n",
      "\tLoss: 0.14026698470115662\n",
      "\tLoss: 0.09781988710165024\n",
      "\tLoss: 0.09429316222667694\n",
      "\tLoss: 0.08263234794139862\n",
      "\tLoss: 0.1019863411784172\n",
      "\tLoss: 0.10592375695705414\n",
      "\tLoss: 0.11980625987052917\n",
      "\tLoss: 0.07925345003604889\n",
      "\tLoss: 0.07820789515972137\n",
      "\tLoss: 0.07445799559354782\n",
      "\tLoss: 0.11826986074447632\n",
      "\tLoss: 0.10009189695119858\n",
      "\tLoss: 0.1334569752216339\n",
      "\tLoss: 0.08389811217784882\n",
      "\tLoss: 0.09363339841365814\n",
      "\tLoss: 0.14533524215221405\n",
      "\tLoss: 0.1280839741230011\n",
      "\tLoss: 0.11848492920398712\n",
      "\tLoss: 0.11287561058998108\n",
      "\tLoss: 0.1165839284658432\n",
      "\tLoss: 0.1573568880558014\n",
      "\tLoss: 0.07766905426979065\n",
      "\tLoss: 0.09781155735254288\n",
      "\tLoss: 0.12836989760398865\n",
      "[time] Epoch 31: 416.03000762313604s = 6.933833460385601m\n",
      "\n",
      "Epoch 32...\n",
      "\tTrabajando con el 0-ésimo DataLoader:\n",
      "\tLoss: 0.12147108465433121\n",
      "\tLoss: 0.12009087949991226\n",
      "\tLoss: 0.11015871167182922\n",
      "\tLoss: 0.06753593683242798\n",
      "\tLoss: 0.13291911780834198\n",
      "\tLoss: 0.13695929944515228\n",
      "\tLoss: 0.09522810578346252\n",
      "\tLoss: 0.11281038820743561\n",
      "\tLoss: 0.13512665033340454\n",
      "\tLoss: 0.05523776262998581\n",
      "\tLoss: 0.10567399859428406\n",
      "\tLoss: 0.08205003291368484\n",
      "\tLoss: 0.08025719225406647\n",
      "\tLoss: 0.10601377487182617\n",
      "\tLoss: 0.10390488058328629\n",
      "\tLoss: 0.12101709097623825\n",
      "\tLoss: 0.09935339540243149\n",
      "\tLoss: 0.12748710811138153\n",
      "\tLoss: 0.12470223009586334\n",
      "\tLoss: 0.11698846518993378\n",
      "\tLoss: 0.08356303721666336\n",
      "\tLoss: 0.1234799325466156\n",
      "\tLoss: 0.11607685685157776\n",
      "\tLoss: 0.15417885780334473\n",
      "\tLoss: 0.14091095328330994\n",
      "\tLoss: 0.11154291778802872\n",
      "\tLoss: 0.21242916584014893\n",
      "\tLoss: 0.16551917791366577\n",
      "\tLoss: 0.08777214586734772\n",
      "\tLoss: 0.09409135580062866\n",
      "\tLoss: 0.0890040248632431\n",
      "\tLoss: 0.14165225625038147\n",
      "\tLoss: 0.13957569003105164\n",
      "\tLoss: 0.07108975946903229\n",
      "\tLoss: 0.11833740770816803\n",
      "\tLoss: 0.05911010131239891\n",
      "\tLoss: 0.13434246182441711\n",
      "\tLoss: 0.16183900833129883\n",
      "\tLoss: 0.13129949569702148\n",
      "\tLoss: 0.06788195669651031\n",
      "\tLoss: 0.1380743682384491\n",
      "\tLoss: 0.1648440659046173\n",
      "\tLoss: 0.14204122126102448\n",
      "\tLoss: 0.1223306804895401\n",
      "\tLoss: 0.07944484055042267\n",
      "\tLoss: 0.14030678570270538\n",
      "\tLoss: 0.1218678206205368\n",
      "\tLoss: 0.16059556603431702\n",
      "\tLoss: 0.1240692138671875\n",
      "\tLoss: 0.10822486132383347\n",
      "\tLoss: 0.06224483624100685\n",
      "\tLoss: 0.06139292195439339\n",
      "\tLoss: 0.15991607308387756\n",
      "\tLoss: 0.07863318920135498\n",
      "\tLoss: 0.13768650591373444\n",
      "\tLoss: 0.1538604497909546\n",
      "\tLoss: 0.10584599524736404\n",
      "\tLoss: 0.08621110767126083\n",
      "\tLoss: 0.10846401005983353\n",
      "\tLoss: 0.2002403736114502\n",
      "\tLoss: 0.08732804656028748\n",
      "\tLoss: 0.10286323726177216\n",
      "\tLoss: 0.13325509428977966\n",
      "\tLoss: 0.11776381731033325\n",
      "\tLoss: 0.1251257210969925\n",
      "\tLoss: 0.11896354705095291\n",
      "\tLoss: 0.07222959399223328\n",
      "\tLoss: 0.12868040800094604\n",
      "\tLoss: 0.12428420037031174\n",
      "\tLoss: 0.16414663195610046\n",
      "\tLoss: 0.07593494653701782\n",
      "\tLoss: 0.13515864312648773\n",
      "\tLoss: 0.12787018716335297\n",
      "\tLoss: 0.12289252877235413\n",
      "\tLoss: 0.10824299603700638\n",
      "\tLoss: 0.08937910944223404\n",
      "\tLoss: 0.1856461763381958\n",
      "\tLoss: 0.06102694571018219\n",
      "\tLoss: 0.06404484063386917\n",
      "\tLoss: 0.11352062225341797\n",
      "\tLoss: 0.08963188529014587\n",
      "\tLoss: 0.07340992987155914\n",
      "\tLoss: 0.056433096528053284\n",
      "\tLoss: 0.1325835883617401\n",
      "\tLoss: 0.060433197766542435\n",
      "\tLoss: 0.12568128108978271\n",
      "\tLoss: 0.1314586102962494\n",
      "\tLoss: 0.07259564846754074\n",
      "\tLoss: 0.14043165743350983\n",
      "\tLoss: 0.15131992101669312\n",
      "\tLoss: 0.14269451797008514\n",
      "\tLoss: 0.10465676337480545\n",
      "\tLoss: 0.08953790366649628\n",
      "\tLoss: 0.0724901333451271\n",
      "\tLoss: 0.13261079788208008\n",
      "\tLoss: 0.12568005919456482\n",
      "\tLoss: 0.07454725354909897\n",
      "\tLoss: 0.09512810409069061\n",
      "\tLoss: 0.1506049633026123\n",
      "\tLoss: 0.17151138186454773\n",
      "\tLoss: 0.17211171984672546\n",
      "\tLoss: 0.09687137603759766\n",
      "\tTrabajando con el 1-ésimo DataLoader:\n",
      "\tLoss: 0.14816901087760925\n",
      "\tLoss: 0.13940483331680298\n",
      "\tLoss: 0.1135784238576889\n",
      "\tLoss: 0.09368909895420074\n",
      "\tLoss: 0.06330965459346771\n",
      "\tLoss: 0.1323273926973343\n",
      "\tLoss: 0.14379501342773438\n",
      "\tLoss: 0.09928090870380402\n",
      "\tLoss: 0.1099783182144165\n",
      "\tLoss: 0.10692641139030457\n",
      "\tLoss: 0.09470944106578827\n",
      "\tLoss: 0.11552469432353973\n",
      "\tLoss: 0.12912265956401825\n",
      "\tLoss: 0.12480120360851288\n",
      "\tLoss: 0.09526348114013672\n",
      "\tLoss: 0.09907356649637222\n",
      "\tLoss: 0.0708799660205841\n",
      "\tLoss: 0.16004598140716553\n",
      "\tLoss: 0.11244766414165497\n",
      "\tLoss: 0.1387123167514801\n",
      "\tLoss: 0.14096957445144653\n",
      "\tLoss: 0.09811762720346451\n",
      "\tLoss: 0.16298598051071167\n",
      "\tLoss: 0.15498925745487213\n",
      "\tLoss: 0.07592959702014923\n",
      "\tLoss: 0.11039552092552185\n",
      "\tLoss: 0.10909256339073181\n",
      "\tLoss: 0.09024135023355484\n",
      "\tLoss: 0.17992296814918518\n",
      "\tLoss: 0.06561081111431122\n",
      "\tLoss: 0.11594994366168976\n",
      "\tLoss: 0.11650336533784866\n",
      "\tLoss: 0.1029675230383873\n"
     ]
    }
   ],
   "source": [
    "# The loss function is the quantity that will be\n",
    "# minimized during training.\n",
    "# TO-DO: Escoger 'loss' adecuado.\n",
    "#loss_func = losses.ContrastiveLoss().to(device)\n",
    "loss_func = nn.CosineEmbeddingLoss().to(device)\n",
    "\n",
    "# The optimizer determines how the network will be\n",
    "# updated based on the loss function.\n",
    "# TO-DO: Escoger 'optimizer' adecuado.\n",
    "#optimizer = torch.optim.SGD(red.parameters(), lr=lr, momentum=momentum) # Usado por MNIST Colab.\n",
    "optimizer = torch.optim.Adam(red.parameters(), lr = lr) # Usado por AlexNet (TMLoss y MSELoss).\n",
    "\n",
    "# Para obtener estadísticas del entrenamiento.\n",
    "loss_history = []\n",
    "epoch_timing = []\n",
    "\n",
    "def train(epoch):\n",
    "    \"\"\"\n",
    "    Entrenamiento de la RN.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicio, en segundos, del epoch.\n",
    "    epoch_start = timer()\n",
    "    \n",
    "    # Imprimimos el número de epoch.\n",
    "    print(f\"Epoch {epoch}...\")\n",
    "    \n",
    "    # Recorremos cada DataLoader.\n",
    "    for dl_idx, dl in enumerate(birds_dl_train):\n",
    "        \n",
    "        # DEBUG.\n",
    "        print(f\"\\tTrabajando con el {dl_idx}-ésimo DataLoader:\")\n",
    "        \n",
    "        for batch_idx, data in enumerate(dl):\n",
    "\n",
    "            # DEBUG.\n",
    "            relative_batch_str = f\"{batch_idx+1}/{len(dl)}\"\n",
    "            #print(f\"\\tProcesando lote {relative_batch_str}...\")\n",
    "        \n",
    "            # Completando lotes que no tienen tamaño batch_size.\n",
    "            incremento = 0\n",
    "            tam_original = len(data[2])\n",
    "            while len(data[2]) < batch_size:\n",
    "                x,y,l = birds_ds.__getitem__()\n",
    "                x = torch.tensor(x)[None, :]\n",
    "                y = torch.tensor(y)[None, :]\n",
    "                l = torch.unsqueeze(torch.tensor(l), 0)\n",
    "                data[0] = torch.cat((data[0], x), 0)\n",
    "                data[1] = torch.cat((data[1], y), 0)\n",
    "                data[2] = torch.cat((data[2], l), 0)\n",
    "                incremento += 1\n",
    "            if incremento != 0:\n",
    "                print(f\"\\tLote {relative_batch_str} de tamaño {tam_original} incrementado en {incremento}.\")\n",
    "            assert len(data[0]) == len(data[1])\n",
    "            assert len(data[0]) == len(data[2])\n",
    "\n",
    "            # 'data' es una lista que representa un lote:\n",
    "            # data[0] contiene los primeros cachos de audio.\n",
    "            # data[1] contiene los segundos cachos de audio.\n",
    "            # data[2] contiene las etiquetas.\n",
    "            for i,d in enumerate(data):\n",
    "                data[i] = d.to(device)\n",
    "\n",
    "            # Convertimos las etiquetas a tipo flotante.\n",
    "            # Necesario para la función de pérdida BCE.\n",
    "            #data[2] = data[2].to(torch.float32)\n",
    "            # TO-DO: ¿Ya no se usará BCE?\n",
    "\n",
    "            # Vaciamos los gradientes para este lote.\n",
    "            optimizer.zero_grad()\n",
    "            # In PyTorch, for every mini-batch during the training phase,\n",
    "            # we typically want to explicitly set the gradients to zero\n",
    "            # before starting to do backpropragation (i.e., updating the\n",
    "            # Weights and biases) because PyTorch accumulates the gradients\n",
    "            # on subsequent backward passes.\n",
    "\n",
    "            # Metemos los datos a la red neuronal.\n",
    "            output_x, output_y = red(data[0], data[1])\n",
    "\n",
    "            # TO-DO: Describir.\n",
    "\n",
    "            start = timer()\n",
    "            loss = loss_func(output_x, output_y, data[2])\n",
    "            end = timer()\n",
    "            #print(f\"\\t[time] Loss function: {end-start}s\") # DEBUG\n",
    "\n",
    "            start = timer()\n",
    "            loss.backward() #dloss/dx for every variable\n",
    "            end = timer()\n",
    "            #print(f\"\\t[time] Loss backward: {end-start}s\") # DEBUG\n",
    "\n",
    "            # TensorBoard.\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            start = timer()\n",
    "            optimizer.step() #to do a one-step update on our parameter.\n",
    "            end = timer()\n",
    "            #print(f\"\\t[time] Optimizer step: {end-start}s\") # DEBUG\n",
    "\n",
    "            # Guardamos estadísticas del entrenamiento.\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "\n",
    "            # DEBUG:\n",
    "            print(f\"\\tLoss: {loss}\")\n",
    "            #print(f\"\\tEtiquetas: {data[2]}\")\n",
    "            #print(f\"\\tOutput: {output}\")\n",
    "            #print()\n",
    "\n",
    "            #break # DEBUG: Permite el entrenamiento de sólo un lote.\n",
    "    \n",
    "    # Fin, en segundos, del epoch.\n",
    "    epoch_end = timer()\n",
    "    \n",
    "    # Tiempo total, en segundos, del epoch.\n",
    "    epoch_time_seconds = epoch_end-epoch_start\n",
    "    epoch_timing.append(epoch_time_seconds)\n",
    "    \n",
    "    # DEBUG:\n",
    "    print(f\"[time] Epoch {epoch}: {epoch_time_seconds}s = {epoch_time_seconds/(60)}m\")\n",
    "    print()\n",
    "    \n",
    "# red.train() le indica al modelo que está siendo entrenado.\n",
    "# Esto ayuda con capas como Dropout y BatchNorm, que están\n",
    "# diseñadas para comporsarse distinto durante entrenamiento\n",
    "# y evaluación.\n",
    "red.train()\n",
    "    \n",
    "# Ejecutamos el entrenamiento definido, \"epochs\" veces.\n",
    "train_start = timer()\n",
    "for epoch in range(1, epochs+1): # Rango [a, b)\n",
    "    train(epoch)\n",
    "    #break # DEBUG: Permite la ejecución de sólo un epoch.\n",
    "train_end = timer()\n",
    "\n",
    "# Call flush() method to make sure that all pending events have been written to disk.\n",
    "# If you do not need the summary writer anymore, call close() method.\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas del entrenamiendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20, 10))\n",
    "plt.plot(range(0,len(loss_history)), loss_history, 'r')\n",
    "plt.title('loss_history')\n",
    "plt.xlabel('Lote')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time] Total training time: 25553.962s = 7.098h\n",
      "[time] Time for each epoch:\n",
      "\t1     483.766s\n",
      "\t2     465.571s\n",
      "\t3     460.902s\n",
      "\t4     458.150s\n",
      "\t5     457.734s\n",
      "\t6     448.027s\n",
      "\t7     444.644s\n",
      "\t8     444.826s\n",
      "\t9     447.817s\n",
      "\t10    446.618s\n",
      "\t11    443.406s\n",
      "\t12    434.817s\n",
      "\t13    437.940s\n",
      "\t14    431.820s\n",
      "\t15    435.544s\n",
      "\t16    427.586s\n",
      "\t17    432.632s\n",
      "\t18    427.361s\n",
      "\t19    426.750s\n",
      "\t20    427.039s\n",
      "\t21    420.703s\n",
      "\t22    427.466s\n",
      "\t23    422.952s\n",
      "\t24    422.712s\n",
      "\t25    424.048s\n",
      "\t26    422.926s\n",
      "\t27    422.429s\n",
      "\t28    415.984s\n",
      "\t29    419.158s\n",
      "\t30    423.262s\n",
      "\t31    416.030s\n",
      "\t32    419.769s\n",
      "\t33    420.621s\n",
      "\t34    416.342s\n",
      "\t35    415.580s\n",
      "\t36    415.727s\n",
      "\t37    414.112s\n",
      "\t38    417.956s\n",
      "\t39    409.589s\n",
      "\t40    415.481s\n",
      "\t41    414.825s\n",
      "\t42    410.128s\n",
      "\t43    409.754s\n",
      "\t44    416.563s\n",
      "\t45    418.170s\n",
      "\t46    419.837s\n",
      "\t47    414.465s\n",
      "\t48    415.286s\n",
      "\t49    416.893s\n",
      "\t50    416.765s\n",
      "\t51    415.972s\n",
      "\t52    418.183s\n",
      "\t53    413.261s\n",
      "\t54    413.578s\n",
      "\t55    410.729s\n",
      "\t56    410.862s\n",
      "\t57    414.303s\n",
      "\t58    409.367s\n",
      "\t59    414.139s\n",
      "\t60    415.079s\n"
     ]
    }
   ],
   "source": [
    "train_time_seconds = train_end-train_start\n",
    "print(\"[time] Total training time: {:.3f}s = {:.3f}h\".format(train_time_seconds, train_time_seconds/(60*60)))\n",
    "print(f\"[time] Time for each epoch:\")\n",
    "for i,x in enumerate(epoch_timing):\n",
    "    print (\"\\t{:<5} {:.3f}s\".format(i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testeo de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1...\n",
      "\tProcesando lote 1/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 28/32\n",
      "\tProcesando lote 2/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 3/103...\n",
      "\t\tCon umbral -0.5: 10/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 4/103...\n",
      "\t\tCon umbral -0.5: 9/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 5/103...\n",
      "\t\tCon umbral -0.5: 10/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 6/103...\n",
      "\t\tCon umbral -0.5: 26/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 27/32\n",
      "\tProcesando lote 7/103...\n",
      "\t\tCon umbral -0.5: 22/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 8/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 9/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 10/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 11/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 12/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 13/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 14/103...\n",
      "\t\tCon umbral -0.5: 24/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 15/103...\n",
      "\t\tCon umbral -0.5: 13/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 16/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 22/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 17/103...\n",
      "\t\tCon umbral -0.5: 13/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 18/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 19/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 27/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 20/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 27/32\n",
      "\tProcesando lote 21/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 22/103...\n",
      "\t\tCon umbral -0.5: 12/32\n",
      "\t\tCon umbral    0: 21/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 23/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 21/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 24/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 25/103...\n",
      "\t\tCon umbral -0.5: 21/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 26/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 27/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 28/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 22/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 29/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 30/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 31/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 32/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 33/103...\n",
      "\t\tCon umbral -0.5: 13/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 34/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 35/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 36/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 37/103...\n",
      "\t\tCon umbral -0.5: 11/32\n",
      "\t\tCon umbral    0: 20/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 38/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 39/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 22/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 40/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 41/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 42/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 43/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 44/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 22/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 45/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 46/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 29/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 47/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 48/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 49/103...\n",
      "\t\tCon umbral -0.5: 12/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 50/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 28/32\n",
      "\tProcesando lote 51/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 52/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 53/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 54/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 55/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 56/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 57/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 58/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 20/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 59/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 60/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 61/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 62/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 63/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 30/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 27/32\n",
      "\tProcesando lote 64/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 65/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 66/103...\n",
      "\t\tCon umbral -0.5: 21/32\n",
      "\t\tCon umbral    0: 29/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 67/103...\n",
      "\t\tCon umbral -0.5: 13/32\n",
      "\t\tCon umbral    0: 18/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 68/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 69/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 70/103...\n",
      "\t\tCon umbral -0.5: 11/32\n",
      "\t\tCon umbral    0: 20/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 71/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 72/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 73/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 74/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 75/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 76/103...\n",
      "\t\tCon umbral -0.5: 22/32\n",
      "\t\tCon umbral    0: 29/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 77/103...\n",
      "\t\tCon umbral -0.5: 21/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 78/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 79/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 80/103...\n",
      "\t\tCon umbral -0.5: 11/32\n",
      "\t\tCon umbral    0: 20/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 81/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 82/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 83/103...\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 84/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 85/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 86/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 87/103...\n",
      "\t\tCon umbral -0.5: 13/32\n",
      "\t\tCon umbral    0: 21/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 88/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 89/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 90/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 28/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 91/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 29/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 92/103...\n",
      "\t\tCon umbral -0.5: 12/32\n",
      "\t\tCon umbral    0: 22/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 93/103...\n",
      "\t\tCon umbral -0.5: 16/32\n",
      "\t\tCon umbral    0: 21/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 94/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 95/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 96/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 27/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 32/32\n",
      "\tProcesando lote 97/103...\n",
      "\t\tCon umbral -0.5: 15/32\n",
      "\t\tCon umbral    0: 24/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 98/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 29/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 30/32\n",
      "\tProcesando lote 99/103...\n",
      "\t\tCon umbral -0.5: 20/32\n",
      "\t\tCon umbral    0: 28/32\n",
      "\t\tCon umbral  0.5: 29/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 100/103...\n",
      "\t\tCon umbral -0.5: 19/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 31/32\n",
      "\t\tCon umbral 0.75: 31/32\n",
      "\tProcesando lote 101/103...\n",
      "\t\tCon umbral -0.5: 18/32\n",
      "\t\tCon umbral    0: 23/32\n",
      "\t\tCon umbral  0.5: 30/32\n",
      "\t\tCon umbral 0.75: 28/32\n",
      "\tProcesando lote 102/103...\n",
      "\t\tCon umbral -0.5: 14/32\n",
      "\t\tCon umbral    0: 25/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 29/32\n",
      "\tProcesando lote 103/103...\n",
      "\tLote de tamaño 13 incrementado en 19.\n",
      "\t\tCon umbral -0.5: 17/32\n",
      "\t\tCon umbral    0: 26/32\n",
      "\t\tCon umbral  0.5: 32/32\n",
      "\t\tCon umbral 0.75: 32/32\n"
     ]
    }
   ],
   "source": [
    "# Cargado de la RN.\n",
    "fecha = \"2023-01-28-17-37\"\n",
    "#red = RN().to(device)\n",
    "#red.load_state_dict(torch.load(DIR_models+fecha+\".pt\"))\n",
    "red.eval()\n",
    "    \n",
    "def test(epoch):\n",
    "    \"\"\"\n",
    "    Testeo de la RN.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicio, en segundos, del epoch.\n",
    "    epoch_start = timer()\n",
    "    \n",
    "    # Imprimimos el número de epoch.\n",
    "    print(f\"Epoch {epoch}...\")\n",
    "    \n",
    "    for batch_idx, data in enumerate(birds_dl_test):\n",
    "\n",
    "        # DEBUG.\n",
    "        print(f\"\\tProcesando lote {batch_idx+1}/{len(birds_dl_test)}...\")\n",
    "        \n",
    "        # Completando lotes que no tienen tamaño batch_size.\n",
    "        incremento = 0\n",
    "        tam_original = len(data[2])\n",
    "        while len(data[2]) < batch_size:\n",
    "            x,y,l = birds_ds.__getitem__()\n",
    "            x = torch.tensor(x)[None, :]\n",
    "            y = torch.tensor(y)[None, :]\n",
    "            l = torch.unsqueeze(torch.tensor(l), 0)\n",
    "            data[0] = torch.cat((data[0], x), 0)\n",
    "            data[1] = torch.cat((data[1], y), 0)\n",
    "            data[2] = torch.cat((data[2], l), 0)\n",
    "            incremento += 1\n",
    "        if incremento != 0:\n",
    "            print(f\"\\tLote de tamaño {tam_original} incrementado en {incremento}.\")\n",
    "        assert len(data[0]) == len(data[1])\n",
    "        assert len(data[0]) == len(data[2])\n",
    "        \n",
    "        # 'data' es una lista que representa un lote:\n",
    "        # data[0] contiene los primeros cachos de audio.\n",
    "        # data[1] contiene los segundos cachos de audio.\n",
    "        # data[2] contiene las etiquetas.\n",
    "        for i,d in enumerate(data):\n",
    "            data[i] = d.to(device)\n",
    "        \n",
    "        # Metemos los datos a la red neuronal.\n",
    "        output_x, output_y = red(data[0], data[1])\n",
    "        \n",
    "        # Realizamos la diferencia con Similitud Coseno.\n",
    "        cos = nn.CosineSimilarity()\n",
    "        diff = cos(output_x, output_y)\n",
    "        \n",
    "        #print(f\"Etiquetas: {data[2]}\")\n",
    "        #print(f\"CosineSimilarity: {diff}\")\n",
    "\n",
    "        # Estadísticas del testeo del lote.\n",
    "        correctNeg50 = 0\n",
    "        correctZero = 0\n",
    "        correctPos50 = 0\n",
    "        correctUmbral = 0\n",
    "        umbral = 0.75 # Umbral entre -1 y 1.\n",
    "        \n",
    "        for i,l in enumerate(data[2]):\n",
    "            #total += 1\n",
    "            \n",
    "            if (diff[i] >= -0.5 and l == 1) or (diff[i] < -0.5 and l == -1):\n",
    "                correctNeg50 += 1\n",
    "            \n",
    "            if (diff[i] >= 0 and l == 1) or (diff[i] < 0 and l == -1):\n",
    "                correctZero += 1\n",
    "            \n",
    "            if (diff[i] >= 0.5 and l == 1) or (diff[i] < 0.5 and l == -1):\n",
    "                correctPos50 += 1\n",
    "            \n",
    "            if (diff[i] >= umbral and l == 1) or (diff[i] < umbral and l == -1):\n",
    "                correctUmbral += 1\n",
    "        \n",
    "        print(f\"\\t\\tCon umbral -0.5: {correctNeg50}/{len(data[2])}\")\n",
    "        print(f\"\\t\\tCon umbral    0: {correctZero}/{len(data[2])}\")\n",
    "        print(f\"\\t\\tCon umbral  0.5: {correctPos50}/{len(data[2])}\")\n",
    "        print(f\"\\t\\tCon umbral {umbral}: {correctUmbral}/{len(data[2])}\")\n",
    "        \n",
    "        # DEBUG: Permite el testeo de un sólo lote\n",
    "        #break\n",
    "        \n",
    "        # TO-DO: Terminar definición del testeo.\n",
    "\n",
    "# Ejecutamos el testeo definido, \"epochs\" veces.\n",
    "test_start = timer()\n",
    "for epoch in range(1, epochs+1): # Rango [a, b)\n",
    "    test(epoch)\n",
    "    break # DEBUG: Permite la ejecución de sólo un epoch.\n",
    "test_end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas del testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time] Total testing time: 204.017s = 3.400m = 0.057h\n"
     ]
    }
   ],
   "source": [
    "test_time_seconds = test_end-test_start\n",
    "print(\"[time] Total testing time: {:.3f}s = {:.3f}m = {:.3f}h\".format(test_time_seconds, test_time_seconds/60, test_time_seconds/(60*60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardado de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(red.state_dict(), DIR_models+dt_string+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG\n",
    "\n",
    "Esta celda y las siguientes son para testear. Han de ser eliminadas cuando se limpie el código de este *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado de objetos.\n",
    "with open(DIR_objects+dt_string+\"_loss-history\", \"wb\") as file:\n",
    "    pickle.dump(loss_history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargado de objetos.\n",
    "fecha = \"\"\n",
    "\n",
    "#with open(DIR_objects+fecha+\"_loss-history\", \"rb\") as file:\n",
    "    #pickle.dump(loss_history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = birds_df.iloc[5][file_col_name]\n",
    "#librosa_process(birds_path+f, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.uniform(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9\n",
      "1 2 3 4 5 6 7 8 9\n"
     ]
    }
   ],
   "source": [
    "print(*range(0,10))\n",
    "print(*range(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05-02-2023-08-23-34\n"
     ]
    }
   ],
   "source": [
    "dt_string = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "print(dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargado de modelos.\n",
    "fecha = \"\"\n",
    "\n",
    "# Definimos un modelo que alojará a la Red Neuronal.\n",
    "#modelo = RN().to(device)\n",
    "\n",
    "# Actualizamos el modelo.\n",
    "#modelo.load_state_dict(torch.load(DIR_models+fecha+\".pt\"))\n",
    "\n",
    "# modelo.eval() le indica al modelo que ha de evaluar.\n",
    "#modelo.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3277\n",
      "3277\n",
      "total_audios/batch_size = 102.40625\n"
     ]
    }
   ],
   "source": [
    "print(len(birds_df))\n",
    "print(len(birds_ds))\n",
    "print(f\"total_audios/batch_size = {len(birds_df)/batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha = \"2023-02-01-05-24\"\n",
    "#red = RN().to(device)\n",
    "#red.load_state_dict(torch.load(DIR_models+fecha+\".pt\"))\n",
    "#red.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n",
      "diff: tensor([0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893,\n",
      "        0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893,\n",
      "        0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893, 0.7893,\n",
      "        0.7893, 0.7893, 0.7893, 0.7893, 0.7893], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x,y,l = birds_ds.__getitem__()\n",
    "\n",
    "x = np.expand_dims(x, 0)\n",
    "x = np.repeat(x, batch_size, axis=0)\n",
    "x = torch.tensor(x).to(device)\n",
    "\n",
    "y = np.expand_dims(y, 0)\n",
    "y = np.repeat(y, batch_size, axis=0)\n",
    "y = torch.tensor(y).to(device)\n",
    "\n",
    "#l = np.repeat(l, batch_size)\n",
    "\n",
    "output_x, output_y = red(x, y)\n",
    "cos = nn.CosineSimilarity()\n",
    "diff = cos(output_x, output_y)\n",
    "\n",
    "print(f\"label: {l}\")\n",
    "print(f\"diff: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En México:\n",
      "2023-02-05 02:23:34.766261-06:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "print(\"En México:\")\n",
    "print(datetime.now(timezone(timedelta(hours=-6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
