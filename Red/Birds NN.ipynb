{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Este *notebook* incluye:\n",
    "- Pequeños ejemplos de uso de *pandas*.\n",
    "- Un *DataSet* (de *PyTorch*) que almacena información de los archivos de audio con los cantos de las aves. Este *DataSet*, al solicitársele el i-ésimo *item*, devuelve un cacho del i-ésimo audio, un cacho de un j-ésimo audio, y un 0 o 1 si `i != j` o `i == j` respectivamente.\n",
    "- Un *DataLoader* (de *PyTorch*) que envuelve al *DataSet* previamente descrito.\n",
    "- Una Red Neuronal (de *PyTorch*) que toma los espectrogramas de dos audios de longitud 1s cada uno, y devuelve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones\n",
    "\n",
    "Importación de las bibliotecas a utilizar, y una pequeña descripción de cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pandas is an open source data analysis and manipulation tool.\n",
    "import pandas as pd\n",
    "\n",
    "# NumPy is for scientific computing with Python\n",
    "import numpy as np\n",
    "\n",
    "# TensorFlow is a free and open-source software library\n",
    "# for machine learning and artificial intelligence.\n",
    "import tensorflow as tf\n",
    "\n",
    "# PyTorch is an open source machine learning framework.\n",
    "import torch\n",
    "\n",
    "# PyTorch provides the torch.nn module to help us\n",
    "# in creating and training of the neural network.\n",
    "import torch.nn as nn\n",
    "\n",
    "# PyTorch has two primitives to work with data:\n",
    "# torch.utils.data.Dataset stores the samples and their corresponding labels.\n",
    "# torch.utils.data.DataLoader wraps an iterable around the Dataset.\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# \"The easiest way to use deep metric learning in your application\".\n",
    "# Written in PyTorch.\n",
    "# https://github.com/KevinMusgrave/pytorch-metric-learning\n",
    "from pytorch_metric_learning import losses\n",
    "\n",
    "# librosa is for music and audio analysis; it provides\n",
    "# the building blocks necessary to create music\n",
    "# information retrieval systems.\n",
    "import librosa\n",
    "\n",
    "# Displays a spectrogram/chromagram/cqt/etc.\n",
    "from librosa.display import specshow\n",
    "\n",
    "# matplotlib.pyplot is a collection of functions that make\n",
    "# matplotlib work like MATLAB. Each pyplot function makes\n",
    "# some change to a figure: e.g., creates a figure, creates\n",
    "# a plotting area in a figure, plots some lines in a plotting\n",
    "# area, decorates the plot with labels, etc.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorBoard is a visualization toolkit for machine learning\n",
    "# experimentation. TensorBoard allows tracking and visualizing\n",
    "# metrics such as loss and accuracy, visualizing the model graph,\n",
    "# viewing histograms, displaying images and much more.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Para utilizar t-SNE.\n",
    "from tensorboard.plugins import projector\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Para tomar el tiempo que toman ciertos procesos de la siguiente manera:\n",
    "# start = timer()\n",
    "# (algún proceso)\n",
    "# end = timer()\n",
    "# El tiempo en segundos es end-start.\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# CUDA-accelerated PyTorch implementation of the\n",
    "# T-Stochastic Neighbor Embedding algorithm.\n",
    "#from tsne_torch import TorchTSNE as TSNE\n",
    "\n",
    "# Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Manejo de guardado y cargado de objetos mediante archivos.\n",
    "import pickle\n",
    "\n",
    "# Manejo de pseudo-aleatoriedad.\n",
    "import random\n",
    "\n",
    "# Manejo de funciones matemáticas.\n",
    "import math\n",
    "\n",
    "# Manejo de fecha y tiempo.\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables generales\n",
    "\n",
    "Variables generales/globales que se utilizarán a lo largo del *notebook*. Conviene tener este apartado para consultarlas y modificarlas fácilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uso del GPU, si está disponible.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilizando {device} para el procesamiento de datos.\")\n",
    "\n",
    "# Cadena con la ubicación del archivo CSV que contiene\n",
    "# el DataFrame con datos de los audios de aves.\n",
    "birds_csv = \"/media/birds/BirdsDataFrame.csv\"\n",
    "\n",
    "# Cadena con la ubicación de los archivos WAV y XML\n",
    "# correspondientes a los audios de aves a procesar.\n",
    "birds_path = \"/media/birds/data/\"\n",
    "\n",
    "# Otras ubicaciones útiles.\n",
    "DIR_runs = \"./runs/\"\n",
    "DIR_objects = DIR_runs+\"python_objects/\"\n",
    "DIR_tensorboard = DIR_runs+\"tensorboard/\"\n",
    "DIR_notebooks = DIR_runs+\"notebooks/\"\n",
    "DIR_models = DIR_runs+\"models/\"\n",
    "\n",
    "# Nombre de la columna, dentro del DataFrame,\n",
    "# que contiene el nombre de los archivos de audio.\n",
    "file_col_name = \"FileName\"\n",
    "\n",
    "# DataFrame (de 'pandas') del archivo CSV dado.\n",
    "birds_df = pd.read_csv(birds_csv)\n",
    "\n",
    "# Los audios de aves se cortarán en cachos cuya longitud\n",
    "# varíe entre len_min segundos y len_max segundos.\n",
    "len_min = 1\n",
    "len_max = 1\n",
    "\n",
    "# Ancho y alto de cada espectrograma.\n",
    "# TO-DO: ¿Es posible calcular esto mediante una fórmula? Resulta del size()/shape de aplicar \"stft\" al audio \"y\".\n",
    "ancho,alto = 1025,87\n",
    "\n",
    "# Número de canales que tendrá cada audio.\n",
    "# Hasta ahora, si un audio tiene 1 canal, y aquí se\n",
    "# especifican 2, se copia el primer canal en un\n",
    "# segundo canal. Si un audio tiene más de 2 canales,\n",
    "# la operación no está definida.\n",
    "audio_channels = 2\n",
    "\n",
    "# Frecuencia de muestreo a la cual TODOS los audios se\n",
    "# muestrearán. Esto es necesario para que los vectores\n",
    "# que representan a los audios tengan los mismos tamaños.\n",
    "sr = 44100\n",
    "\n",
    "# Probabilidad de que dos audios de aves (o\n",
    "# cachos de audios) compartan cierta propiedad.\n",
    "p_prop = 0.5\n",
    "\n",
    "# Variables asociadas a la Red Neuronal.\n",
    "batch_size = 64 # Número de muestras que se tomarán por lote/epoch.\n",
    "epochs = 60 # Veces que se recorrerá un DataSet entero.\n",
    "lr = 6e-03 # Learning Rate.\n",
    "#momentum = 0.5 # The SGD momentum (default: 0.5) is the moving average of our gradients (helps to keep direction).\n",
    "\n",
    "# Para TensorBoard, creamos un SummaryWriter.\n",
    "# Éste escribiría al directorio ./runs/ por defecto.\n",
    "dt_string = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "writer = SummaryWriter(log_dir=DIR_tensorboard+dt_string+\"_adbekunkus\")\n",
    "\n",
    "# Función a utilizar para procesar los audios de aves.\n",
    "def librosa_process(path, cut, cut_len=None):\n",
    "    \"\"\"\n",
    "    Función que carga un audio con Librosa y devuelve el vector\n",
    "    unidimensional que representa al audio, y su frecuencia de muestreo.\n",
    "    :param str path: Ruta donde se ubica el audio.\n",
    "    :param bool cut: ¿Se cortará (y devolverá) sólo un cacho aleatorio del audio?\n",
    "    :param float cut_len: Longitud del cacho de audio (si cut==True).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Longitud del audio completo en segundos.\n",
    "    audio_len = librosa.get_duration(filename=path)\n",
    "    \n",
    "    # Si se desea el audio completo, 'librosa' lo\n",
    "    # cargará desde el inicio hasta el final.\n",
    "    start = 0\n",
    "    duracion = audio_len\n",
    "        \n",
    "    # Si se desea sólo un cacho del audio...\n",
    "    if cut:\n",
    "        \n",
    "        # Determinamos la longitud del cacho\n",
    "        # aleatorio de audio en segundos.\n",
    "        duracion = cut_len if cut_len != None else random.uniform(len_min, len_max) # Rango [a,b].\n",
    "        \n",
    "        # Aseguramos que el audio completo es más\n",
    "        # grande que el tamaño del cacho que queremos.\n",
    "        assert audio_len > duracion\n",
    "        \n",
    "        # Definimos en dónde empezará\n",
    "        # (aleatoriamente) el cacho de audio.\n",
    "        start = random.uniform(0, audio_len-duracion) # Rango [a,b].\n",
    "    \n",
    "    # Obtenemos el audio-vector y su (nueva) frecuencia de muestreo.\n",
    "    y, sampling_rate = librosa.load(path, sr=sr, offset=start, duration=duracion, mono=False)\n",
    "    \n",
    "    # Algunos audios fueron grabados en dos canales (stereo), y otros en\n",
    "    # uno (mono). Convertimos los que fueron grabados en un canal en\n",
    "    # audios de dos canales (al duplicar el único canal que tienen).\n",
    "    if y.ndim == 1:\n",
    "        y = np.repeat(y[np.newaxis, :], 2, axis=0)\n",
    "    \n",
    "    # Función no definida para audios que tienen más de dos canales.\n",
    "    # Igual se lanza un error si los vectores no tienen la longitud adecuada (sr).\n",
    "    assert(y.shape == (2, sr))\n",
    "    \n",
    "    # Short-time Fourier transform (STFT).\n",
    "    # The STFT represents a signal in the time-frequency domain by computing\n",
    "    # discrete Fourier transforms (DFT) over short overlapping windows.\n",
    "    stft = librosa.stft(y)\n",
    "    \n",
    "    # This function (stft) returns a complex-valued matrix D such that\n",
    "    # np.abs(D[..., f, t]) is the magnitude of frequency bin f at frame t.\n",
    "    magnitude = np.abs(stft)\n",
    "    \n",
    "    # Converts an amplitude spectrogram to dB-scaled spectrogram.\n",
    "    spectogram = librosa.amplitude_to_db(magnitude)\n",
    "    \n",
    "    # Devolvemos el espectrograma y su frecuencia de muestreo.\n",
    "    return spectogram, sampling_rate\n",
    "\n",
    "# Comprobaciones sobre las variables aquí definidas.\n",
    "assert len_min <= len_max # Lógicamente, min<=max.\n",
    "assert p_prop >= 0 and p_prop <= 1 # Las probabilidades se encuentran en este rango."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _pandas_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el archivo `birds_csv` cuenta con *N* columnas `columna0,columna1,...,columnaN-1`, imprimimos a continuación el nombre de cada columna, enumerándolas desde cero.\n",
    "\n",
    "**NOTA**: La primera columna no tiene nombre, por lo que *pandas*, al convertir el archivo CSV en un *DataFrame* mediante la función `read_csv()`, le asigna el nombre `Unnamed: 0`. Esta columna sirve para indexar a las entradas dentro del archivo CSV (no confundir con la columna 'index' cuyo propósito es indexar a los archivos de audio de otra manera)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Por cada columna del DataFrame, imprimimos dicha columna.\n",
    "#for i,col in enumerate(birds_df.columns):\n",
    "#    print(f\"{i}:{col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplificamos con la primera entrada del archivo al imprimir qué valor tiene asociado a cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"iloc\" permite indexar por posiciones mediante el uso de enteros.\n",
    "# Por cada columna y valor en la primera línea, imprimimos ambos.\n",
    "#for col, val in birds_df.iloc[0].iteritems():\n",
    "#    print(f\"{col}:{val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algunas columnas tal que todas las entradas del archivo comparten un mismo valor dentro de esa columna. A continuación imprimimos los nombres de las columnas que cumplen ésto, así como el valor que todas las entradas comparten en dicha columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Por cada columna del DataFrame...\n",
    "#for col in birds_df.columns:\n",
    "    \n",
    "    # Si todas las entradas tienen el mismo valor en dicha\n",
    "    # columna, imprimimos la columna y el valor correspondiente.\n",
    "    #if (birds_df[col] == birds_df[col][0]).all():\n",
    "        #print(f\"{col}:{birds_df[col][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del *DataSet*\n",
    "\n",
    "Creamos el *DataSet* de *PyTorch* que guarda y maneja los datos de los archivos de audio (que contienen los cantos de las aves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBirdDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset de audios de aves.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, process_func, audio_path, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        The __init__ function is run once when instantiating the Dataset object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 'df' es el DataFrame a almacenar.\n",
    "        self.df = df\n",
    "        \n",
    "        # 'process_func' toma la ruta de un audio a procesar, y lo procesa.\n",
    "        self.process_func = process_func\n",
    "        \n",
    "        # 'audio_path' es la ruta donde se ubican los archivos de audio.\n",
    "        self.audio_path = audio_path\n",
    "        \n",
    "        # 'transform' and 'target_transform' modify the samples and labels respectively.\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The __len__ function returns the number of samples in our dataset.\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx=None):\n",
    "        \"\"\"\n",
    "        The __getitem__ function loads and returns a sample from the dataset at the given index 'idx'.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Si no se especifica un índice, se toma una muestra aleatoria.\n",
    "        if idx == None:\n",
    "            idx = random.randrange(0, birds_ds.__len__()) # Rango [a,b).\n",
    "        \n",
    "        # Obtenemos la idx-ésima línea del DataFrame almacenado.\n",
    "        # Y el nombre del archivo de audio a procesar.\n",
    "        item = self.df.iloc[idx]\n",
    "        filename = item[file_col_name]\n",
    "        \n",
    "        # Procesamos el primer cacho de audio.\n",
    "        x,_ = self.process_func(self.audio_path+filename, True)\n",
    "        \n",
    "        # Si se desea que ambos cachos de audio compartan la propiedad,\n",
    "        # sólo dejamos la etiqueta como \"1\", y volvemos a procesar\n",
    "        # el mismo archivo de audio de manera aleatoria (más adelante).\n",
    "        if (random.random() < p_prop):\n",
    "            target = 1\n",
    "            \n",
    "        # Si, por otro lado, se desea que los cachos no compartan la\n",
    "        # propiedad, dejamos la etiqueta como \"-1\", y buscamos otro\n",
    "        # archivo de audio para procesar.\n",
    "        else:\n",
    "            target = -1\n",
    "            \n",
    "            # Guardamos la especie del ave del primer cacho de audio.\n",
    "            primera_especie = item[\"Species\"]\n",
    "            \n",
    "            # Quitamos el primer archivo de audio (que ya fue procesado) del\n",
    "            # DataFrame (temporalmente), obtenemos algún renglón aleatorio de\n",
    "            # este nuevo DataFrame (sample() devuelve un DataFrame, por lo que\n",
    "            # es necesario tomar el primer renglón con iloc[0]), y obtenemos\n",
    "            # el nombre del nuevo archivo de audio a procesar.\n",
    "            item = self.df.drop(idx).sample().iloc[0]\n",
    "            filename = item[file_col_name]\n",
    "                  \n",
    "            # Guardamos la especie del ave del segundo cacho de audio.\n",
    "            segunda_especie = item[\"Species\"]\n",
    "            \n",
    "            # TO-DO: Si son la misma especie, ¿sigo buscando otro segundo cacho\n",
    "            # de audio, o cambio el \"target\" a 1? Por ahora sólo lo cambio a 1.\n",
    "            #print(f\"El primer cacho de audio pertenece a un ave {primera_especie}, y el segundo pertenece a un ave {segunda_especie}.\")\n",
    "            if primera_especie == segunda_especie:\n",
    "                target = 1\n",
    "\n",
    "        # Procesamos el segundo cacho de audio.\n",
    "        y,_ = self.process_func(self.audio_path+filename, True)\n",
    "            \n",
    "        # NOTA:\n",
    "        # Aún no se define el uso para 'transform' y 'target_transform'.\n",
    "        # Una propuesta es que 'transform' sustituya a 'process_func'.\n",
    "        \n",
    "        # Devolvemos el primer cacho de audio, el segundo cacho de audio,\n",
    "        # y la etiqueta que indica si ambos comparten (1) o no (0) la propiedad.\n",
    "        return x, y, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el *DataSet* al pasarle:\n",
    "- El *DataFrame* creado previamente con *pandas*.\n",
    "- La función a utilizar para procesar los audios.\n",
    "- La ruta del directorio en el cual se encuentran los archivos de audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_ds = CustomBirdDataset(birds_df, librosa_process, birds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo del *DataSet*\n",
    "\n",
    "Y obtenemos una muestra aleatoria del *DataSet* mediante su función `__getitem__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#birds_ds.__getitem__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del *DataLoader*\n",
    "\n",
    "Creamos dos *DataLoader* de *PyTorch* que envuelven el *DataSet* previamente definido. Uno está definido para el entrenamiento, mientras que otro está definido para el testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_dl_train = [\n",
    "    DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    DataLoader(birds_ds, batch_size=batch_size, shuffle=False, drop_last=True),\n",
    "    #DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "    #DataLoader(birds_ds, batch_size=batch_size, shuffle=False, drop_last=True),\n",
    "    #DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "]\n",
    "\n",
    "birds_dl_test = DataLoader(birds_ds, batch_size=batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo del **DataLoader**\n",
    "\n",
    "El *DataLoader* contiene listas (que regresa la función `__getitem__()` correspondiente al *DataSet*). Estas listas contienen los lotes de tamaño `batch_size` y, para abarcar todos los datos, contiene aproximadamente `tamaño_de_todos_los_datos/batch_size` listas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"- Tamaño del DataSet (de PyTorch) = {len(birds_ds)} = {len(birds_df)} = Tamaño del DataFrame (de pandas)\")\n",
    "#print(f\"- Tamaño del DataLoader (de PyTorch): {len(birds_dl)}\")\n",
    "#iterador = iter(birds_dl)\n",
    "#primer_lote = next(iterador)\n",
    "#print(f\"- Tamaño de la primera lista del DataLoader: {len(primer_lote)}\")\n",
    "#print(f\"- Tamaño de los elementos de la primera lista: {len(primer_lote[0])} {len(primer_lote[1])} {len(primer_lote[2])}\")\n",
    "#print(f\"- Tamaño del DataLoader por el tamaño de cada lote: {len(birds_dl)*batch_size} ≈ {len(birds_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos datos sobre el primer lote para ejemplificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#primeros_cachos, segundos_cachos, labels = primer_lote\n",
    "#print(f\"- Los primeros cachos de audio del primer lote tienen tamaño: {primeros_cachos.size()}\")\n",
    "#print(f\"- Los segundos cachos de audio del primer lote tienen tamaño: {segundos_cachos.size()}\")\n",
    "#print(f\"- Las etiquetas del primer lote tienen tamaño: {labels.size()}\")\n",
    "#print(f\"- Etiquetas del primer lote: {labels}\")\n",
    "#print(f\"- Primeros cachos del primer lote: {primeros_cachos}\")\n",
    "#print(f\"- Segundos cachos del primer lote: {segundos_cachos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module is the base class for all neural network modules.\n",
    "# Our models should also subclass this class.\n",
    "# Modules can also contain other Modules, allowing to nest them in a tree structure.\n",
    "class RN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red Neuronal.\n",
    "    \"\"\"\n",
    "    \n",
    "    #This defines the structure of the NN.\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicialización de la Red Neuronal.\n",
    "        Aquí se define su estructura.\n",
    "        \"\"\"\n",
    "        \n",
    "        #\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inicio de las capas convolucionales.\n",
    "        conv_layers = []\n",
    "        \n",
    "        # Primera capa convolucional.\n",
    "        self.conv1 = nn.Conv2d(in_channels=audio_channels, out_channels=batch_size, kernel_size=(10,10), stride=(2,1))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        conv_layers += [self.conv1, self.relu1, self.mp1]\n",
    "        \n",
    "        # Segunda capa convolucional.\n",
    "        self.conv2 = nn.Conv2d(in_channels=batch_size, out_channels=(batch_size//2), kernel_size=(7,7), stride=(2,1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        conv_layers += [self.conv2, self.relu2, self.mp2]\n",
    "        \n",
    "        # Tercera capa convolucional.\n",
    "        self.conv3 = nn.Conv2d(in_channels=(batch_size//2), out_channels=(batch_size//4), kernel_size=(4,4), stride=(2,1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.mp3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        conv_layers += [self.conv3, self.relu3, self.mp3]\n",
    "        \n",
    "        # Fin de las capas convoluciones.\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        # Inicio de las capas lineales (fully-connected).\n",
    "        fc_layers = []\n",
    "        \n",
    "        # Primera capa lineal.\n",
    "        # TO-DO: Determinar entrada.\n",
    "        # TO-DO: Determinar salida.\n",
    "        self.fc1 = nn.Linear(int(22.5*batch_size),512)\n",
    "        fc_layers += [self.fc1]\n",
    "        \n",
    "        # Fin de las capas lineales.\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "        \n",
    "        # Segunda capa lineal.\n",
    "        # TO-DO: Determinar entrada.\n",
    "        # TO-DO: Determinar salida. ¿Es 1 valor para cada entrada?\n",
    "        self.fc2 = nn.Linear(4063232, 1)\n",
    "        # Ésta no se agrega a las demás,\n",
    "        # pues no se aplica individualmente a\n",
    "        # cada entrada; primero es necesario\n",
    "        # realizar la operación de distancia\n",
    "        # sobre éstas para después aplicar\n",
    "        # esta capa lineal.\n",
    "    \n",
    "    def invididual_process(self, z):\n",
    "        \n",
    "        start = timer()\n",
    "        z = self.conv(z)\n",
    "        end = timer()\n",
    "        #print(f\"\\t\\t[time] Capas convolucionales: time={end-start}s out_size={z.size()}\") # DEBUG\n",
    "        \n",
    "        # Para que 'z' tenga sólo una dimensión...\n",
    "        #print(f\"\\t\\tz antes de z.view: {z.size()}\") # DEBUG\n",
    "        z = z.view(batch_size, -1)\n",
    "        #print(f\"\\t\\tz después de z.view: {z.size()}\") # DEBUG\n",
    "        \n",
    "        start = timer()\n",
    "        z = self.fc(z)\n",
    "        end = timer()\n",
    "        #print(f\"\\t\\t[time] Capas lineales: time={end-start}s out_size={z.size()}\") # DEBUG\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        # TO-DO: Analizar las múltiples dimensiones en 2 o 3 dimensiones.\n",
    "        #TSNE(n_components=2, verbose=True).fit_transform(x)\n",
    "        \n",
    "        #print(f\"\\tProcesando x: {x.size()}\") # DEBUG\n",
    "        x = self.invididual_process(x)\n",
    "        #print(f\"\\tProcesando y: {y.size()}\") # DEBUG\n",
    "        y = self.invididual_process(y)\n",
    "        \n",
    "        # Para que 'x' y 'y' tengan sólo una dimensión...\n",
    "        x = x.view(batch_size, -1)\n",
    "        #print(f\"\\t\\tDespués de aplanar x: {x.size()}\") # DEBUG\n",
    "        y = y.view(batch_size, -1)\n",
    "        #print(f\"\\t\\tDespués de aplanar y: {y.size()}\") # DEBUG\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Definición del modelo.\n",
    "red = RN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The loss function is the quantity that will be\n",
    "# minimized during training.\n",
    "# TO-DO: Escoger 'loss' adecuado.\n",
    "#loss_func = losses.ContrastiveLoss().to(device)\n",
    "loss_func = nn.CosineEmbeddingLoss().to(device)\n",
    "\n",
    "# The optimizer determines how the network will be\n",
    "# updated based on the loss function.\n",
    "# TO-DO: Escoger 'optimizer' adecuado.\n",
    "#optimizer = torch.optim.SGD(red.parameters(), lr=lr, momentum=momentum) # Usado por MNIST Colab.\n",
    "optimizer = torch.optim.Adam(red.parameters(), lr = lr) # Usado por AlexNet (TMLoss y MSELoss).\n",
    "\n",
    "# Para obtener estadísticas del entrenamiento.\n",
    "loss_history = []\n",
    "epoch_timing = []\n",
    "\n",
    "def train(epoch):\n",
    "    \"\"\"\n",
    "    Entrenamiento de la RN.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicio, en segundos, del epoch.\n",
    "    epoch_start = timer()\n",
    "    \n",
    "    # Imprimimos el número de epoch.\n",
    "    print(f\"Epoch {epoch}...\")\n",
    "    \n",
    "    # Recorremos cada DataLoader.\n",
    "    for dl_idx, dl in enumerate(birds_dl_train):\n",
    "        \n",
    "        # DEBUG.\n",
    "        print(f\"\\tTrabajando con el {dl_idx}-ésimo DataLoader:\")\n",
    "        \n",
    "        for batch_idx, data in enumerate(dl):\n",
    "\n",
    "            # DEBUG.\n",
    "            relative_batch_str = f\"{batch_idx+1}/{len(dl)}\"\n",
    "            #print(f\"\\tProcesando lote {relative_batch_str}...\")\n",
    "        \n",
    "            # Completando lotes que no tienen tamaño batch_size.\n",
    "            incremento = 0\n",
    "            tam_original = len(data[2])\n",
    "            while len(data[2]) < batch_size:\n",
    "                x,y,l = birds_ds.__getitem__()\n",
    "                x = torch.tensor(x)[None, :]\n",
    "                y = torch.tensor(y)[None, :]\n",
    "                l = torch.unsqueeze(torch.tensor(l), 0)\n",
    "                data[0] = torch.cat((data[0], x), 0)\n",
    "                data[1] = torch.cat((data[1], y), 0)\n",
    "                data[2] = torch.cat((data[2], l), 0)\n",
    "                incremento += 1\n",
    "            if incremento != 0:\n",
    "                print(f\"\\tLote {relative_batch_str} de tamaño {tam_original} incrementado en {incremento}.\")\n",
    "            assert len(data[0]) == len(data[1])\n",
    "            assert len(data[0]) == len(data[2])\n",
    "\n",
    "            # 'data' es una lista que representa un lote:\n",
    "            # data[0] contiene los primeros cachos de audio.\n",
    "            # data[1] contiene los segundos cachos de audio.\n",
    "            # data[2] contiene las etiquetas.\n",
    "            for i,d in enumerate(data):\n",
    "                data[i] = d.to(device)\n",
    "\n",
    "            # Convertimos las etiquetas a tipo flotante.\n",
    "            # Necesario para la función de pérdida BCE.\n",
    "            #data[2] = data[2].to(torch.float32)\n",
    "            # TO-DO: ¿Ya no se usará BCE?\n",
    "\n",
    "            # Vaciamos los gradientes para este lote.\n",
    "            optimizer.zero_grad()\n",
    "            # In PyTorch, for every mini-batch during the training phase,\n",
    "            # we typically want to explicitly set the gradients to zero\n",
    "            # before starting to do backpropragation (i.e., updating the\n",
    "            # Weights and biases) because PyTorch accumulates the gradients\n",
    "            # on subsequent backward passes.\n",
    "\n",
    "            # Metemos los datos a la red neuronal.\n",
    "            output_x, output_y = red(data[0], data[1])\n",
    "\n",
    "            # TO-DO: Describir.\n",
    "\n",
    "            start = timer()\n",
    "            loss = loss_func(output_x, output_y, data[2])\n",
    "            end = timer()\n",
    "            #print(f\"\\t[time] Loss function: {end-start}s\") # DEBUG\n",
    "\n",
    "            start = timer()\n",
    "            loss.backward() #dloss/dx for every variable\n",
    "            end = timer()\n",
    "            #print(f\"\\t[time] Loss backward: {end-start}s\") # DEBUG\n",
    "\n",
    "            # TensorBoard.\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "            start = timer()\n",
    "            optimizer.step() #to do a one-step update on our parameter.\n",
    "            end = timer()\n",
    "            #print(f\"\\t[time] Optimizer step: {end-start}s\") # DEBUG\n",
    "\n",
    "            # Guardamos estadísticas del entrenamiento.\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "\n",
    "            # DEBUG:\n",
    "            print(f\"\\tLoss: {loss}\")\n",
    "            #print(f\"\\tEtiquetas: {data[2]}\")\n",
    "            #print(f\"\\tOutput: {output}\")\n",
    "            #print()\n",
    "\n",
    "            #break # DEBUG: Permite el entrenamiento de sólo un lote.\n",
    "    \n",
    "    # Fin, en segundos, del epoch.\n",
    "    epoch_end = timer()\n",
    "    \n",
    "    # Tiempo total, en segundos, del epoch.\n",
    "    epoch_time_seconds = epoch_end-epoch_start\n",
    "    epoch_timing.append(epoch_time_seconds)\n",
    "    \n",
    "    # DEBUG:\n",
    "    print(f\"[time] Epoch {epoch}: {epoch_time_seconds}s = {epoch_time_seconds/(60)}m\")\n",
    "    print()\n",
    "    \n",
    "# red.train() le indica al modelo que está siendo entrenado.\n",
    "# Esto ayuda con capas como Dropout y BatchNorm, que están\n",
    "# diseñadas para comporsarse distinto durante entrenamiento\n",
    "# y evaluación.\n",
    "red.train()\n",
    "    \n",
    "# Ejecutamos el entrenamiento definido, \"epochs\" veces.\n",
    "train_start = timer()\n",
    "for epoch in range(1, epochs+1): # Rango [a, b)\n",
    "    train(epoch)\n",
    "    #break # DEBUG: Permite la ejecución de sólo un epoch.\n",
    "train_end = timer()\n",
    "\n",
    "# Call flush() method to make sure that all pending events have been written to disk.\n",
    "# If you do not need the summary writer anymore, call close() method.\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas del entrenamiendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 10))\n",
    "plt.plot(range(0,len(loss_history)), loss_history, 'r')\n",
    "plt.title('loss_history')\n",
    "plt.xlabel('Lote')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_seconds = train_end-train_start\n",
    "print(\"[time] Total training time: {:.3f}s = {:.3f}h\".format(train_time_seconds, train_time_seconds/(60*60)))\n",
    "print(f\"[time] Time for each epoch:\")\n",
    "for i,x in enumerate(epoch_timing):\n",
    "    print (\"\\t{:<5} {:.3f}s\".format(i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testeo de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargado de la RN.\n",
    "#fecha = \"2023-01-28-17-37\"\n",
    "#red = RN().to(device)\n",
    "#red.load_state_dict(torch.load(DIR_models+fecha+\".pt\"))\n",
    "red.eval()\n",
    "\n",
    "def test(epoch):\n",
    "    \"\"\"\n",
    "    Testeo de la RN.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Estadísticas del testeo.\n",
    "    VP, FP, VN, FN = [0]*4\n",
    "    \n",
    "    # Inicio, en segundos, del epoch.\n",
    "    epoch_start = timer()\n",
    "    \n",
    "    # Imprimimos el número de epoch.\n",
    "    print(f\"Epoch {epoch}...\")\n",
    "    \n",
    "    for batch_idx, data in enumerate(birds_dl_test):\n",
    "\n",
    "        # DEBUG.\n",
    "        print(f\"\\tProcesando lote {batch_idx+1}/{len(birds_dl_test)}...\")\n",
    "        \n",
    "        # Completando lotes que no tienen tamaño batch_size.\n",
    "        incremento = 0\n",
    "        tam_original = len(data[2])\n",
    "        while len(data[2]) < batch_size:\n",
    "            x,y,l = birds_ds.__getitem__()\n",
    "            x = torch.tensor(x)[None, :]\n",
    "            y = torch.tensor(y)[None, :]\n",
    "            l = torch.unsqueeze(torch.tensor(l), 0)\n",
    "            data[0] = torch.cat((data[0], x), 0)\n",
    "            data[1] = torch.cat((data[1], y), 0)\n",
    "            data[2] = torch.cat((data[2], l), 0)\n",
    "            incremento += 1\n",
    "        if incremento != 0:\n",
    "            print(f\"\\tLote de tamaño {tam_original} incrementado en {incremento}.\")\n",
    "        assert len(data[0]) == len(data[1])\n",
    "        assert len(data[0]) == len(data[2])\n",
    "        \n",
    "        # 'data' es una lista que representa un lote:\n",
    "        # data[0] contiene los primeros cachos de audio.\n",
    "        # data[1] contiene los segundos cachos de audio.\n",
    "        # data[2] contiene las etiquetas.\n",
    "        for i,d in enumerate(data):\n",
    "            data[i] = d.to(device)\n",
    "        \n",
    "        # Metemos los datos a la red neuronal.\n",
    "        output_x, output_y = red(data[0], data[1])\n",
    "        \n",
    "        # Realizamos la diferencia con Similitud Coseno.\n",
    "        cos = nn.CosineSimilarity()\n",
    "        diff = cos(output_x, output_y)\n",
    "        \n",
    "        #print(f\"Etiquetas: {data[2]}\")\n",
    "        #print(f\"CosineSimilarity: {diff}\")\n",
    "\n",
    "        # Estadísticas del testeo del lote.\n",
    "        correctNeg50 = 0\n",
    "        correctZero = 0\n",
    "        correctPos50 = 0\n",
    "        correctUmbral = 0\n",
    "        umbral = 0.75 # Umbral entre -1 y 1.\n",
    "        \n",
    "        for i,l in enumerate(data[2]):\n",
    "            #total += 1\n",
    "            \n",
    "            if (diff[i] >= -0.5 and l == 1) or (diff[i] < -0.5 and l == -1):\n",
    "                correctNeg50 += 1\n",
    "            \n",
    "            if (diff[i] >= 0 and l == 1) or (diff[i] < 0 and l == -1):\n",
    "                correctZero += 1\n",
    "            \n",
    "            if (diff[i] >= 0.5 and l == 1) or (diff[i] < 0.5 and l == -1):\n",
    "                correctPos50 += 1\n",
    "            \n",
    "            if (diff[i] >= umbral and l == 1) or (diff[i] < umbral and l == -1):\n",
    "                correctUmbral += 1\n",
    "        \n",
    "        #print(f\"\\t\\tCon umbral -0.5: {correctNeg50}/{len(data[2])}\")\n",
    "        #print(f\"\\t\\tCon umbral    0: {correctZero}/{len(data[2])}\")\n",
    "        #print(f\"\\t\\tCon umbral  0.5: {correctPos50}/{len(data[2])}\")\n",
    "        #print(f\"\\t\\tCon umbral {umbral}: {correctUmbral}/{len(data[2])}\")\n",
    "        \n",
    "        # ROC_AUC\n",
    "        y_pred = diff.cpu().detach().numpy()\n",
    "        y_true = data[2].cpu().detach().numpy()\n",
    "        print(f\"\\t{roc_auc_score(y_true, y_pred)}\")\n",
    "        \n",
    "        # DEBUG: Permite el testeo de un sólo lote\n",
    "        break\n",
    "        \n",
    "        # TO-DO: Terminar definición del testeo.\n",
    "\n",
    "# Ejecutamos el testeo definido, \"epochs\" veces.\n",
    "test_start = timer()\n",
    "for epoch in range(1, epochs+1): # Rango [a, b)\n",
    "    test(epoch)\n",
    "    #break # DEBUG: Permite la ejecución de sólo un epoch.\n",
    "test_end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas del testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_seconds = test_end-test_start\n",
    "print(\"[time] Total testing time: {:.3f}s = {:.3f}m = {:.3f}h\".format(test_time_seconds, test_time_seconds/60, test_time_seconds/(60*60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardado de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(red.state_dict(), DIR_models+dt_string+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG\n",
    "\n",
    "Esta celda y las siguientes son para testear. Han de ser eliminadas cuando se limpie el código de este *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado de objetos.\n",
    "with open(DIR_objects+dt_string+\"_loss-history\", \"wb\") as file:\n",
    "    pickle.dump(loss_history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargado de objetos.\n",
    "fecha = \"\"\n",
    "\n",
    "#with open(DIR_objects+fecha+\"_loss-history\", \"rb\") as file:\n",
    "    #pickle.dump(loss_history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = birds_df.iloc[5][file_col_name]\n",
    "#librosa_process(birds_path+f, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.uniform(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*range(0,10))\n",
    "print(*range(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_string = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "print(dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargado de modelos.\n",
    "fecha = \"\"\n",
    "\n",
    "# Definimos un modelo que alojará a la Red Neuronal.\n",
    "#modelo = RN().to(device)\n",
    "\n",
    "# Actualizamos el modelo.\n",
    "#modelo.load_state_dict(torch.load(DIR_models+fecha+\".pt\"))\n",
    "\n",
    "# modelo.eval() le indica al modelo que ha de evaluar.\n",
    "#modelo.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(birds_df))\n",
    "print(len(birds_ds))\n",
    "print(f\"total_audios/batch_size = {len(birds_df)/batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha = \"2023-02-01-05-24\"\n",
    "#red = RN().to(device)\n",
    "#red.load_state_dict(torch.load(DIR_models+fecha+\".pt\"))\n",
    "#red.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,l = birds_ds.__getitem__()\n",
    "\n",
    "x = np.expand_dims(x, 0)\n",
    "x = np.repeat(x, batch_size, axis=0)\n",
    "x = torch.tensor(x).to(device)\n",
    "\n",
    "y = np.expand_dims(y, 0)\n",
    "y = np.repeat(y, batch_size, axis=0)\n",
    "y = torch.tensor(y).to(device)\n",
    "\n",
    "#l = np.repeat(l, batch_size)\n",
    "\n",
    "output_x, output_y = red(x, y)\n",
    "cos = nn.CosineSimilarity()\n",
    "diff = cos(output_x, output_y)\n",
    "\n",
    "print(f\"label: {l}\")\n",
    "print(f\"diff: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "print(\"En México:\")\n",
    "print(datetime.now(timezone(timedelta(hours=-6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
